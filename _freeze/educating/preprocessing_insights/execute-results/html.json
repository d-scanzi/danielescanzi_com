{
  "hash": "b0356150daa1197ee9ea011ffd25d4eb",
  "result": {
    "markdown": "---\ntitle: 'test'\n---\n\n# EEG PREPROCESSING - Insights\n\n*This page will be a work-in progress project. I will update when as the\nopportunity comes*\n\nThe goal of today's lab meeting is to gain insight into the most common\nEEG data cleaning (preprocessing) steps. I'm going to use MNE (Python)\nfor a couple of reasons. Firstly, we will introduce an alternative to\nMATLAB in the hope that we can all move away from it for our studies.\nSecondly, because the point of today is not to learn the \"how-to\"\npreprocess data (how to make a script, what functions to use, how to\ncode, etc...). I believe that is the easiest part, and there are many\ntutorials around. Instead, we will try to gain a deeper understanding of\nwhat we are doing to the data every time we perform specific cleaning\nsteps. I believe this is a more interesting and useful approach,\nespecially in an hour.\n\nWe will start by loading the EEG data. This is a pilot recording I have\nconducted on myself while performing Simon's task.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n```\n:::\n\n\n## Looking at the data\n\nThe first thing you should do when starting to work with EEG data is to\nlook at it. This is useful to get accustomed to how clean EEG data looks\nlike, how common artefacts appear (e.g. blinks, muscle noise) and, in\ngeneral, to ensure that your recording is as you expect it.\n\n``` python\nraw_data.plot()\n```\n\n![](img/data_scroll.png)\n\n## Downsampling\n\nIf the EEG data was collected with a high sample rate (commonly 1000\nHz), then you can decide to downsample the data to a lower sampling\nrate, like 500 Hz or 250 Hz. To my understanding, the primary reason for\ndownsampling is to reduce the size of the data and speed up the\nanalyses. This was especially important back in the days when computers\ndid not have much memory and computing power. In my opinion, nowadays,\nthis is not really necessary as most computers are able to handle large\ndatasets. Moreover, by downsampling, we remove some information from the\ndata. Unless downsampling is necessary (e.g. it would take months for\nyour PC to preprocess your data), then I would not do it. However, some\nonline discussions suggest that downsampling might be useful to obtain a\nbetter ICA decomposition (I don't think this has been formally tested).\n\nAt its core, downsampling is an easy step that simply requires selecting\none point for every N. How many N points you skip is defined by the\nsampling rate you want to obtain. Let's check this.\n\n",
    "supporting": [
      "preprocessing_insights_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}