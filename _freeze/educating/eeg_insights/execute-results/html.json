{
  "hash": "b7c60d4ef714a44f1a13f81575fd84c4",
  "result": {
    "markdown": "---\ntitle: 'EEG Preprocessing Insights'\n---\n\n# EEG PREPROCESSING - Insights\n\n*This page will be a work-in progress project. I will update as the\nopportunity comes. An interactive Jupyter notebook and a Python script can be found [here](https://github.com/d-scanzi/danielescanzi_com/tree/main/extra_material/eeg_insights_extra)*\n\n       \nThe goal of today's lab meeting is to gain insight into the most common EEG data cleaning (preprocessing) steps. I'm going to use MNE (Python) for a couple of reasons. Firstly, we will introduce an alternative to MATLAB in the hope that we can all move away from it for our studies. Secondly, because the point of today is not to learn the \"how-to\" preprocess data (how to make a script, what functions to use, how to code, etc...). I believe that is the easiest part, and there are many tutorials around. Instead, we will try to gain a deeper understanding of\nwhat we are doing to the data every time we perform specific cleaning steps. I believe this is a more interesting and useful approach, especially in an hour.\n\nWe will start by loading the EEG data. This is a pilot recording I have\nconducted on myself while performing Simon's task.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n```\n:::\n\n\n## Looking at the data\n\nThe first thing you should do when starting to work with EEG data is to\nlook at it. This is useful to get accustomed to how clean EEG data looks\nlike, how common artefacts appear (e.g. blinks, muscle noise) and, in\ngeneral, to ensure that your recording is as you expect it.\n\n``` python\nraw_data.plot()\n```\n\n![](img/data_scroll.png)\n\n## Downsampling\n\nIf the EEG data was collected with a high sample rate (commonly 1000\nHz), then you can decide to downsample the data to a lower sampling\nrate, like 500 Hz or 250 Hz. To my understanding, the primary reason for\ndownsampling is to reduce the size of the data and speed up the\nanalyses. This was especially important back in the days when computers\ndid not have much memory and computing power. In my opinion, nowadays,\nthis is not really necessary as most computers are able to handle large\ndatasets. Moreover, by downsampling, we remove some information from the\ndata. Unless downsampling is necessary (e.g. it would take months for\nyour PC to preprocess your data), then I would not do it. However, some\nonline discussions suggest that downsampling might be useful to obtain a\nbetter ICA decomposition (I don't think this has been formally tested).\n\nAt its core, downsampling is an easy step that simply requires selecting\none point for every N. How many N points you skip is defined by the\nsampling rate you want to obtain. Let's check this.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample (MODIFY THE NEXT LINE)\nnew_sampling_rate = 50\n\nnumber_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * new_sampling_rate)\nstep_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\ndownsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\ndownsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n\nplt.plot(samples_vector, sine_signal, \"-k\", linewidth=8, alpha=0.3)\nplt.scatter(downsampled_time, downsampled_signal, c=\"r\", s=6, alpha=0.6)\nplt.plot(downsampled_time, downsampled_signal, \"-r\", linewidth=1)\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](eeg_insights_files/figure-html/cell-3-output-1.png){width=582 height=411}\n:::\n:::\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of\nour signal. Although here we see distortions only at very low values, we\nneed to think about how a lower resolution could affect a complex signal\nas the EEG data. For instance, by downsampling, we are likely to remove\ninformation represented as high-frequency oscillatory activity.\n\n***NOTE: this is a simplified representation of downsampling. In\nreality, filters are applied and a sliding average is computed to avoid\ndistortions (eg. aliasing). In other words, do not use this code on your\ndata! Consider what you have observed and what we said about frequencies\nbeing distorted, and you can understand why filtering is important\nhere***\n\nNow let's downsample the data.\n\n``` python\nraw_data.resample(sfreq=250)\n```\n\n## Filtering\n\nThis is an essential step, but I think it's probably one of the most\ntechnical and complex topics in signal processing. Filtering aims to\nremove parts of the data that are likely to contain artefacts or not to\nrepresent brain activity. For the sake of simplicity, we will only\ndiscuss *high-pass filters* and *low-pass filters*without stopping on\nany specific type of filter (eg. Finite Impulse Response vs Infinite\nImpulse Response). What we will introduce, though, are some of the\nparameters you will definitely encounter and will be relevant to your\ndata.\n\nIn general, a filter can be described as a mathematical procedure that\nallows the removal of specific frequencies from the data. It is\nimportant to understand this point. When a filter is applied, you modify\nthe frequency information of your signal. Although downsampling partly\naffects the frequency domain too, with filters we do not trim the data.\nIn other words, the time resolution of your data is left untouched.\n\nLet's see.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, \"k\", linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), \"r\")\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors=\"k\", linestyles=\"dashed\", linewidth=3)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<matplotlib.collections.LineCollection at 0x29a708ccf20>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](eeg_insights_files/figure-html/cell-4-output-2.png){width=805 height=411}\n:::\n:::\n\n\nHere we filtered only the second half of the signal. Let's see it's\ntime-frequency decomposition.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to=\"magnitude\", mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\", extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color=\"gray\", linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors=\"r\", linestyles=\"dashed\", linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2))\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(0.0, 2.0)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](eeg_insights_files/figure-html/cell-5-output-2.png){width=804 height=416}\n:::\n:::\n\n\nWe can see here that the 20 Hz component of our signal disappears after\nthe filter is applied.\n\nCool, now to the technical part. As I said, filters are not so\nstraightforward, and when you decide which one to apply to your data,\nyou should pause and think about it. To begin understanding why this is\nthe case, let's look at how a commonly used filter (Butterworth)\nattenuates frequencies.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncutoff_frequency = 50\nfilter_orders    = [2, 3, 4, 8, 10, 12]\n\n\nfig, axs = plt.subplots(2, 1)\nplt.tight_layout()\nfor filter_order in filter_orders:\n    b,a = signal.butter(filter_order, cutoff_frequency, \"low\", analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    axs[0].plot(w, 20 * np.log10(abs(h)), label=f\"Order: {filter_order}\")\n    axs[0].vlines(50, -50, 5, colors=\"k\")\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    axs[0].legend()\n    \n    axs[1].plot(t, y)\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n```\n\n::: {.cell-output .cell-output-display}\n![](eeg_insights_files/figure-html/cell-6-output-1.png){width=977 height=484}\n:::\n:::\n\n\nTo understand filters and the information you might find online or\npublished in some papers, we need to define a few terms. Note that these\nterms should be reported in a publication to ensure the correct\nreplicability of your analyses - though not many people do this.\n\n-   Cutoff Frequency: frequency threshold from/to where (ideally) the\n    filter starts acting\n-   Passband: Frequency region (ideally) unaffected by the filter\n-   Stopband: Frequency region (ideally) suppressed by the filter\n-   Transition band: Frequency region where the filter attenuates the\n    amplitude without complete suppression\n-   Lowpass filter: a filter that reduces/suppresses high frequencies\n    (beyond the defined cutoff + passband)\n-   Highpass filter: a filter that reduces/suppresses low frequencies\n    (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function `pop_eegfiltnew`, you\nwill need to provide, at least, the *passband* limits and not the cutoff\nfrequency!\n\n![](img\\eeglab_filt.png)\n\nAs you can see here, by increasing the filter length, we make the filter\nless steep, and we push the ringing outside the transition band. The\ndownside is that we are now keeping more frequencies that are beyond the\ncutoff. By modifying the filter length, as well as the type of filter\nand its specific parameters, we can modify how long the transition band\nis, how steep the filter is and how many of those ripples you can see\nabove we get.\n\nTalking about ripples. You need to be aware of them as they can\nintroduce artefacts in your data (the data itself can become \"ripply\").\nUsually, if they appear in the stopband, you should be mostly fine, as\ntheir effects will affect a dimension of the data you supposedly are not\ninterested in (otherwise, why would you filter out frequencies you want\nto analyse). However, you need to pay attention to filters that\nintroduce ripples in the passband. Below, you can see that the\n*rectangular* filter induces some oscillations before the transition\nband (\\<50 Hz). Because these ripples affect frequencies that will be\ncontained in your filtered data, they will alter the data itself.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# FIR\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.set_xlim((0, 200))\n    ax.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](eeg_insights_files/figure-html/cell-7-output-1.png){width=822 height=411}\n:::\n:::\n\n\nUsually, you won't need to create filters from scratch. Most EEG\npackages (MNE, EEGLAB, Brainstorm, etc...) provide functions with\n(usually) sensible default values. However, always check your filters\nand their outputs!\n\n``` python\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None)\nraw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)\n```\n\n## Find bad channels\n\nLots can happen during an EEG session and one of the most common events\nis that one or more electrodes do not record the signal properly. The\ncauses for this can vary, incorectly setting up the electode cap, the\nparticipant touching the electordes, electrical noise, or some\nelectrodes dying. Independently from the cause, we want to capture these\nelectrode and remove them from the data. If we do not do so, we run the\nrisk to retain or even introduce noise and artefact in the data we will\nanalyse (see Rereferencing to average section).\n\nSo, how do we go about finiding bad channels? I don;t have a set answer\nfor this. There are multiple procedures and the choice on how you want\nto go about this step is yours. The simplest thing you can do is to look\nat the data manually. If you open the data scroll (or plot the\nchannels), you might be able to pick up channels that have unrealistic\nhigh or low voltages throughout the recording or for part of it.\n\n![](img\\data_scroll_noise_ch.PNG)\n\nHere, for instance, you can see that one channel oscillates all over the\nplace. That doesn't look like brain at all and we might want to flag\nthis channel to remove it. If you don't want to look at every channel,\nthen you need to decide on one or more rules to flag noisy channels.\nEEGLAB has a pervasive set of algorithms to achieve this (way too many\nfor my taste). Their default now is to use a function called\n`clean_rawdata`, which would remove a channel if:\n\n1.  It is flat for longer than a threshold\n2.  It is not correlated with other electrodes\n3.  It contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest\nyou run it a few times on the same dataset and ensure that the removed\nchannels are always the same (or almost always). In my experience,\nsometimes the results are not consistent, especially when you have a\nshort recording (short in terms of samples) or a low number of\nelectrodes.\n\n***NOTE: I haven't seen this detail being reported much, but there is\none important element (to me) that you should consider when removing\nchannels: their location. One thing is to remove a few channels here and\nthere; another thing is to remove a cluster of adjacent channels. If bad\nchannels are close to each other, there might be a systematic issue that\naffects that cluster specifically. Moreover, it would be dangerous to\nproceed with their reconstruction through interpolation (next\nsection).***\n\n## Interpolation\n\nOnce you have removed bad channels, what should you do? If you are a\npurist, you might want to leave these channels out. Hopefully you do not\nneed them for your analyses. Otherwise, you could reconstruct them using\nthe information in the other channels. This step is called\n***interpolation***, and it is based on the idea that EEG electrodes\ncontain data that is correlated. That is, two nearby channels partly\npick up the same signal. Thus, if we remove one channel, we can use the\nother to reconstruct what the signal in the removed channel should look\nlike.\n\nFor EEG data, this is usually done through a ***spherical\ninterpolation***, a procedure that projects the channels to the surface\nof a sphere and then uses the information contained in the retained\nchannels to reconstruct the removed one.\n\n",
    "supporting": [
      "eeg_insights_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}