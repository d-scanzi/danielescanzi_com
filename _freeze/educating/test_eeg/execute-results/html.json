{
  "hash": "f5cfa13d1e7fdce4e2b175e41a5c00a5",
  "result": {
    "markdown": "---\ntitle: 'test'\n---\n\n# EEG PREPROCESSING - Insights\n    \n*This page will be a work-in progress project. I will update when as the\nopportunity comes*\n    \n    The goal of today's lab meeting is to gain insight into the most common\nEEG data cleaning (preprocessing) steps. I'm going to use MNE (Python)\nfor a couple of reasons. Firstly, we will introduce an alternative to\nMATLAB in the hope that we can all move away from it for our studies.\nSecondly, because the point of today is not to learn the \"how-to\"\npreprocess data (how to make a script, what functions to use, how to\n                 code, etc...). I believe that is the easiest part, and there are many\ntutorials around. Instead, we will try to gain a deeper understanding of\nwhat we are doing to the data every time we perform specific cleaning\nsteps. I believe this is a more interesting and useful approach,\nespecially in an hour.\n\nWe will start by loading the EEG data. This is a pilot recording I have\nconducted on myself while performing Simon's task.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n```\n:::\n\n\n## Looking at the data\n\nThe first thing you should do when starting to work with EEG data is to\nlook at it. This is useful to get accustomed to how clean EEG data looks\nlike, how common artefacts appear (e.g. blinks, muscle noise) and, in\ngeneral, to ensure that your recording is as you expect it.\n\n``` python\nraw_data.plot()\n```\n\n![](img/data_scroll.png)\n\n## Downsampling\n\nIf the EEG data was collected with a high sample rate (commonly 1000\nHz), then you can decide to downsample the data to a lower sampling\nrate, like 500 Hz or 250 Hz. To my understanding, the primary reason for\ndownsampling is to reduce the size of the data and speed up the\nanalyses. This was especially important back in the days when computers\ndid not have much memory and computing power. In my opinion, nowadays,\nthis is not really necessary as most computers are able to handle large\ndatasets. Moreover, by downsampling, we remove some information from the\ndata. Unless downsampling is necessary (e.g. it would take months for\nyour PC to preprocess your data), then I would not do it. However, some\nonline discussions suggest that downsampling might be useful to obtain a\nbetter ICA decomposition (I don't think this has been formally tested).\n\nAt its core, downsampling is an easy step that simply requires selecting\none point for every N. How many N points you skip is defined by the\nsampling rate you want to obtain. Let's check this.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample (MODIFY THE NEXT LINE)\nnew_sampling_rate = 50\n\nnumber_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * new_sampling_rate)\nstep_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\ndownsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\ndownsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n\nplt.plot(samples_vector, sine_signal, \"-k\", linewidth=8, alpha=0.3)\nplt.scatter(downsampled_time, downsampled_signal, c=\"r\", s=6, alpha=0.6)\nplt.plot(downsampled_time, downsampled_signal, \"-r\", linewidth=1)\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](test_eeg_files/figure-html/cell-3-output-1.png){width=582 height=411}\n:::\n:::\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\n\n***NOTE: this is a simplified representation of downsampling. In reality, filters are applied and a sliding average is computed to avoid distortions (eg. aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here***\n\nNow let's downsample the data.\n\n```{.python}\nraw_data.resample(sfreq=250)\n```\n\n## Filtering\n\nThis is an essential step, but I think it's probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss *high-pass filters* and *low-pass filters*without stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data. \n\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\n\nLet's see.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, \"k\", linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), \"r\")\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors=\"k\", linestyles=\"dashed\", linewidth=3)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n<matplotlib.collections.LineCollection at 0x166ff8ef2c0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](test_eeg_files/figure-html/cell-4-output-2.png){width=805 height=411}\n:::\n:::\n\n\nHere we filtered only the second half of the signal. Let's see it's time-frequency decomposition.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to=\"magnitude\", mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\", extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color=\"gray\", linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors=\"r\", linestyles=\"dashed\", linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2))\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(0.0, 2.0)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](test_eeg_files/figure-html/cell-5-output-2.png){width=804 height=416}\n:::\n:::\n\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied. \n\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let's look at how a commonly used filter (Butterworth) attenuates frequencies.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncutoff_frequency = 50\nfilter_orders    = [2, 3, 4, 8, 10, 12]\n\n\nfig, axs = plt.subplots(2, 1)\nplt.tight_layout()\nfor filter_order in filter_orders:\n    b,a = signal.butter(filter_order, cutoff_frequency, \"low\", analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    axs[0].plot(w, 20 * np.log10(abs(h)), label=f\"Order: {filter_order}\")\n    axs[0].vlines(50, -50, 5, colors=\"k\")\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    axs[0].legend()\n    \n    axs[1].plot(t, y)\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n```\n\n::: {.cell-output .cell-output-display}\n![](test_eeg_files/figure-html/cell-6-output-1.png){width=977 height=484}\n:::\n:::\n\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\n* Cutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\n* Passband: Frequency region (ideally) unaffected by the filter\n* Stopband: Frequency region (ideally) suppressed by the filter\n* Transition band: Frequency region where the filter attenuates the amplitude without complete suppression\n* Lowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\n* Highpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function `pop_eegfiltnew`, you will need to provide, at least, the *passband* limits and not the cutoff frequency!\n\n![](img\\eeglab_filt.png)\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\n\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become \"ripply\"). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the *rectangular* filter induces some oscillations before the transition band (<50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# FIR\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.set_xlim((0, 200))\n    ax.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](test_eeg_files/figure-html/cell-7-output-1.png){width=822 height=411}\n:::\n:::\n\n\nUsually, you won't need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc...) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\n\n\n```{.python}\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None)\nraw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)\n```\n\n## Find bad channels\n\nLots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section). \n\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\n![](img\\data_scroll_noise_ch.PNG)\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn't look like brain at all and we might want to flag this channel to remove it. \nIf you don't want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called `clean_rawdata`, which would remove a channel if:\n\n1. It is flat for longer than a threshold\n2. It is not correlated with other electrodes\n3. It contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\n\n***NOTE: I haven't seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section).***\n\n## Interpolation\n\nOnce you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called ***interpolation***, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like. \n\nFor EEG data, this is usually done through a ***spherical interpolation***, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one. \n\n",
    "supporting": [
      "test_eeg_files"
    ],
    "filters": [],
    "includes": {}
  }
}