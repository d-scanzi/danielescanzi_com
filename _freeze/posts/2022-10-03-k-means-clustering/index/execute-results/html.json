{
  "hash": "717ab0dbb13a71796545eb0f77493f24",
  "result": {
    "markdown": "---\ntitle: \"K means clustering\"\nauthor: \"Daniele Scanzi\"\ndate: \"2022/10/03\"\ndescription: \"K means algortihm walk through presented at the lab meeting to explain this technique.\" \ncategories: [R, coding, microstates]\nimage: clustering_example.png\n---\n\n\n## Brief overlook\n\nK-means is a class of unsupervised clustering algorithms that was\ndeveloped within the field of signal processing. Given a set of data\npoints $X={x_{1}, x_{2}, x_{3}, ..., x_{N}}$, the aim is to find *k*\nclusters of points so that the Euclidean mean of the points within each\ncluster is minimised. Conversely, the distance between clusters is\nmaximised.\n\nAlthough there are many different k-means algorithms, the general\nprocedure is the following:\n\n1.  Define the number of clusters (*k*)\n2.  Initialise the center point (centroid) for each cluster\n3.  Compute the distance between each point and every centroid\n4.  Assign points to the cluster whose centroid is minimally distant\n5.  Update the centroid location\n6.  Repeat assignment and updates until the difference between\n    iterations in negligible\n\nFor this simulation we are going to use the data provided\n[here](https://www.dominodatalab.com/blog/getting-started-with-k-means-clustering-in-python)\n\n\n\n\n\nThe dataframe looks like this.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 2\n       x     y\n   <dbl> <dbl>\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5 \n```\n:::\n:::\n\n\n## Define the number of clusters\n\nThe number of clusters depends on the specific application. For\ninstance, in [EEG microstate\nanalysis](https://en.wikipedia.org/wiki/EEG_microstates) one common\npractice is to define the use of 4 clusters, which are descriptively\ncalled *A*, *B*, *C*, and *D*. However, note that defining a number a\npriori is a drawback of this technique. Ideally we would like to find a\nvalue that allows explaining the greatest proportion of variability in\nthe data (without assigning each data point to a different group).\nConsequently, forcing the use of 4 - or any other number - of clusters\nmight be a suboptimal option. We are going to discuss this a bit more\nlater on. For the moment let's be sneaky and look at the data.\n\nOur data contains X and Y coordinates for 60 points which are so\ndistributed:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nFrom the plot we can reasonably say that there are **3** clusters, so we\nare going to work with that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define number of clusters (k)\nk <- 3\n```\n:::\n\n\n## Initialise centroids\n\nThe first step of this algorithm is to select the *k* points that are\n*stereotypes* of the clusters. In other words, points that are\nrepresentative of the groups we want to create. These points are called\n***centroids*** or ***prototypes***. Obviously, we do not know what\ngroups we will end up with, so the simplest way to select the first\ncentroids is to pick *k* points at random. Let's define a function that\ndoes exactly this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define function that select k centroids from a dataset.By default the function will select 2 centroids if no k is provided\n\npick_centroids <- function(data, k, seed=1234){\n  # Randomly select k rows from the dataset provided\n  set.seed(seed)\n  centroids <- data[sample(nrow(data),k), ]\n  # Add a unique letter label\n  centroids <- cbind(centroids, 'label'=LETTERS[1:k])\n  return(centroids)\n}\n```\n:::\n\n\nWe can now pick 3 random centroids and visualize them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select first centroids\ncentroids_1 <- pick_centroids(df, k=3, seed=19)\n\n# Visualise them\ndf %>% \n  ggplot(aes(x=x, y=y)) +\n  geom_point(size=2, alpha=0.5, colour='gray') +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=5, shape=15) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Compute the distance between each point and each cluster\n\nOnce the first centroids have been selected, we can start to divide all\nthe other points into the corresponding clusters. Each point will be\nassigned to the cluster represented by the centroid that is its closest\ngeometrically. To do so, we need to compute the *Euclidean distance*\nbetween every point and every centroid. Then, we select the minimum\ndistance and assign the point to that centroid's group. The Euclidean\nformula is:\n\n$$ \\bar{A,B} = \\sqrt{(x_{A} - x_{B})^{2} + (y_{A} - y_{B})^{2}} $$ The\nfollowing function returns two pieces of information for each point.\nFirstly, the assigned group as defined by the minimum Euclidean distance\nfrom the corresponding centroid. Secondly, an \"error' value defined as\nthe distance between the point and its closest centroid. We will use\nthis error to set up a stopping rule for our k-means algorithm later on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define function to compute the Euclidean distance\neuclidean_distance <- function(data, centroid){\n  distance <- sapply(1:nrow(data), function(i){\n    sqrt(sum((data[i,] - centroid)^2))\n  })\n  return(distance)\n}\n\n\n# Define a function that applies the euclidean distance to each point and returns the minimum \n# Note that this function presupposes that the centroids have a x and y coordinates columns\nfind_min_distance <- function(data, centroids, c_coord){\n  # Firstly we compute the distance between each point and each centroid\n  distances <- sapply(1:nrow(centroids), function(i){\n    euclidean_distance(data, centroids[i, c_coord])\n  })\n  \n  # For each point let's find the centroid with the minimum distance\n  min_idx <- apply(distances, 1, which.min)\n  \n  # We also extract the minimum distance so we can return it\n  min_distance <- apply(distances, 1, FUN = min)\n  \n  # Extract the associated labels\n  min_labels <- sapply(1:length(min_idx), function(i){\n    centroids$label[min_idx[i]]\n  })\n  \n  return(list('error'=min_distance, 'labels'=min_labels))\n}\n```\n:::\n\n\nNow we can apply this to every point in our dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_iter <- find_min_distance(df, centroids_1, c('x', 'y'))\n\n# Let's plot this\ncbind(df, 'label' = first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour = label)) +\n  geom_point(size=2, alpha=0.7) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThat looks like a good starting point, although the B group dominates\nmost of the data. To improve the categorisation, we can build from here\nby repeating this process over and over. Each time we will select new\ncentroids and assign the points to the group represented by the closest\ncentroid. We do this until there are no more significant changes in the\ngroups.\n\n## Update centroids\n\nAfter one iteration, we need to update the centroids. A simple way to do\nthis is by computing the mean coordinate values for each group. The new\ncentroids will be defined by these mean coordinates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupdate_centroids <- function(df, labels){\n  new <- cbind(df, 'label' = labels) %>% \n    group_by(label) %>% \n    summarise(x = mean(x), \n              y = mean(y)) %>% \n    relocate(label)\n    \n  return(new)\n}\n\n# Compute new centroids\ncentroids_2 <- update_centroids(df, first_iter$labels)\n\n# Plot old and new centroids\ncbind(df, 'label'=first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour=label)) +\n  geom_point(size=2, alpha=0.5) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=3, shape=15) +\n  geom_point(data=centroids_2, aes(x=x, y=y, colour=label), size=5, shape=4) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe squares represent the original centroids, the Xs represent the new\nones, and the points are still coloured according to their original\ncategorisation. Notice how the blue centroid is now in the centre of its\ngroup.\n\n## Reiterate\n\nWe are ready to reiterate the update and assignation process *N* times.\nAs said above, we will stop when there are no more significant\ndifferences between one categorisation and the next one. To quantify\nthis, we will use the *error* value we introduce just before. For each\npoint, this is represented by the distance of the point from its closest\ncentroid. Thus, we can sum these error values and use this sum as the\nstopping rule. When the sum of errors is reduced below a predetermined\nthreshold, then we can stop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_kmeans <- function(data, k=2, c_coord= c('x', 'y'), tolerance=1e-4, seed=1234){\n  # Firstly we find the first centroids\n  current_centroids <- pick_centroids(data, k=k, seed=seed)\n  \n  # Create datasets were to store results\n  labelling <- c()\n  centroids <- current_centroids\n  \n  # Reiterate labelling - assignment - update centroids\n  continue <- TRUE\n  iter <- 0\n  previous_error <- 0\n  while(continue){\n    \n    # Assign data to centroids\n    current_groups <- find_min_distance(data, current_centroids, c_coord)\n    \n    # Store assigned labels with column name as the iteration number\n    iter <- iter + 1\n    labelling <- cbind(labelling, current_groups$labels)\n    \n    # Update centroids\n    current_centroids <- update_centroids(data, current_groups$labels)\n    centroids <- rbind(centroids, current_centroids)\n    \n    # Check if we have minimizes the error below the threshold\n    current_error <- sum(current_groups$error)\n    current_err_diff <- abs(previous_error - current_error)\n    print(sprintf('Iteration %s -> Error: %s', iter, current_err_diff))\n    if(current_err_diff <= tolerance){\n      continue = FALSE\n    }\n    # If we did not reach the tolerance, update the current error\n    previous_error <- current_error\n    \n  }\n  colnames(labelling) <- 1:iter\n  # remove last centroid data as it has not been used and assign iter values\n  centroids <- centroids[1:(nrow(centroids)-k), ]\n  centroids <- cbind(centroids, 'iter'=rep(1:iter, each=k))\n  return(list('lables'=labelling, 'centroids'=centroids, 'error'=current_groups$error))\n}\n```\n:::\n\n\nLet's iterate on our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults1 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 458.171667198993\"\n[1] \"Iteration 2 -> Error: 72.4112246075529\"\n[1] \"Iteration 3 -> Error: 0.655622083297658\"\n[1] \"Iteration 4 -> Error: 0\"\n```\n:::\n:::\n\n\nSweet, for this particular case the algorithm converged in 4 iterations.\nLet's see the final result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(df, 'group'=results1$lables[, ncol(results1$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThis looks all good! Yay!\n\n## Initialization problems\n\nThe previous result might make you think that this algorithm is amazing.\nIt categorised our data in just four iterations and with a perfect\ndivision. However, things are always more complicated than they\ninitially appear. Indeed, a drawback of the k-means approach is that the\nfinal result is highly dependent on the initial centroids. We can\ndemonstrate this by starting the algorithm with different initial\ncentroids. We will exploit the seed argument we provided to our\nfunctions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults2 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=19)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 1278.04142581076\"\n[1] \"Iteration 2 -> Error: 549.175741899334\"\n[1] \"Iteration 3 -> Error: 120.573809438913\"\n[1] \"Iteration 4 -> Error: 7.0347771777997\"\n[1] \"Iteration 5 -> Error: 10.0949551678153\"\n[1] \"Iteration 6 -> Error: 5.53498901037312\"\n[1] \"Iteration 7 -> Error: 4.606053850374\"\n[1] \"Iteration 8 -> Error: 1.49594562166419\"\n[1] \"Iteration 9 -> Error: 2.36620189065968\"\n[1] \"Iteration 10 -> Error: 0\"\n```\n:::\n\n```{.r .cell-code}\ncbind(df, 'group'=results2$lables[, ncol(results2$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2, alpha=.5) +\n  geom_point(data=results2$centroids %>% filter(iter==max(iter)), aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHere the algorithm converged to a suboptimal solution. During the years\ndifferent solutions have been created to address this problem, with the\nmost popular and reliable being the *kmeans++* algorithm created by\nDavid Arthur and Sergei Vassilvitskii. If you are interested, the\nprocedure is presented\n[here](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)\n\n## How many clusters?\n\nAs stated in the introduction, one obvious limitation of this paradigm\nis that the number of clusters needs to be defined a priori. Thus, we\nneed a system that would allow us to select the optimal number of\nclusters that reduces the classification error as much as possible\nwithout \"overfitting\". One simple method to do so is to run the\nalgorithm with different number of clusters and use the *scree plot* of\nthe error as a guide. To explain this let's change dataset and pick\nsomething with a greater number of observations and a less clear number\nof clusters. We will use the `iris dataset` provided in R. Our task is\nto cluster the flowers into species based on the `sepal length` and the\n`petal width`.\n\nIf we look at the raw data we can see two possible groups, but now the\nsituation is more complex than before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nTo establish the optimal amount of clusters, we are going to run our\nk-means algorithm *10 times* adding one cluster at each iteration. Each\ntime we will store the final error, so we can plot it later. Here's the\ncode:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nks <- 1:10\nerrors <- rep(0, length(ks))\niris_df <- iris[, c('Sepal.Length', 'Petal.Width')]\ncolnames(iris_df) <- c('x', 'y')\nfor(r in ks){\n  errors[r] <- sum(my_kmeans(iris_df, k=r)$error)\n}\n```\n:::\n\n\nNow we can create the scree plot by visualising the final error for each\niteration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make scree plot of the errors\ndata.frame('error'=errors, 'k'=1:length(errors)) %>% \n  ggplot(aes(x=k, y=error)) +\n  geom_point(size=2) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:length(errors)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe scree plot is a \"descriptive tools\" so it won't tell specifically\nthe correct number of clusters. The main idea here is to look at the\n*elbow* of the plot, that is the point at which the trend plateau. This\nflexion point indicates the value after which increasing the number of\ngroups does not provide a significant decrease in the error. Looking at\nthe plot we can say that the elbow is in between *k*=3 or *k*=4. As a\nstarting point we can visualise both of them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 84.5340503544893\"\n[1] \"Iteration 2 -> Error: 22.5320451284015\"\n[1] \"Iteration 3 -> Error: 0.12222997427822\"\n[1] \"Iteration 4 -> Error: 0.194408399479336\"\n[1] \"Iteration 5 -> Error: 0\"\n```\n:::\n\n```{.r .cell-code}\niris_k4 <- my_kmeans(iris_df, k = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 67.9944121319274\"\n[1] \"Iteration 2 -> Error: 9.13499108999842\"\n[1] \"Iteration 3 -> Error: 1.30457150911509\"\n[1] \"Iteration 4 -> Error: 0.932100341554694\"\n[1] \"Iteration 5 -> Error: 1.0714446191438\"\n[1] \"Iteration 6 -> Error: 0.716841774592375\"\n[1] \"Iteration 7 -> Error: 0.675081843250041\"\n[1] \"Iteration 8 -> Error: 0.318333061193755\"\n[1] \"Iteration 9 -> Error: 0\"\n```\n:::\n\n```{.r .cell-code}\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThe groups are very similar, or at least there are no weird differences.\nThis is a good thing. To better investigate possible difference between\nthe two clustering systems, we could re-run them with a different seed\nand see if they are consistent (although we have discussed the\nimplications that the first initialization can have).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3, seed = 0905)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 99.0311396766634\"\n[1] \"Iteration 2 -> Error: 24.4105692091305\"\n[1] \"Iteration 3 -> Error: 4.88652621704111\"\n[1] \"Iteration 4 -> Error: 3.47693331688762\"\n[1] \"Iteration 5 -> Error: 2.56320926311736\"\n[1] \"Iteration 6 -> Error: 1.62704674573919\"\n[1] \"Iteration 7 -> Error: 0.0724143145959744\"\n[1] \"Iteration 8 -> Error: 0.057240636048725\"\n[1] \"Iteration 9 -> Error: 0.0225024050884883\"\n[1] \"Iteration 10 -> Error: 0\"\n```\n:::\n\n```{.r .cell-code}\niris_k4 <- my_kmeans(iris_df, k = 4, seed = 0905)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Iteration 1 -> Error: 97.5708407922748\"\n[1] \"Iteration 2 -> Error: 29.7874124595873\"\n[1] \"Iteration 3 -> Error: 7.83693472432838\"\n[1] \"Iteration 4 -> Error: 2.47590455673429\"\n[1] \"Iteration 5 -> Error: 0.153319685392383\"\n[1] \"Iteration 6 -> Error: 0.0496830708143321\"\n[1] \"Iteration 7 -> Error: 0.0799826359733871\"\n[1] \"Iteration 8 -> Error: 0\"\n```\n:::\n\n```{.r .cell-code}\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nNote that the colours can change between iterations. What we are\ninterested in are the patterns of groups. The results are interesting.\n*k*=3 produced the same results as before. Conversely, *k*=4 did\nsomething different; it divided the lower cluster of points into two\ngroups. Although this is not a formal assessment, from here we might\nwant to say that *k*=3 is more reliable and go with it. As a final test,\nlet's compare these results agains the real division by species.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour=Species)) +\n  geom_point(size=2) + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nLet's hihglight the differences:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's assign a species label to to A, B, D clusters\nspecies <- unique(iris$Species)\n\n# Merge datasets and code whether there groups are the same\niris_binded <- cbind(iris, 'my_kmeans' = iris_k3$lables[, ncol(iris_k3$lables)]) %>% \n  mutate(\n    my_species = case_when(\n      my_kmeans == 'A' ~ species[3],\n      my_kmeans == 'B' ~ species[1], \n      TRUE ~ species[2]),\n    equal = case_when(my_species != Species ~ 0,\n                      TRUE ~ 1))\n\n# Plot highlighting difference\niris_binded %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour = factor(equal))) +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c('red', 'lightgray'), name = 'Difference', labels = c('different', 'equal')) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThe red dots are the observations that our kmeans algorithm categorised\ndifferently compared to the original division into species. Overall, the\nresult can be improved but it's not too bad considering that the\nmajority of points were correctly identified.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}