{"title":"Bayesian Engines","markdown":{"yaml":{"title":"Bayesian Engines","format":{"revealjs":{"theme":"dark","css":"my_style.css"}}},"headingText":"Last week","containsRefs":false,"markdown":"\n\n\n::: {.v-center-container}\n> 2E4. The Bayesian statistician Bruno de Finetti (1906--1985) began his\n> 1973 book on probability theory with the declaration: \"PROBABILITY\n> DOES NOT EXIST.\" The capitals appeared in the original, so I imagine\n> de Finetti wanted us to shout this statement. What he meant is that\n> probability is a device for describing uncertainty from the\n> perspective of an observer with limited knowledge; it has no objective\n> reality. Discuss the globe tossing example from the chapter, in light\n> of this statement. What does it mean to say \"the probability of water\n> is 0.7\"?\n\n:::\n\n# Discussion {.center}\n\n## Discussion\n\n::: {.v-center-container}\n> In contrast, Bayesian estimates are valid for any sample size. This\n> does not mean that more data isn't helpful---it certainly is. Rather,\n> the estimates have a clear and valid interpretation, no matter the\n> sample size. But the price for this power is dependency upon the\n> initial plausibilities, the prior. If the prior is a bad one, then the\n> resulting inference will be misleading.\n\n::: \n\n# Video {.center}\n\n## Video\n\n<iframe width=\"95%\" height=\"70%\" src=\"https://www.youtube.com/embed/guTdrfycW2Q?start=2638\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\n\n</iframe>\n\nClick [here](https://youtu.be/guTdrfycW2Q?t=2638)\n\n## Post-video discussion \n\n::: {.v-center-container}\nWhy sampling?\n:::\n\n## Implement the engines\n\n1.  Draw cards\n2.  Define grid resolution\n3.  Initialise grid\n4.  Define prior\n5.  Define Likelihood\n6.  Compute posterior\n7.  Plot!\n\n## Draw cards\n\n::: columns\n::: {.column width=\"40%\"}\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"1-2\"}\n# Store draws (1 = R, 0 = B)\ndraws \n```\n:::\n:::\n\n## Draw cards\n\n::: columns\n::: {.column width=\"40%\"}\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"1-2\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n```\n:::\n:::\n\n## Grid resolution\n\n:::: columns\n\n::: {.column width=\"50%\"}\n```{r}\nlibrary(tidyverse)\npoints_plot <- tibble(\n    x = seq(0, 1, by = .4),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\nline_plot <- tibble(\n    x = seq(0, 1, by = .01),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_line(data = line_plot, aes(x = x, y = y), alpha = .7, size = 1.5, colour = \"black\") +\n    geom_point(colour = \"red\", size = 4) +\n    geom_vline(xintercept = points_plot$x, colour = \"red\", alpha = .8) +\n    geom_hline(yintercept = points_plot$y, colour = \"red\", alpha = .8) +\n    scale_x_continuous(breaks = seq(0 , 1, by = .05)) +\n    theme_minimal()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n```{r}\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_path(size = 1.5) +\n    geom_point(colour = \"red\", size = 4) +\n    theme_minimal()\n```\n\n:::\n\n::::\n\n## Grid resolution\n\n:::: columns\n\n::: {.column width=\"50%\"}\n```{r}\npoints_plot <- tibble(\n    x = seq(0, 1, by = .2),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\nline_plot <- tibble(\n    x = seq(0, 1, by = .01),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_line(data = line_plot, aes(x = x, y = y), alpha = .7, size = 1.5, colour = \"black\") +\n    geom_point(colour = \"red\", size = 4) +\n    geom_vline(xintercept = points_plot$x, colour = \"red\", alpha = .8) +\n    geom_hline(yintercept = points_plot$y, colour = \"red\", alpha = .8) +\n    scale_x_continuous(breaks = seq(0 , 1, by = .05)) +\n    theme_minimal()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n```{r}\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_path(size = 1.5) +\n    geom_point(colour = \"red\", size = 4) +\n    theme_minimal()\n```\n\n:::\n\n::::\n## Grid resolution\n\n:::: columns\n\n::: {.column width=\"50%\"}\n```{r}\npoints_plot <- tibble(\n    x = seq(0, 1, by = .05),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\nline_plot <- tibble(\n    x = seq(0, 1, by = .01),\n    y = dnorm(x, mean = 0.3, sd = 0.2)\n)\n\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_line(data = line_plot, aes(x = x, y = y), alpha = .7, size = 1.5, colour = \"black\") +\n    geom_point(colour = \"red\", size = 4) +\n    geom_vline(xintercept = points_plot$x, colour = \"red\", alpha = .8) +\n    geom_hline(yintercept = points_plot$y, colour = \"red\", alpha = .8) +\n    scale_x_continuous(breaks = seq(0 , 1, by = .05)) +\n    theme_minimal()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n```{r}\npoints_plot %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_path(size = 1.5) +\n    geom_point(colour = \"red\", size = 4) +\n    theme_minimal()\n```\n\n:::\n\n::::\n\n## Grid resolution\n\n``` {.r code-line-numbers=\"3-4\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n# We need to define how coarse the grid is\ngrid_points <- \n```\n\n## Grid resolution\n\n``` {.r code-line-numbers=\"3-4\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n# We need to define how coarse the grid is\ngrid_points <- 100\n```\n\n## Implementing Grid Approximation\n\n``` {.r code-line-numbers=\"7-16\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    \n    # UNINFORMATIVE PRIOR\n    \n    # LIKELIHOOD\n    \n    # POSTERIOR\n)\n```\n\n## Grid of parameter values\n\n::: columns\n::: {.column width=\"40%\"}\nA grid is simply a selection of values that the parameter(s) of interest\ncan take. It's a way to discretize a continuous distribution\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"10-13\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    grid = seq(from       = 0, \n               to         = 1, \n               length.out = grid_points),\n    # UNINFORMATIVE PRIOR\n    \n    # LIKELIHOOD\n    \n    # POSTERIOR\n)\n```\n:::\n:::\n\n## Uninformative prior\n\n::: columns\n::: {.column width=\"40%\"}\nAn uninformative prior is a uniform distribution where every parameter\nvalue has the same probability than the others\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"14-15\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    grid = seq(from       = 0, \n               to         = 1, \n               length.out = grid_points),\n    # UNINFORMATIVE PRIOR\n    prior      = 1,\n    # LIKELIHOOD\n    \n    # POSTERIOR\n)\n```\n:::\n:::\n\n## Likelihood\n\n::: columns\n::: {.column width=\"40%\"}\nThe probability of the data given a specific parameter value\n\\[P(D\\|p)\\]. Our data consist of red and black cards, so we are asking\n*what is the probability of observing N red cards in N draws?*\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"16-17\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    grid = seq(from       = 0, \n               to         = 1, \n               length.out = grid_points),\n    # UNINFORMATIVE PRIOR\n    prior      = 1,\n    # LIKELIHOOD\n    likelihood = dbinom(sum(draws), size = length(draws), prob = grid),\n    # POSTERIOR\n)\n```\n:::\n:::\n\n## Posterior\n\n::: columns\n::: {.column width=\"40%\"}\nWe have the grid, the prior and the likelihood, let's apply Bayes'\ntheorem\n:::\n\n::: {.column width=\"60%\"}\n``` {.r code-line-numbers=\"18-19\"}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    grid = seq(from       = 0, \n               to         = 1, \n               length.out = grid_points),\n    # UNINFORMATIVE PRIOR\n    prior      = 1,\n    # LIKELIHOOD\n    likelihood = dbinom(sum(draws), size = length(draws), prob = grid),\n    # POSTERIOR\n    posterior  = (prior * likelihood) / sum(prior * likelihood)\n)\n```\n:::\n:::\n\n## Plot!\n\n``` {r}\n# Store draws (1 = R, 0 = B)\ndraws <- c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1)\n\n# We need to define how coarse the grid is\ngrid_points <- 100\n\n# Define Bayes theorem through grid approximation\ngrid_posterior <- tibble(\n\n    # GRID OF PARAMETER VALUES\n    grid = seq(from       = 0, \n               to         = 1, \n               length.out = grid_points),\n    # UNINFORMATIVE PRIOR\n    prior      = 1,\n    # LIKELIHOOD\n    likelihood = dbinom(sum(draws), size = length(draws), prob = grid),\n    # POSTERIOR\n    posterior  = (prior * likelihood) / sum(prior * likelihood)\n) %>% \n    pivot_longer(cols = !grid,\n                 names_to = \"distribution\",\n                 values_to = \"plausibility\")\n```\n\n```{r}\n#| echo: true\n\ngrid_posterior %>% \n    ggplot(aes( x = grid, y = plausibility, colour = distribution)) +\n    geom_point() +\n    theme_minimal() +\n    facet_grid(distribution~., scales = \"free_y\")\n    \n```\n# Exercises {.center}\n\n## Exercises\n\n::: {.v-center-container}\n\n>2M1. Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p. \n(1) W, W, W \n(2) W, W, W, L \n(3) L, W, W, L, W, W, W\n\n:::\n\n## Exercises\n\n::: {.v-center-container}\n\n>2M2. Now assume a prior for p that is equal to zero when p < 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n\n:::\n\n## Discussion\n\n::: {.v-center-container}\n\n> If you don't have a strong argument for any particular prior, then try\n> different ones. Because the prior is an assumption, it should be\n> interrogated like other assumptions: by altering it and checking how\n> sensitive inference is to the assumption. No one is required to swear\n> an oath to the assumptions of a model, and no set of assumptions\n> deserves our obedience.\n\n:::\n\n# Next time {.center}\n\n## Next time\n\n* Finish reading chapter 2 (from 2.3 to 2.5)\n* Exercise: 2E1, 2E2, 2E3, 2M3,  2M4,  2M5,  2M6,  2M7\n\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["my_style.css"],"output-file":"slide_02.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.269","auto-stretch":true,"editor":"visual","title":"Bayesian Engines","theme":"dark"}}}}