[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "~/About",
    "section": "",
    "text": "Daniele completed his Bachelor of Science in Psychology and Communication at the University of Milan - Bicocca in 2018. He joined the University of Auckland in 2021 as an Honours student. He is now starting a PhD working with Paul Corballis, director of the Cognitive Psychophysiology Lab and co-advised by Christopher Erb. He is interested in investigating the neural correlates of consciousness using EEG, eye-tracking and whatever machine he can get his hands on. He has a second identity as a magician and his assistant is a tiny blue plunger."
  },
  {
    "objectID": "exploring/exploring_intro.html",
    "href": "exploring/exploring_intro.html",
    "title": "Exploring B/Log",
    "section": "",
    "text": "This is me (on the left), my sister (eating the bread) and my cousins at Monte Avaro. For most people this name means nothing, but this place is probably one of the most important for me. Still today, some of the most important and memories I have comes from there.\nMy grandparents used to bring us four up to this mountain every summer. However, this was not just a one day family hike, this was a month-long stay. A month-long stay in a van that my grandfather repourposed as a self-contained van, without running water, toilet or anything else. We slept all together, some years all inside the van, some other years we got a tent on top of it. We collected water from a spring a couple of hundred meters down the road, my grandmother used to cook us with a small gas stove or on a campfire, where some nights we prepared vin brulé (the Italian version of mulled wine).\nThis place was amazing. It was not popular, there was hardly anyone around, just us, an alpine cow farmer who became a family friend (you see, my grandparents used to bring our parent up here as well when they young) and a couple of people owning a hut up there. Electricity was not a thing, I remember drinking hot chocolate in the hut lighted up by oil lamps.\n\n\n\n\n\n\nSo how do we spend the majority of our time? Well, hiking for hours with my grandfather looking for mushrooms: boletus pinicola, boletus luridus, Lactarius deliciosus (but only if the sape was orange), mazze di tamburo (lit. drumsticks, or macrolepiota procera) amanita vaginata, russola virescens, russola cyanoxantha and a few more. These excursions were always fun for us and scary for our parents. The reason? My grandfather never used tracks, we always explored the woods, away from the areas where other people would go. So imagine, I was 6 and I was walking 8 hours next to cliffs or underneath steep woods. Surely a hard work, but fun and rewarding.\n\n\n\n\n\n\nboletus pinicola\n\n\n\n\n\n\n\nboletus luridus\n\n\n\n\n\n\n\n\n\namanita vaginata\n\n\n\n\n\n\n\nmacrolepiota procera\n\n\n\n\n\nThroughout the years I acquired some specific knowledge. First of all, how to navigate that terrain safely while keeping up with my grandfather. He’s a fast walker, something I acquired too. Then, how to recognise good mushrooms (above), from those that we should avoid (amanita phalloidis, amanita muscaria, russola emetica). How to deal without the comforts of our homes for a long time. But most importantly, the love for the mountain and nature.\nRecently, I went back home in Bergamo. It was a fantastic time and I managed to squeeze in a few days in the mountains. It was winter there, so found some snow (not as mush as I would have imagined, but still fun). My family and I went up to the rifugio Vodala equipped with crampons. After a great lunch, we decided to keep going up and my sister, my cousin (Luca, the one not looking at the camera in the first picture), my mum and I attempted to reach Cima Timogno. However, fog started to build up and we needed enough time to go down. So we stopped half way and turned back. I am not really experienced in winter alpine terrain, so that was a nice challenge. It made me want some training and experience in winter alpine excursions.\n\n\n\n\n\n\nI also brought my wife up to Monte Avaro twice. Once to explore around and once to try bob sleading. Not the one on the ice track, but the kid version on a plastic sled. We climbed up a steep section of Monte Avaro where there were no people around and we went down a few times. It was fun to try this again after many years.\n\n\n\n\n\n\nMy family going up to Rifugio Vodala\n\n\n\n\n\n\n\nTurn around point with my cousin\n\n\n\n\n\n\n\n\n\nMonte avaro winter 2022\n\n\n\n\n\n\n\nDescending from monte tetta - Avaro 2022\n\n\n\n\n\nNow I am back in New Zealand, and I have a strong wanting to explore and reconnect with this part of my life I have put a bit aside over the past few years since I moved here. For a draw of luck, a friend of mine and his partner are into hiking and camping too, so it would be nice to have people around here to explore with. As said at the very beginning, this section will be a collection of the different excursions, short or long, I’ll do. Each page will have a short description (definitely shorter than this), reporting the major elements of the hike.\n\nThis section is dedicated to my grandfather that, now 88 years old, keeps asking me when we will go to the mountain to look for mushrooms. He always jokes that he might not be able to hike much anymore, but every time we end up walking 6 hours no stop…and he leads the whole way"
  },
  {
    "objectID": "exploring/monte_avaro.html",
    "href": "exploring/monte_avaro.html",
    "title": "Monte Avaro collection",
    "section": "",
    "text": "My sister and I with Boletus Pinicola\n\n\n\n\n\nRiding a goat"
  },
  {
    "objectID": "exploring/monte_avaro.html#december-2022",
    "href": "exploring/monte_avaro.html#december-2022",
    "title": "Monte Avaro collection",
    "section": "December 2022",
    "text": "December 2022\n\n\n\nEating panettone in the snow\n\n\n\n\n\nDrinking from a natural spring\n\n\n\n\n\nWinter view\n\n\n\n\n\nGoing into the woods"
  },
  {
    "objectID": "exploring/monte_avaro.html#videos",
    "href": "exploring/monte_avaro.html#videos",
    "title": "Monte Avaro collection",
    "section": "Videos",
    "text": "Videos\n\n\nVideo\nLaura (wife) decided she had enough of me\n\n\n\n\nVideo\nI decided to slide too\n\n\n\n\nVideo\nBad attempt to a front flip\n\n\n\n\nVideo\nBad attempt to stay bare feet in the snow\n\n\n\n\nVideo\nWoods with snow\n\n\n\n\nVideo\nBob sleading first attempt\n\n\n\n\nVideo\nLaura’s first time on a bob\n\n\n\n\nVideo\nA steeper but better bob ride"
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html",
    "href": "exploring/waihaha_2023_09_04.html",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "",
    "text": "Location: Pureora forest\nDate: from 2023/09/02 to 2023/09/04\nThree of us finally managed to squeeze two nights out for an end-of-winter camping trip. We headed to the Pureora Forest Park to walk the Waihāhā hut Track. The track follows the Waihāhā River and it offers a nice and easy walk across a variety of vegetation.\nWe spent the night at the Waihāhā hut, and the next day, we made our way back to the car. We decided to take the day slow, visiting Kinloch and then relaxing at the Wairakei Terraces.\nGiven that the weather was nice and would have remained clear until the next afternoon, we decided to camp out for another night. We headed to the Kakaho Campsite. The site was desert, we were the only one there. The night was clear, and we got treated to an amazing starry sky while trying to keep warm next to a campfire.\nAfter a quite cold night, we packed up and completed the short Rimu Track Loop within the Pureora Forest."
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "href": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "Notes for the future",
    "text": "Notes for the future\n\nFor winterish camping, source an Italian Army Blanket (or similar) or bring a hot water bottle. The night was fine, but it would have been nicer to have an extra source of warmth.\nInstant polenta is a great camping food. It packs compact and it’s filling. It might be better than pasta.\nNeed to learn some handy knots"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniele Scanzi",
    "section": "",
    "text": "I spend my day looking at squiggly lines produced by your brain. I would love to know how a bat sees the world. One day I ate five razorblades in front of 400 people."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html",
    "href": "posts/2022-10-03-k-means-clustering/index.html",
    "title": "K means clustering",
    "section": "",
    "text": "K-means is a class of unsupervised clustering algorithms that was developed within the field of signal processing. Given a set of data points \\(X={x_{1}, x_{2}, x_{3}, ..., x_{N}}\\), the aim is to find k clusters of points so that the Euclidean mean of the points within each cluster is minimised. Conversely, the distance between clusters is maximised.\nAlthough there are many different k-means algorithms, the general procedure is the following:\n\nDefine the number of clusters (k)\nInitialise the center point (centroid) for each cluster\nCompute the distance between each point and every centroid\nAssign points to the cluster whose centroid is minimally distant\nUpdate the centroid location\nRepeat assignment and updates until the difference between iterations in negligible\n\nFor this simulation we are going to use the data provided here\nThe dataframe looks like this.\n\n\n# A tibble: 6 × 2\n       x     y\n   <dbl> <dbl>\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "title": "K means clustering",
    "section": "Define the number of clusters",
    "text": "Define the number of clusters\nThe number of clusters depends on the specific application. For instance, in EEG microstate analysis one common practice is to define the use of 4 clusters, which are descriptively called A, B, C, and D. However, note that defining a number a priori is a drawback of this technique. Ideally we would like to find a value that allows explaining the greatest proportion of variability in the data (without assigning each data point to a different group). Consequently, forcing the use of 4 - or any other number - of clusters might be a suboptimal option. We are going to discuss this a bit more later on. For the moment let’s be sneaky and look at the data.\nOur data contains X and Y coordinates for 60 points which are so distributed:\n\n\n\n\n\nFrom the plot we can reasonably say that there are 3 clusters, so we are going to work with that.\n\n#Define number of clusters (k)\nk <- 3"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "title": "K means clustering",
    "section": "Initialise centroids",
    "text": "Initialise centroids\nThe first step of this algorithm is to select the k points that are stereotypes of the clusters. In other words, points that are representative of the groups we want to create. These points are called centroids or prototypes. Obviously, we do not know what groups we will end up with, so the simplest way to select the first centroids is to pick k points at random. Let’s define a function that does exactly this.\n\n# Define function that select k centroids from a dataset.By default the function will select 2 centroids if no k is provided\n\npick_centroids <- function(data, k, seed=1234){\n  # Randomly select k rows from the dataset provided\n  set.seed(seed)\n  centroids <- data[sample(nrow(data),k), ]\n  # Add a unique letter label\n  centroids <- cbind(centroids, 'label'=LETTERS[1:k])\n  return(centroids)\n}\n\nWe can now pick 3 random centroids and visualize them.\n\n# Select first centroids\ncentroids_1 <- pick_centroids(df, k=3, seed=19)\n\n# Visualise them\ndf %>% \n  ggplot(aes(x=x, y=y)) +\n  geom_point(size=2, alpha=0.5, colour='gray') +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=5, shape=15) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "href": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "title": "K means clustering",
    "section": "Compute the distance between each point and each cluster",
    "text": "Compute the distance between each point and each cluster\nOnce the first centroids have been selected, we can start to divide all the other points into the corresponding clusters. Each point will be assigned to the cluster represented by the centroid that is its closest geometrically. To do so, we need to compute the Euclidean distance between every point and every centroid. Then, we select the minimum distance and assign the point to that centroid’s group. The Euclidean formula is:\n\\[ \\bar{A,B} = \\sqrt{(x_{A} - x_{B})^{2} + (y_{A} - y_{B})^{2}} \\] The following function returns two pieces of information for each point. Firstly, the assigned group as defined by the minimum Euclidean distance from the corresponding centroid. Secondly, an “error’ value defined as the distance between the point and its closest centroid. We will use this error to set up a stopping rule for our k-means algorithm later on.\n\n# Define function to compute the Euclidean distance\neuclidean_distance <- function(data, centroid){\n  distance <- sapply(1:nrow(data), function(i){\n    sqrt(sum((data[i,] - centroid)^2))\n  })\n  return(distance)\n}\n\n\n# Define a function that applies the euclidean distance to each point and returns the minimum \n# Note that this function presupposes that the centroids have a x and y coordinates columns\nfind_min_distance <- function(data, centroids, c_coord){\n  # Firstly we compute the distance between each point and each centroid\n  distances <- sapply(1:nrow(centroids), function(i){\n    euclidean_distance(data, centroids[i, c_coord])\n  })\n  \n  # For each point let's find the centroid with the minimum distance\n  min_idx <- apply(distances, 1, which.min)\n  \n  # We also extract the minimum distance so we can return it\n  min_distance <- apply(distances, 1, FUN = min)\n  \n  # Extract the associated labels\n  min_labels <- sapply(1:length(min_idx), function(i){\n    centroids$label[min_idx[i]]\n  })\n  \n  return(list('error'=min_distance, 'labels'=min_labels))\n}\n\nNow we can apply this to every point in our dataset.\n\nfirst_iter <- find_min_distance(df, centroids_1, c('x', 'y'))\n\n# Let's plot this\ncbind(df, 'label' = first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour = label)) +\n  geom_point(size=2, alpha=0.7) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThat looks like a good starting point, although the B group dominates most of the data. To improve the categorisation, we can build from here by repeating this process over and over. Each time we will select new centroids and assign the points to the group represented by the closest centroid. We do this until there are no more significant changes in the groups."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "title": "K means clustering",
    "section": "Update centroids",
    "text": "Update centroids\nAfter one iteration, we need to update the centroids. A simple way to do this is by computing the mean coordinate values for each group. The new centroids will be defined by these mean coordinates.\n\nupdate_centroids <- function(df, labels){\n  new <- cbind(df, 'label' = labels) %>% \n    group_by(label) %>% \n    summarise(x = mean(x), \n              y = mean(y)) %>% \n    relocate(label)\n    \n  return(new)\n}\n\n# Compute new centroids\ncentroids_2 <- update_centroids(df, first_iter$labels)\n\n# Plot old and new centroids\ncbind(df, 'label'=first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour=label)) +\n  geom_point(size=2, alpha=0.5) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=3, shape=15) +\n  geom_point(data=centroids_2, aes(x=x, y=y, colour=label), size=5, shape=4) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe squares represent the original centroids, the Xs represent the new ones, and the points are still coloured according to their original categorisation. Notice how the blue centroid is now in the centre of its group."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "href": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "title": "K means clustering",
    "section": "Reiterate",
    "text": "Reiterate\nWe are ready to reiterate the update and assignation process N times. As said above, we will stop when there are no more significant differences between one categorisation and the next one. To quantify this, we will use the error value we introduce just before. For each point, this is represented by the distance of the point from its closest centroid. Thus, we can sum these error values and use this sum as the stopping rule. When the sum of errors is reduced below a predetermined threshold, then we can stop.\n\nmy_kmeans <- function(data, k=2, c_coord= c('x', 'y'), tolerance=1e-4, seed=1234){\n  # Firstly we find the first centroids\n  current_centroids <- pick_centroids(data, k=k, seed=seed)\n  \n  # Create datasets were to store results\n  labelling <- c()\n  centroids <- current_centroids\n  \n  # Reiterate labelling - assignment - update centroids\n  continue <- TRUE\n  iter <- 0\n  previous_error <- 0\n  while(continue){\n    \n    # Assign data to centroids\n    current_groups <- find_min_distance(data, current_centroids, c_coord)\n    \n    # Store assigned labels with column name as the iteration number\n    iter <- iter + 1\n    labelling <- cbind(labelling, current_groups$labels)\n    \n    # Update centroids\n    current_centroids <- update_centroids(data, current_groups$labels)\n    centroids <- rbind(centroids, current_centroids)\n    \n    # Check if we have minimizes the error below the threshold\n    current_error <- sum(current_groups$error)\n    current_err_diff <- abs(previous_error - current_error)\n    print(sprintf('Iteration %s -> Error: %s', iter, current_err_diff))\n    if(current_err_diff <= tolerance){\n      continue = FALSE\n    }\n    # If we did not reach the tolerance, update the current error\n    previous_error <- current_error\n    \n  }\n  colnames(labelling) <- 1:iter\n  # remove last centroid data as it has not been used and assign iter values\n  centroids <- centroids[1:(nrow(centroids)-k), ]\n  centroids <- cbind(centroids, 'iter'=rep(1:iter, each=k))\n  return(list('lables'=labelling, 'centroids'=centroids, 'error'=current_groups$error))\n}\n\nLet’s iterate on our data.\n\nresults1 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=4)\n\n[1] \"Iteration 1 -> Error: 458.171667198993\"\n[1] \"Iteration 2 -> Error: 72.4112246075529\"\n[1] \"Iteration 3 -> Error: 0.655622083297658\"\n[1] \"Iteration 4 -> Error: 0\"\n\n\nSweet, for this particular case the algorithm converged in 4 iterations. Let’s see the final result.\n\ncbind(df, 'group'=results1$lables[, ncol(results1$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThis looks all good! Yay!"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "title": "K means clustering",
    "section": "Initialization problems",
    "text": "Initialization problems\nThe previous result might make you think that this algorithm is amazing. It categorised our data in just four iterations and with a perfect division. However, things are always more complicated than they initially appear. Indeed, a drawback of the k-means approach is that the final result is highly dependent on the initial centroids. We can demonstrate this by starting the algorithm with different initial centroids. We will exploit the seed argument we provided to our functions.\n\nresults2 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=19)\n\n[1] \"Iteration 1 -> Error: 1278.04142581076\"\n[1] \"Iteration 2 -> Error: 549.175741899334\"\n[1] \"Iteration 3 -> Error: 120.573809438913\"\n[1] \"Iteration 4 -> Error: 7.0347771777997\"\n[1] \"Iteration 5 -> Error: 10.0949551678153\"\n[1] \"Iteration 6 -> Error: 5.53498901037312\"\n[1] \"Iteration 7 -> Error: 4.606053850374\"\n[1] \"Iteration 8 -> Error: 1.49594562166419\"\n[1] \"Iteration 9 -> Error: 2.36620189065968\"\n[1] \"Iteration 10 -> Error: 0\"\n\ncbind(df, 'group'=results2$lables[, ncol(results2$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2, alpha=.5) +\n  geom_point(data=results2$centroids %>% filter(iter==max(iter)), aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nHere the algorithm converged to a suboptimal solution. During the years different solutions have been created to address this problem, with the most popular and reliable being the kmeans++ algorithm created by David Arthur and Sergei Vassilvitskii. If you are interested, the procedure is presented here"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "title": "K means clustering",
    "section": "How many clusters?",
    "text": "How many clusters?\nAs stated in the introduction, one obvious limitation of this paradigm is that the number of clusters needs to be defined a priori. Thus, we need a system that would allow us to select the optimal number of clusters that reduces the classification error as much as possible without “overfitting”. One simple method to do so is to run the algorithm with different number of clusters and use the scree plot of the error as a guide. To explain this let’s change dataset and pick something with a greater number of observations and a less clear number of clusters. We will use the iris dataset provided in R. Our task is to cluster the flowers into species based on the sepal length and the petal width.\nIf we look at the raw data we can see two possible groups, but now the situation is more complex than before.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nTo establish the optimal amount of clusters, we are going to run our k-means algorithm 10 times adding one cluster at each iteration. Each time we will store the final error, so we can plot it later. Here’s the code:\n\nks <- 1:10\nerrors <- rep(0, length(ks))\niris_df <- iris[, c('Sepal.Length', 'Petal.Width')]\ncolnames(iris_df) <- c('x', 'y')\nfor(r in ks){\n  errors[r] <- sum(my_kmeans(iris_df, k=r)$error)\n}\n\nNow we can create the scree plot by visualising the final error for each iteration.\n\n# Make scree plot of the errors\ndata.frame('error'=errors, 'k'=1:length(errors)) %>% \n  ggplot(aes(x=k, y=error)) +\n  geom_point(size=2) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:length(errors)) +\n  theme_minimal()\n\n\n\n\nThe scree plot is a “descriptive tools” so it won’t tell specifically the correct number of clusters. The main idea here is to look at the elbow of the plot, that is the point at which the trend plateau. This flexion point indicates the value after which increasing the number of groups does not provide a significant decrease in the error. Looking at the plot we can say that the elbow is in between k=3 or k=4. As a starting point we can visualise both of them:\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3)\n\n[1] \"Iteration 1 -> Error: 84.5340503544893\"\n[1] \"Iteration 2 -> Error: 22.5320451284015\"\n[1] \"Iteration 3 -> Error: 0.12222997427822\"\n[1] \"Iteration 4 -> Error: 0.194408399479336\"\n[1] \"Iteration 5 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4)\n\n[1] \"Iteration 1 -> Error: 67.9944121319274\"\n[1] \"Iteration 2 -> Error: 9.13499108999842\"\n[1] \"Iteration 3 -> Error: 1.30457150911509\"\n[1] \"Iteration 4 -> Error: 0.932100341554694\"\n[1] \"Iteration 5 -> Error: 1.0714446191438\"\n[1] \"Iteration 6 -> Error: 0.716841774592375\"\n[1] \"Iteration 7 -> Error: 0.675081843250041\"\n[1] \"Iteration 8 -> Error: 0.318333061193755\"\n[1] \"Iteration 9 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nThe groups are very similar, or at least there are no weird differences. This is a good thing. To better investigate possible difference between the two clustering systems, we could re-run them with a different seed and see if they are consistent (although we have discussed the implications that the first initialization can have).\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 99.0311396766634\"\n[1] \"Iteration 2 -> Error: 24.4105692091305\"\n[1] \"Iteration 3 -> Error: 4.88652621704111\"\n[1] \"Iteration 4 -> Error: 3.47693331688762\"\n[1] \"Iteration 5 -> Error: 2.56320926311736\"\n[1] \"Iteration 6 -> Error: 1.62704674573919\"\n[1] \"Iteration 7 -> Error: 0.0724143145959744\"\n[1] \"Iteration 8 -> Error: 0.057240636048725\"\n[1] \"Iteration 9 -> Error: 0.0225024050884883\"\n[1] \"Iteration 10 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 97.5708407922748\"\n[1] \"Iteration 2 -> Error: 29.7874124595873\"\n[1] \"Iteration 3 -> Error: 7.83693472432838\"\n[1] \"Iteration 4 -> Error: 2.47590455673429\"\n[1] \"Iteration 5 -> Error: 0.153319685392383\"\n[1] \"Iteration 6 -> Error: 0.0496830708143321\"\n[1] \"Iteration 7 -> Error: 0.0799826359733871\"\n[1] \"Iteration 8 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nNote that the colours can change between iterations. What we are interested in are the patterns of groups. The results are interesting. k=3 produced the same results as before. Conversely, k=4 did something different; it divided the lower cluster of points into two groups. Although this is not a formal assessment, from here we might want to say that k=3 is more reliable and go with it. As a final test, let’s compare these results agains the real division by species.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour=Species)) +\n  geom_point(size=2) + \n  theme_minimal()\n\n\n\n\nLet’s hihglight the differences:\n\n# Let's assign a species label to to A, B, D clusters\nspecies <- unique(iris$Species)\n\n# Merge datasets and code whether there groups are the same\niris_binded <- cbind(iris, 'my_kmeans' = iris_k3$lables[, ncol(iris_k3$lables)]) %>% \n  mutate(\n    my_species = case_when(\n      my_kmeans == 'A' ~ species[3],\n      my_kmeans == 'B' ~ species[1], \n      TRUE ~ species[2]),\n    equal = case_when(my_species != Species ~ 0,\n                      TRUE ~ 1))\n\n# Plot highlighting difference\niris_binded %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour = factor(equal))) +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c('red', 'lightgray'), name = 'Difference', labels = c('different', 'equal')) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe red dots are the observations that our kmeans algorithm categorised differently compared to the original division into species. Overall, the result can be improved but it’s not too bad considering that the majority of points were correctly identified."
  },
  {
    "objectID": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "href": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "title": "You don’t need to be competitive if you know you’re competent",
    "section": "",
    "text": "In the past two months, I’ve been obsessed with the new album of Pinguini Tattici Nucleari. Every song deals with a different theme in a powerful way. I cried multiple times to Ricordi and its earthly depiction of dealing with a parent suffering from Alzheimer’s. I reflect on the concept of faith with Fede and the Italian socio-political situation with Coca Zero. However, Zen is the single I go back to over and over again.\nThe reason for this is simple. It describes something I often feel, being often worried about the future and looking for a moment of Zen. But one sentence got stuck in my mind, and it has nothing to do with my psychological state and everything with academia.\n\n Essere competitivo non serve se sai di еssere competеnte\n\n\nWhich translates to:\n\nYou don’t need to be competitive if you know you’re competent\n\n\nIt’s just one sentence, a simple one. Still, it captures so well the academic environment, where people feel the need to show their knowledge, achievement and prizes just for the sake of establishing their position. Something exacerbated by the Universities themselves, putting people against each to obtain one position (often underpaid), a breadcrumb of notoriety in the field. So much so that we are still here having well-established academics that have to act as the “bad cop” in recruitment panels for the sake of it. Or you need to fight for years for an opportunity to become a clinical psychologist because, you know, we need psychologists, but only if they are elitists.\nBut do I need to take part in this? Do I need to play the game?\nNo, I don’t. I know my worth, I know my strengths and weaknesses. I don’t want to be competitive. It’s useless. It’s worthless. It’s damaging to others.\nA simple song, a verse sentence. Yet so much to think about."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "You don’t need to be competitive if you know you’re competent\n\n\n\n\n\n\n\nopinions\n\n\nacademia\n\n\n\n\nMusical reflections on academic competitiveness\n\n\n\n\n\n\nJune 8, 2023\n\n\nDaniele Scanzi\n\n\n\n\n\n\n  \n\n\n\n\nK means clustering\n\n\n\n\n\n\n\nR\n\n\ncoding\n\n\nmicrostates\n\n\n\n\nK means algortihm walk through presented at the lab meeting to explain this technique.\n\n\n\n\n\n\nOctober 3, 2022\n\n\nDaniele Scanzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/bayesian_stat_material/bayes_intro.html",
    "href": "projects/bayesian_stat_material/bayes_intro.html",
    "title": "Overview",
    "section": "",
    "text": "A student-led group to learn Bayesian statistics - For students by students\n\n\nThis project was initially born when a friend of mine, Dylan Taylor (PhD student at the University of Auckland), and I wanted to meet over lunch to work through the book Statistical Rethinking. Talking with other PhD students, we soon realised that many researchers are interested in applying bayesian stats. However, most students didn’t have the opportunity to learn this and feel like learning it alone can be too difficult and time-consuming - a feeling we share too. So, we expanded the group to other interested people, and we began a series of weekly meetups. During these, we go through the 2022 lecture series accompanying the book, we have discussions and work through the exercises together. The group has expanded to around 15 people and has become a great opportunity to learn something useful together and create new connections between like-minded people.\nIn this section, you can find the slides and material we create to provide direction during the meetups.\n\nThis project has now been awarded a Creating Connections Grant which will help to provide refreshment during the meetups"
  },
  {
    "objectID": "projects/bayesian_stat_material/slides_list.html",
    "href": "projects/bayesian_stat_material/slides_list.html",
    "title": "Slides",
    "section": "",
    "text": "Here you can find the link to the slides openly available on RPubs.\n\nIntroduction to Bayesian Stat\nBayes Theorem\nBayesian Engines"
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "",
    "text": "The idea for this document comes from Makoto’s useful EEGLAB code. The intention behind this document is to have a place to store code that I produce while working on my projects and that I find useful. Besides being a somewhat unstructured place to save code that I could use in the future, this document might be helpful for researchers interested in using R to analyse EEG data."
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#from-eegutils-to-tidy-data",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#from-eegutils-to-tidy-data",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "From eegUtils to tidy data",
    "text": "From eegUtils to tidy data\neegUtils is an R package developed by Matt Craddock that allows you to analyse EEG data in R. Currently, I pre-process data in Matlab with EEGLAB. Thus, I use eegUtils to load pre-processed and epoched data in R to conduct statistical analysis. Although eegUils is a handy package, and I love that I can easily use R to do EEG data analysis and plot scalp topographies, some aspects of this package do not align with my preferences. Specifically:\n\nThe voltage data is stored in wide format, with one column for each electrode\nTime information is stored separately from the EEG signal information\n\nConsequently, I find it easier to extract the data I care about (voltage, electrode names and time values) and store those in a long-format tibble. As I do this often, I wrote a function for this.\nNote that some of the functions you find in the other paragraphs require the data extracted from eegUtils using this function. I highlighted when this is the case\n\ntidy_signal_from_eegUtils <- function(eegUtilsDataset) {\n  \n  tidy_data <- eegUtilsDataset %>% \n  # Add time column\n  dplyr::mutate(time = signal[[\"timings\"]][[\"time\"]]) %>% \n  # Reorganise the dataset \n  dplyr::pivot_longer(cols      = !time,\n               names_to  = \"electrode\",\n               values_to = \"volt\") \n  \n  return(tidy_data)\n  }"
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#computing-erp-amplitudes",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#computing-erp-amplitudes",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "Computing ERP amplitudes",
    "text": "Computing ERP amplitudes\nThe following code presupposes that the EEG data is stored in a dataframe or tibble in long format. The dataset must contain three columns:\n\ntime: time in ms for each sample\nelectrode: electrode name\nvolt: the voltage value for each sample\n\nTo create such a dataset from a file loaded through eegUtils see working with eegUtils.\nThere are two common ways to compute an ERP amplitude: average amplitude and peak amplitude. Average amplitude involves averaging the microvolts of a signal over a specified time range (where the ERP is expected). The peak amplitude consists in finding the maximum value of a peak or the minimum value of a trough. Peaks and troughs are the ERPs. Remember that peak amplitudes have drawbacks, and the average amplitude should be preferred.\nHere are some functions to compute both values. I also provide a wrapper function that extract both average and peak amplitudes.\n\nExtracting time bins\nFor flexibility, we need a function to select the signal within a specific time bin. We will use this function to allow the user to define the time range where to extract the average and peak values.\n\n# Function to extract the signal in a defined time bin. The time bin \n# can be specified as:\n#     - c(min, max): with the minimum and max time in ms\n#     - max: only the maximum value in ms\nextract_bin_signal <- function(eegDataset, bin = NULL) {\n    # Time bin must be provided\n    if (is.null(bin)) {\n        stop(\"Time bin is missing.\")\n    }\n    # If min and max provided\n    if (length(bin == 2)) {\n        binned <- eegDataset %>% \n          # Group by electrode, so to extract signal for each electrode independently\n          dplyr::group_by(electrode) %>% \n          # Select the requested time range\n          dplyr::filter(time >= bin[1] & time <= bin[2]) %>% \n          dplyr::ungroup()\n    # If only the max is provided\n    } else if (length(bin == 1)) {\n        binned <- eegDataset %>%\n          # Group by electrode, so to extract signal for each electrode independently\n          dplyr::group_by(electrode) %>% \n          # Select the requested time range - from first time point to requested max\n          dplyr::filter(time <= bin) %>% \n          dplyr::ungroup()\n    # If the time range is not a vector of one or two values throw an error\n    } else {\n        stop(\"Bin must be a vector of one or two values\")\n    }\n}\n\n\n\nCompute average amplitude\nTo extract the average amplitude, we can simply compute the mean of the voltage for each electrode.\n\n# Define a function to find the average amplitude\n# Note that this function requires that the dataset contains the electrode and \n# volt columns \nfind_average_amplitudes <- function(eegDataset) {\n    # compute average amplitude for each electrode\n    return(eegDataset %>%\n               group_by(electrode) %>% \n               summarise(average_amplitude = mean(volt)) %>% \n               ungroup()\n    )\n    \n}\n\n\n\nCompute the peak amplitude\nThis function is quite complex. The complexity is driven by the fact that an ERP can be any positive or negative deflection in an EEG signal. For a positive deflection, we want to find its maximum value. For a negative deflection, we want to find the minimum value. To do this, we use the findpeaks function of the pracma package. As this function finds only the local peaks of a signal, to find the local minima we will multiply the signal by -1 to invert peaks and troughs.\nNote that this function is quite flexible as the user is able to select whether to extract only peak, only troughs or both amplitudes. Importantly, as a signal might have only peaks and not troughs, or vice versa, if one or the other is not found, an NA is produced.\n\n#|eval: false\n\nfind_peak_amplitudes <- function(eegDataset, peaks = TRUE, valleys = TRUE) {\n    \n    # Split eeg dataset into list of dataframes, each one containing the signal\n    # for one channel\n    data_list <- eegDataset %>% \n        dplyr::group_by(electrode) %>% \n        dplyr::group_split()\n    \n    # Extract only the signal to pass to findpeaks\n    data_volt       <- lapply(data_list, function(x) x[[\"volt\"]])\n    # Extract the electrode information to use later\n    list_electrodes <- sapply(data_list, function(x) x[[\"electrode\"]][[1]])\n    # Extract time vectors \n    list_times      <- sapply(data_list, function(x) x[[\"time\"]]) \n    \n    \n    ## If both peaks and valleys are requested\n    if (peaks & valleys) {\n        \n        # Find peak amplitudes, peak_idx, [start:stop]\n        peaks_info   <- sapply(data_volt, pracma::findpeaks, npeaks = 1)\n        # Find valleys ampl, peak_idx, [start:stop] - Here we use the function\n        # findpeaks as above. Thus, we need to invert the signal so that valleys\n        # become peaks and will be detected. Because of this, later we will \n        # invert the sign of the signal to revert to the original value\n        valleys_info <- sapply(lapply(data_volt, function(x) x*-1), pracma::findpeaks, npeaks = 1)\n        \n        # NOTE: peaks_info and valleys_info work well when the signal has peaks\n        # and valleys. However, as soon as a signal does not have one of those,\n        # then `findpeaks` returns NULL and `sapply` fails to produce a matrix and \n        # defaults to a list. This, in turn, produce an error in extracting the \n        # peak times. As a NULL list as no dimension, when we call \n        # `[[peaks_info[[2, x]]` (see later), we get an error. To avoid this, we\n        # capture any NULL list, convert it into a matrix of [1 x 4] NA, and \n        # convert this into what sapply would produce (i.e. a matrix). The size\n        # of the matrix reflect the output of findpeaks when a peak is found.\n        \n        # If sapply produced a list - thus we have NULL nested lists\n        if (is.list(peaks_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            peaks_info_no_null <- lapply(peaks_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            peaks_info <- sapply(peaks_info_no_null, function(x) x)\n        }\n        \n        # Repeat the same with valleys_info\n        if (is.list(valleys_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            valleys_info_no_null <- lapply(valleys_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            valleys_info <- sapply(valleys_info_no_null, function(x) x)\n        }\n        \n        # Extract peak times\n        peaks_time   <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(peaks_info[[2, x]])) NA_real_ else list_times[[peaks_info[[2, x]], x]])\n        valleys_time <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(valleys_info[[2, x]])) NA_real_ else list_times[[valleys_info[[2, x]], x]])\n        \n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"peak\"        = peaks_info[1, ],\n            \"valley\"      = -valleys_info[1, ], # Revert the sign for the valleys\n            \"peak_time\"   = peaks_time,\n            \"valley_time\" = valleys_time\n            )\n        )\n    # If only peaks are requested\n    } else if (peaks & !valleys) {\n      # Find peaks ampl, peak_idx, [start:stop]\n        peaks_info   <- sapply(data_volt, pracma::findpeaks, npeaks = 1)\n        \n        # If sapply produced a list - thus we have NULL nested lists\n        if (is.list(peaks_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            peaks_info_no_null <- lapply(peaks_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            peaks_info <- sapply(peaks_info_no_null, function(x) x)\n        }\n        \n        # Extract times\n        peaks_time   <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(peaks_info[[2, x]])) NA_real_ else list_times[[peaks_info[[2, x]], x]])\n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"peak\"        = peaks_info[1, ],\n            \"peak_time\"   = peaks_time\n            )\n        )\n    # If only troughs are requested\n    } else if (!peaks & valleys) {\n        # Find valleys ampl, peak_idx, [start:stop] - Here we use the function\n        # findpeaks as above. Thus, we need to invert the signal so that valleys\n        # become peaks and will be detected. Because of this, later we will \n        # invert the sign of signal to revert to the original value\n        valleys_info <- sapply(lapply(data_volt, function(x) x*-1), pracma::findpeaks, npeaks = 1)\n        \n        # Repeat the same with valleys_info\n        if (is.list(valleys_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            valleys_info_no_null <- lapply(valleys_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            valleys_info <- sapply(valleys_info_no_null, function(x) x)\n        }\n        \n        # Extract times\n        valleys_time <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(valleys_info[[2, x]])) NA_real_ else list_times[[valleys_info[[2, x]], x]])\n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"valley\"      = -valleys_info[1, ], # Revert the sign for the valleys\n            \"valley_time\" = valleys_time\n            )\n        )\n    # if peaks and valleys argument are set both to FALSE\n    } else {\n        stop(\"At least one argument between 'peaks' and 'valleys' must be TRUE\")\n    }\n    \n    # Return the result tibble\n    return(peaks_amplitudes)\n    \n}\n\n\n\nWrapper function for ERP amplitudes\nThe above functions can be used separately. However, it could be handy to have a wrapper function that allows the user to compute both peak and average amplitudes simultaneously. Again, the following function is highly flexible, allowing the user to:\n\nDefine a time bin where to compute the amplitude measures. If no bin is provided, the whole signal is used\nSelecting whether to extract the only peak, only troughs, only averages amplitude or any combination of those\n\n\n# We then put all together in a single function\nfind_erp_amplitudes <- function(eegDataset, bin = NULL, peaks = TRUE, valleys = TRUE, avg = TRUE) {\n    \n    # If a time bin is provided, call extract_bin_signal (see above) to extract \n    # only the signal wirthin the requested time range\n    if (!is.null(bin)) {\n        # Extract the signal in the defined bin\n        binned_signal <- extract_bin_signal(eegDataset, bin = bin)\n    # If no time bin is provided, use the whole signal\n    } else {\n        binned_signal <- eegDataset\n    }\n    \n    # If peaks, troughs and average amplitude are requested\n    if ((peaks | valleys) & avg) {\n        # Extract peak and average amplitudes\n        peak_amps     <- find_peak_amplitudes(binned_signal, peaks = peaks, valleys = valleys)\n        avrg_amps     <- find_average_amplitudes(binned_signal)\n    \n        amplitudes    <- merge(peak_amps, avrg_amps)\n    # If only peak and troughs amplitudes are requested\n    } else if ((peaks | valleys) & !avg) {\n        amplitudes <- find_peak_amplitudes(binned_signal, peaks = peaks, valleys = valleys)\n    # If only troughs and average amplitude are requested\n    } else if (!(peaks | valleys) & avg) {\n        amplitudes <- find_average_amplitudes(binned_signal)\n    # If every amplitude argument is set to FALSE\n    } else {\n        stop(\"You need to provide at least one argument between peaks, valleys or avg\")\n    }\n\n    return(amplitudes)\n}"
  },
  {
    "objectID": "projects/projects_intro.html",
    "href": "projects/projects_intro.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a collection of material related to different projects I have been working on, either alone or in collaboration with other people."
  },
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html",
    "href": "projects/psychopy_code/coloured_gabor_patches.html",
    "title": "Psychopy useful functions",
    "section": "",
    "text": "The following document contains a collection of functions created to run experiments in Psychopy."
  },
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "href": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "title": "Psychopy useful functions",
    "section": "Creating coloured gratings (pseudo-Gabor).",
    "text": "Creating coloured gratings (pseudo-Gabor).\nfrom psychopy import visual, event, monitors, tools\nfrom psychopy.visual import filters\nfrom psychopy.tools import monitorunittools as mut\nimport numpy as np\nimport math\nimport os\n\n###############################################################################\n#                                VARIABLES\n###############################################################################\n\nSAVE_IMAGE = True             #Set to true if you want to output an image\nDIFFERENT_BRIGHTNESS = True   #Should the stimuli have different luminance?\nSWITCH_SIDE = True            #Invert the left and right grating\nWHAT_TO_DRAW = \"right\"        #Either \"both\", \"left\" or \"rigth\"\n# Parameters for the gratings\nparam_stim = {\n    \"resolution\": 5,           #Size of the stimulus grating in deg (this will be coverted in pix later)\n    \"mask_resolution\": 2**11,  #Resolution of the mask used to render the gratings as circles (must be a power of 2)\n    \"ori_left\": 45,            #Orientation of the first grating\n    \"ori_right\": -45,          #Orientation of the second grating \n    \"pos_left\": (0, 0),        #Position of the first grating (0,0 is the center of the screen)\n    \"pos_right\": (0, 0),       #Position of the second grating (0,0 is the center of the screen)\n    \"cycles\": 4*5,             #Spatial frequency of the gratings. This should be resolution X cycles per deg\n    \"vergence_cycles\": 5,      #Spatial frequency of the gratings used to create the vergence patterns\n    \"vergence_sf\": 0.03,       #This value controls the number of gratings used in the vergence patterns (use values < 0.5)\n    \"alpha_left\": 1,\n    \"max_value_first\": -0.3 #Red: Psychopy [-0.0030, -1,-1], RGB [89,0,0], HSV[0,100,35]\n}\n\n# Screen and window parameters - for Psychopy\nparam_pc = {\n    \"resolution\": (1920, 1080),\n    \"width\": 34.2}\n\n# Directories and file names for the image output\nthis_dir = os.path.dirname(os.path.abspath(__file__))\nimage_name = \"red_single_03-1-1.png\"  #Name of the file to output at the end\n\n###############################################################################\n#                                   WINDOW\n###############################################################################\n\n# Create monitor and windows\nmon = monitors.Monitor(\n    name=\"desk_monitor\",\n    width=param_pc[\"width\"],\n    distance=57\n)\nmon.setSizePix = param_pc[\"resolution\"]\n\nwin = visual.Window(\n    size=param_pc[\"resolution\"],\n    monitor=mon,\n    units=\"pix\",\n    allowGUI=False,\n    screen=1,\n    fullscr=False,\n    color=(-1, -1, -1),\n    colorSpace='rgb',\n    blendMode='avg',\n    winType='pyglet',\n    useFBO=True)\n\n\n###############################################################################\n#                                  FROM DEG TO PIX\n###############################################################################\n\n# Convert to pix\nparam_stim[\"resolution\"] = int(mut.deg2pix(param_stim[\"resolution\"], mon))\n# Round pix to the closest power of 2. NOTE this works for \"low\" values but \n# cannot be generalized to high values (eg. 100000). However, here we work with \n# values in the 100 range (eg. 256 pix).\nparam_stim[\"resolution\"] = 2**round(math.log2(param_stim[\"resolution\"]))\n\n###############################################################################\n#                                    STIMULI\n# To create the gratings we start by creating a black texture defined as a \n# matrix of dimension [dim1, dim2, 3], where the three layers represent the RGB\n# colours. Then, we will replace the layer of the colour we are interested in \n# with a grating, which is a [dim1, dim2] array, conatining values from -1 to 1 \n# representing the intensity of the colour. Doing this will create a grating \n# stimulus of the desired colour.\n# For the red stimulusg, we are interested in manipulating its brightness. To do \n# so, we define a colour in HSV space and convert it into RGB (use tool online)\n# Then we modify the grating range, so that it goes from -1 (black) to N, where\n# N is the values obtained online\n###############################################################################\n\n# Switch side if requested\nif SWITCH_SIDE:\n    pos_left  = param_stim[\"pos_left\"]\n    pos_right = param_stim[\"pos_right\"]\n    param_stim[\"pos_left\"] = pos_right\n    param_stim[\"pos_right\"] = pos_left\n    \n# ---Coloured Gabors---#\n\n# Create a black texture for both stimuli...\ngrating_left = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\ngrating_right = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\n\n# GREEN --> For the green stimulus we simply overaly the grating to the G channel\ngrating_right[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                              ori=param_stim[\"ori_right\"],\n                                              cycles=param_stim[\"cycles\"],\n                                              gratType=\"sin\")\n\n\n\n# RED --> For the red stimulus we need to do some more work...\n\n# Create a grating\nsin_mask = filters.makeGrating(res=param_stim[\"resolution\"],\n                                            ori=param_stim[\"ori_left\"],\n                                            cycles=param_stim[\"cycles\"],\n                                            gratType=\"sin\")\n\n# If different luminance is requested\nif DIFFERENT_BRIGHTNESS:\n    # Scale only positive values to change the colour (RED) but not the black through the Rohan's transform\n    # NOTE: it's not a real transform...it was a tip from a friend\n    scale_factor = 0.5*(param_stim[\"max_value_first\"]+1)\n    sin_mask_scaled = scale_factor * (sin_mask + 1) - 1\n   \n    grating_left[:,:,0] = sin_mask_scaled\n# If no difference in brightness is required, apply the grating as above\nelse:\n    grating_left[:, :, 0] = sin_mask\n\n\n#---Vergence Gratings---#\n\n# Create a gray texture (Psychopy [0,0,0] is gray)...\ngrating_vergence = np.zeros((param_stim[\"resolution\"], param_stim[\"resolution\"], 3))\n\n#...then overimpose a grid on all the three RGB channels\ngrating_vergence[:, :, 0] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 2] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\n\n#---Circle Mask---#\n\n# Generate a nice smooth (at least almost) circle mask\nmask = filters.makeMask(matrixSize=param_stim[\"mask_resolution\"],\n                        shape=\"circle\")\n\n#---Generate Stimuli with Psychopy---#\n\n# left grating stimulus\nstim_left = visual.GratingStim(\n    name=\"stimL\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_left,\n    mask=mask,\n    units=\"pix\")\n# Right grating stimulus\nstim_right = visual.GratingStim(\n    name=\"stimR\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_right,\n    mask=mask,\n    units=\"pix\")\n# Left vergence pattern\nvergence_left = visual.GratingStim(\n    name=\"vergL\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n# Right vergence pattern\nvergence_right = visual.GratingStim(\n    name=\"vergR\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n\n# Left fixation dot\nfixation_left = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_left\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\nfixation_right = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_right\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\n\n###############################################################################\n#                                    DRAW\n###############################################################################\n\nif WHAT_TO_DRAW == \"both\":\n# Draw stimuli on buffer\n    vergence_left.draw()\n    vergence_right.draw()\n    stim_left.draw()\n    stim_right.draw()\n    fixation_left.draw()\n    fixation_right.draw()\nelif WHAT_TO_DRAW == \"left\":\n    vergence_left.draw()\n    stim_left.draw()\n    fixation_left.draw()\nelse:\n    vergence_right.draw()\n    stim_right.draw()\n    fixation_right.draw()\n\n# Present stimuli on the window\nwin.flip()\n# Save stimuli if requested\nif SAVE_IMAGE:\n    frame = win.getMovieFrame()\n    frame.save(os.path.join(this_dir, image_name))\n# Terminate when a key is pressed\nevent.waitKeys()\n# Close the Psychopy window\nwin.close()"
  },
  {
    "objectID": "educating/students_review.html",
    "href": "educating/students_review.html",
    "title": "Students Review",
    "section": "",
    "text": "When I started my PhD, I have also begun tutoring every semester. I enjoy teaching the tutorials, interacting with the students and sharing with them the knowledge I am passionate about. I often think about how to be a better educator, how to set up my tutorials and how to create a positive environment in my classes. I like to look at teaching through the lens of my past as a magician. Teaching is an act with a story to convey, attention to grab, and audience participation should be encouraged.\nMy idea of teaching may be far away from what the students want and need, I know. Unfortunately, the courses I teach do not ask students to provide feedback on their tutors, and I find this absurd. We are an integral part of their education experience, we should be able to know how we are doing and what we should work on. So, I have been actively collecting students’ evaluations for all the courses I teach. I do this at the end of each semester through an anonymous survey that students can voluntarily fill out.\nIn the name of transparency, I have decided to post all the anonymous feedback here. I will update this page each semester, and I will address the comments and reflect on what I need to work on.\nAs a side note, I try to encourage students to provide constructive feedback, and I try to stress that negative feedback is more than welcome. I really want to improve and work on how to be better, and negative feedback, if constructive, helps highlight things to change and work on. As such, I won’t hide any comments except if they are un-constructively negative or contain some sensitive information. I will add a note if this happens.\nOne of the survey’s question is to provide 3 words that describe me as a tutor. Above is a summary of the words provided so far"
  },
  {
    "objectID": "educating/students_review.html#students-marks",
    "href": "educating/students_review.html#students-marks",
    "title": "Students Review",
    "section": "Students’ marks",
    "text": "Students’ marks\nLet’s start with a simple and fun (hopefully) section where I swap the roles with the students and they are able to grade my work.\n\n\n\n\n\n\n\n\nHappy to see that there is nothing in Below Average and that the way I am currently running the tutorials seems to be appreciated.\nIn terms of marks, the grades below are scaled by their relative proportion. The larger the letter, the higher the number of students that selected that grade. Again, I’m happy to see the A+ and A being large!"
  },
  {
    "objectID": "educating/educating_intro.html",
    "href": "educating/educating_intro.html",
    "title": "Educating",
    "section": "",
    "text": "One of my favorite aspects of doing a PhD is teaching. I enjoy the educational aspect of academia, and I focus a lot on how to be a better educator. In this section, you’ll find a set of materials, projects, and data all related to my experience as an educator."
  },
  {
    "objectID": "educating/students_review.html#how-students-describe-me",
    "href": "educating/students_review.html#how-students-describe-me",
    "title": "Students Review",
    "section": "How students describe me",
    "text": "How students describe me"
  },
  {
    "objectID": "educating/students_review.html#what-students-appreciate",
    "href": "educating/students_review.html#what-students-appreciate",
    "title": "Students Review",
    "section": "What students appreciate",
    "text": "What students appreciate\nNow get to the core of the feedback. I will highlight at the top the comments that I think represent the major ideas. Nonetheless, at the end of the section you can find a exahustive table with all the comments.\n\nYou were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping.\n\n\nYou did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it.\n\n\nTutorials are fun and friendly environments.\n\nThere are three main elements I think students appreciate:\n1- The content should be presented differently from the lecturers. This makes the tutorial feel less like a simple repetition and more like an opportunity to learn more and/or understand better.\n2- Give people time to ask questions. We all vary with how comfortable we are in asking questions in class. I encourage discussions, debates and questions since the very first tutorial to create an environment where everyone feels comfortable being an active part.\n3- Engage. We all agree that listening to one person talking for two hours is boring. So why do this? Showing a more fun and natural side helps create a connection, keep the attention high and make the tutorial time more enjoyable.\n\nI felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good.\n\nI agree on this point. One of the aspects I need to work more on is structure (more on this in the next session). I do not want to have everything predefined, as this is counterproductive, but I will try to define at the beginning of the tutorial the key areas I will discuss.\n\n\n\n\n \n  \n    Feedback \n  \n \n\n  \n    Interactiveness, enthusiasm and motivation \n  \n  \n    I really loved your passion and the time you took to check everybody understood. Also appreciated when you would draw diagrams to help us understand when we were all struggling to understand. \n  \n  \n    I enjoyed the activities we did!! It was fun especially with how the content was pretty full on but the fact we got to discuss often with our peers was a good aspect for me. I also liked how you went through the content first explaining everything we similarly learnt in lectures before we started because hearing it from the way you talked about it helped me understand lecture content a bit easier. And the white board examples and the pictures helped immensely!!!! It made things so much easier to grasp so please continue that because reading of PowerPoint slides for me personally doesn’t feel engaging \n  \n  \n    Engaged with smaller group discussions by walking around the room \nAnswered questions directly and was ready to do so in the middle of a presentation \n  \n  \n    Tutorials felt well prepared and always were well delivered, the content was explained clearly and sometimes having clarification of lecture material was super helpful. Your delivery was engaging and even content I'm not super into wasn't boring so thank you! You seem to be pretty into the topics that we covered which really helps. The feedback on the essay was pretty insightful too. \n  \n  \n    Trying to engage the class rather than lecturing, despite a lack of input form us 😆 \n  \n  \n    All content was explained clearly, and questions from the class were answered swiftly and very well. The way we walked through different programmes was extremely helpful for understanding the way they work and how data is arranged and understood. \n  \n  \n    You were fun and engaging, incredibly knowledgeable about the topics you were teaching, spent time with us one-on-one or in pairs when able and provided individual feedback + guidance \n  \n  \n    Constructive feedback allowing us to critically think \n  \n  \n    Always trying to make things fun and using interactive ways of learning, checking that everyone is understanding was really nice \n  \n  \n    I find it really helpful that you go in depth into the concepts as it helps me understand more. i also really appreciate in the tips you provide in our academic writing etc finding sources, i find them really helpful :) \n  \n  \n    I appreciated the quality of the content and how it was useful and complementary to the lectures \n  \n  \n    Everything is explained very well, you’re very patient with answering questions no matter how many are asked or if you get interrupted, you know the content well and relate it to us in a very simple and good way as well, you give us a very nice breakdown of how to write things and what to include in assignments (for the TBI essay) and in general you make it a very nice environment to be in and your chill/relaxed and funny vibe makes the tutorials far more engaging \n  \n  \n    Asked us questions that helped go to actively revise. \n  \n  \n    Support! You were very supportive and accurately explained what was required to do well in this class, and explained what we did not need to know as extensively - which was much appreciated as often times labs can be seen as just an overload of information. \n  \n  \n    i liked how you explained things in a different way to the lecturers which often made more sense to me. or having the content presented in a different way deepened my understanding of it. i liked how you took the time to explain something fully to me again if i still didn’t understand it \n  \n  \n    Giving supports to every student, really engaging with the class \n  \n  \n    tutorials are fun and friendly environments \n  \n  \n    Literally so fun for 9am on a Friday! You are just so engaging and knowledgeable and really get us thinking. Thanks for being such a legend! I really enjoy these labs \n  \n  \n    Always try to make tutorials fun, friendly demeanour in general, and genuinely trying to help us, thank you! \n  \n  \n    Always have interesting teaching way \n  \n  \n    Good at explaining thoroughly and gives a real opportunity for students to ask questions. Also answers questions as best as possible. Takes time to check in on each student/group individually and welcoming to ask questions to. Seems like you genuinely want to help your students understand everything clearly and do well. \n  \n  \n    You have always been very transparent and supportive of us. You helped us with everything you could and always gave us the right advice, pushing us to do so much more. The feedback on our assignments has been very thorough which also personally has helped me to incorporate into assignments from my other courses. \n  \n  \n    Support was great! feedback on the assignments was super helpful \n  \n  \n    Really encouraging and great with positive feedback. I really liked the feedback I received on my assignments throughout the course as it guided me as to how to improve in my work. \n  \n  \n    Explained alot of concepts and answers thoroughly and appreciated that he took the time to go through everything. Feedback felt subjective and catered to our learning \n  \n  \n    I felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good. \n  \n  \n    The feedback you gave on my blog post was super helpful! I didn't realise how much blog was lacking something big and your feedback helped me see it but you also provided specific tips to integrate so I didn't feel lost knowing I had alot of work to improve on :) You were also super on top of replying in teams and being there for support so that was something else that made you such an awesome tutor. \n  \n  \n    I appreciate how friendly and easy to approach you were. Unlike some other tutors, you really made it easy for us to ask questions - esp questions that we found hard to ask to other lecturers. Assignment feedbacks were extremely detailed and helpful and also very fair I personally think. I also like how chill you were - as long as we followed the rubric and did everything, you were very fair with your marking - literally made capstone life extremely enjoyable and rewarding to have a tutor who understood what you were doing! \n  \n  \n    Friendly manner, always willing to help \n  \n  \n    Feedback on assignments are good, you take the time to listen and help and make sure we understand \n  \n  \n    Very helpful, compassionate, supportive and understanding, explained really well \n  \n  \n    Explaining things throughly, questioning us and making us work things through \n  \n  \n    Good feedback & ways of trying to improve our learning. \n  \n  \n    You were very helpful and understanding of our places in knowledge. Your feedback was helpful in allowing for a better understanding. \n  \n  \n    The content covered was always relevant and helpful. Trying to engage the class \n  \n  \n    You were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping. \n  \n  \n    Appreciated how patient you were, especially re-explaining something over and over again so our class understood. Your feedback and also going over and beyond. \n  \n  \n    i appreciated fast reply’s on email, and also giving feedback on assignments and work. friendly work environment \n  \n  \n    You did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it."
  },
  {
    "objectID": "educating/students_review.html#what-students-think-i-should-do-better",
    "href": "educating/students_review.html#what-students-think-i-should-do-better",
    "title": "Students Review",
    "section": "What students think I should do better",
    "text": "What students think I should do better\n\nSometimes the discussion time in tutorials felt too long and often we’d finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway. Occasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes.\n\nI will try next year to have more class discussions and less small group works. A couple more people pointed this out. I believed that small groups would have encouraged people to talk more by reducing the pressure of having to speak out loud in front of everyone. However, doing this does take time that would could use in a different way.\n\nFound the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc.\n\nThis is a fair point. I almost always take the full 2 hours most likely because I like to digress. As pointed out above, next year I will try to structure each tutorial a bit more. Specifically, I will highlight at the beginning the major point we will discuss and I will add a recap at the end. However, I will keep some improvisation. I believe that by having just a general structure of what to talk allows for exploration of topics, concepts and ideas that otherwise won’t be addressed. Tutorials, in my view, are not just a recap of the lectures, but a moment where we can dive deeper on a topic or create connections across different areas. They are useful to broader our understanding of the concepts discussed in the lectures and by doing so, learning the material better.\n\nWould appreciate more concise marking, and felt marking scores were abit harsh sometimes compared to other students or the feedback received.\n\nGiven the multiple comments appreciating the lengthy feedback on the essays, I will not plan to change this. Assignments and exams are not just a test, but they are an opportunity to improve and gain skills. I believe that providing detailed feedback students are able to improve their future work. I agree that I can be strict with my marking. This is partly caused by my study background and the way I have been assessed. Partly because I try to mark focusing hardly on the reasoning. That is, I don’t see just repeating the content from a book or lectures are being enough for a A+. The reasoning behind an answer is important. Nonetheless, in the class I teach us tutors try to match our criteria, so if I am being too harsh, other tutors and professors will call me out on this before the marks are finalized.\n\n\n\n\n \n  \n    answers \n  \n \n\n  \n    Nothing really \n  \n  \n    Only suggestion I have is to speak slower. Your excitement and English are amazing, but when you speak really fast with excitement, it made it difficult to understand and keep up sometimes. \n  \n  \n    This isn't really a you thing but it was better communication with the lecturers (not really your fault) as it's kind of disheartening when tutors don't know whata's going on with what lecturers are doing for example when lab quizzes results are released or even whats going on with exams. I also wish we got to discuss with more of our peers around the room, not just the people we sat with but I guess that’s kind of hard with the layout of the room \n  \n  \n    Sometimes skipped over points too quick. \n  \n  \n    Sometimes the discussion time in tutorials felt too long and often we'd finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway. \n  \n  \n    Occasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes \n  \n  \n    Honestly, thought you were amazing. Maybe you could just slow down sometimes with your explanations because it can be a bit hard to keep up with \n  \n  \n    Genuinely cannot think of anything sorry \n  \n  \n    I am happy with the way the tutorials are going :) \n  \n  \n    I don't believe there is anything you could do better :) keep up the great work! \n  \n  \n    Honestly nothing, the way you do things is amazing \n  \n  \n    Summarise the lab at the end \n  \n  \n    I think maybe the first few labs had a LOT of information to be taught and not so much time for questions to be asked. This isn’t so much a tutor specific issue, rather it is a course coordination issue of how much is taught in those labs \n  \n  \n    occasionally (not often) i couldn’t understand what you were saying when you spoke fast \n  \n  \n    some tutorials cover ground that has already been covered in previous years, such as essay writing basics, though this is likely more of an issue with the general course coordination \n  \n  \n    More magic please! \n  \n  \n    Keep doing what you're doing :) \n  \n  \n    Maybe speaking a little bit slower is better \n  \n  \n    More of feedback for tutorials in general rather than the tutor but Maybe, especially in the first lesson, have more focus on having students introduce themselves in the class and have opportunities to work together and discuss with each other so that students can build relationships to each other and to tutor. \n  \n  \n    I honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done. \n  \n  \n    I honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done. \n  \n  \n    communication could've been better, at times was difficult to get our points across as our research differed greatly from yours, which is completely okay but having a tutor who knows the topic and understands the specific research would have made it a wee bit easier for you and us. \n  \n  \n    Would appreciate more concise marking, and felt markinf scores were abit harsh sometimes compared to other students or the feedback received \n  \n  \n    Maybe provide more tips on what we should be careful of when doing each assignment - like providing common places where student often lose marks or mess up \n  \n  \n    Nothing much - everything was all good! Don't really have much feedback \n  \n  \n    deeper explanation of that weeks lecture content \n  \n  \n    I think you are good \n  \n  \n    More magic and pizza \n  \n  \n    Nothing I love this tutor \n  \n  \n    Don’t overestimate students understanding of topics \n  \n  \n    I think maybe just more in depth feedback and being super critical, even though people might not like it it’s actually very helpful. \n  \n  \n    Found the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc."
  },
  {
    "objectID": "educating/students_review.html#extra-comments",
    "href": "educating/students_review.html#extra-comments",
    "title": "Students Review",
    "section": "Extra comments",
    "text": "Extra comments\nAs my survey is meant to be quick, I leave the opportunity to express other ideas freely.\n\n\n\n\n \n  \n    answers \n  \n \n\n  \n    Great job! Thanks for being a great tutor! \n  \n  \n    Thanks so much for your help and enthusiasm! I found Psych 305 to be a really difficult class, but having you as a tutor helped me so much with the work content because your excitement made me get more excited about it, which helped me engage and learn better! I really loved having you teach us! All the best. :) \n  \n  \n    Thanks for a great semester!! \n  \n  \n    Thank you for teaching us over the past semester, and good luck with your PhD! \n  \n  \n    Great job. I personally hate tutorials, and you made them fun and engaging and made me look forward to coming :) \n  \n  \n    Thank you for a lovely lab! \n  \n  \n    I think you are a great tutor (: \n  \n  \n    thank you for all your hard work :) \n  \n  \n    Thank you! \n  \n  \n    I'm not actually in your normal stream but joined it once when I couldn’t attend my normal one and I enjoyed your tutorial so much that I made it my regular stream and have been coming along ever since \n  \n  \n    LOVE YOU DANIELE!!! You are an awesome tutor \n  \n  \n    you never gave us the pizza making class :( \n  \n  \n    Please keep tutoring! You're literally my fav! \n  \n  \n    Thank you for being such a lovely tutor this semester, I wish you luck on all your future eneavours. \n  \n  \n    No \n  \n  \n    Great mustache! And you did a great job giving students opportunity to ask questions individually (rather than to whole class) so felt more comfortable and open. Thanks! \n  \n  \n    Daniele is an amazing tutor and an amazing person. With great professionalism that can be seen on our assignments feedback and advice, he is super friendly and ensures that you are not being negatively criticised. He is highly supportive and always pushes you to perform to the best of your ability. \n  \n  \n    Daniele is an amazing tutor, who is thoughtful and empathetic. He is a great help to those who are stuck and need a direction. \n  \n  \n    Daniele was my tutor for my year 3 capstone course of my degree. He was an amazing communicator who was ways able to convey feedback in an depth manner - because of this I was motivated improve my work rather than seeing it as a daunting task. He also always came to meetings with a positive attitude and willingness to help! \n  \n  \n    Daniele is an extremely friendly tutor. Very thoughtful and always eager to help his students. His tutorial sessions were always helpful and his assignment feedbacks were very detailed and accurate. Would love to have him as my tutor for other classes too. \n  \n  \n    You are a great tutor. Continue to do the awesome work of teaching your students. \n  \n  \n    Daniele is very calm in nature and provides a positive environment that allows for learning and is very entertaining when adding his magic into his tutorials. \n  \n  \n    Daniele was a great tutor to have as he was always so nice to everyone and helpful when questions were asked. I learnt a lot in his class and it made me enjoy going to my tutorial because he was a good teacher."
  }
]