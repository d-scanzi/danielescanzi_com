[
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html",
    "href": "projects/psychopy_code/coloured_gabor_patches.html",
    "title": "Psychopy useful functions",
    "section": "",
    "text": "The following document contains a collection of functions created to run experiments in Psychopy."
  },
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "href": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "title": "Psychopy useful functions",
    "section": "Creating coloured gratings (pseudo-Gabor).",
    "text": "Creating coloured gratings (pseudo-Gabor).\nfrom psychopy import visual, event, monitors, tools\nfrom psychopy.visual import filters\nfrom psychopy.tools import monitorunittools as mut\nimport numpy as np\nimport math\nimport os\n\n###############################################################################\n#                                VARIABLES\n###############################################################################\n\nSAVE_IMAGE = True             #Set to true if you want to output an image\nDIFFERENT_BRIGHTNESS = True   #Should the stimuli have different luminance?\nSWITCH_SIDE = True            #Invert the left and right grating\nWHAT_TO_DRAW = \"right\"        #Either \"both\", \"left\" or \"rigth\"\n# Parameters for the gratings\nparam_stim = {\n    \"resolution\": 5,           #Size of the stimulus grating in deg (this will be coverted in pix later)\n    \"mask_resolution\": 2**11,  #Resolution of the mask used to render the gratings as circles (must be a power of 2)\n    \"ori_left\": 45,            #Orientation of the first grating\n    \"ori_right\": -45,          #Orientation of the second grating \n    \"pos_left\": (0, 0),        #Position of the first grating (0,0 is the center of the screen)\n    \"pos_right\": (0, 0),       #Position of the second grating (0,0 is the center of the screen)\n    \"cycles\": 4*5,             #Spatial frequency of the gratings. This should be resolution X cycles per deg\n    \"vergence_cycles\": 5,      #Spatial frequency of the gratings used to create the vergence patterns\n    \"vergence_sf\": 0.03,       #This value controls the number of gratings used in the vergence patterns (use values &lt; 0.5)\n    \"alpha_left\": 1,\n    \"max_value_first\": -0.3 #Red: Psychopy [-0.0030, -1,-1], RGB [89,0,0], HSV[0,100,35]\n}\n\n# Screen and window parameters - for Psychopy\nparam_pc = {\n    \"resolution\": (1920, 1080),\n    \"width\": 34.2}\n\n# Directories and file names for the image output\nthis_dir = os.path.dirname(os.path.abspath(__file__))\nimage_name = \"red_single_03-1-1.png\"  #Name of the file to output at the end\n\n###############################################################################\n#                                   WINDOW\n###############################################################################\n\n# Create monitor and windows\nmon = monitors.Monitor(\n    name=\"desk_monitor\",\n    width=param_pc[\"width\"],\n    distance=57\n)\nmon.setSizePix = param_pc[\"resolution\"]\n\nwin = visual.Window(\n    size=param_pc[\"resolution\"],\n    monitor=mon,\n    units=\"pix\",\n    allowGUI=False,\n    screen=1,\n    fullscr=False,\n    color=(-1, -1, -1),\n    colorSpace='rgb',\n    blendMode='avg',\n    winType='pyglet',\n    useFBO=True)\n\n\n###############################################################################\n#                                  FROM DEG TO PIX\n###############################################################################\n\n# Convert to pix\nparam_stim[\"resolution\"] = int(mut.deg2pix(param_stim[\"resolution\"], mon))\n# Round pix to the closest power of 2. NOTE this works for \"low\" values but \n# cannot be generalized to high values (eg. 100000). However, here we work with \n# values in the 100 range (eg. 256 pix).\nparam_stim[\"resolution\"] = 2**round(math.log2(param_stim[\"resolution\"]))\n\n###############################################################################\n#                                    STIMULI\n# To create the gratings we start by creating a black texture defined as a \n# matrix of dimension [dim1, dim2, 3], where the three layers represent the RGB\n# colours. Then, we will replace the layer of the colour we are interested in \n# with a grating, which is a [dim1, dim2] array, conatining values from -1 to 1 \n# representing the intensity of the colour. Doing this will create a grating \n# stimulus of the desired colour.\n# For the red stimulusg, we are interested in manipulating its brightness. To do \n# so, we define a colour in HSV space and convert it into RGB (use tool online)\n# Then we modify the grating range, so that it goes from -1 (black) to N, where\n# N is the values obtained online\n###############################################################################\n\n# Switch side if requested\nif SWITCH_SIDE:\n    pos_left  = param_stim[\"pos_left\"]\n    pos_right = param_stim[\"pos_right\"]\n    param_stim[\"pos_left\"] = pos_right\n    param_stim[\"pos_right\"] = pos_left\n    \n# ---Coloured Gabors---#\n\n# Create a black texture for both stimuli...\ngrating_left = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\ngrating_right = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\n\n# GREEN --&gt; For the green stimulus we simply overaly the grating to the G channel\ngrating_right[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                              ori=param_stim[\"ori_right\"],\n                                              cycles=param_stim[\"cycles\"],\n                                              gratType=\"sin\")\n\n\n\n# RED --&gt; For the red stimulus we need to do some more work...\n\n# Create a grating\nsin_mask = filters.makeGrating(res=param_stim[\"resolution\"],\n                                            ori=param_stim[\"ori_left\"],\n                                            cycles=param_stim[\"cycles\"],\n                                            gratType=\"sin\")\n\n# If different luminance is requested\nif DIFFERENT_BRIGHTNESS:\n    # Scale only positive values to change the colour (RED) but not the black through the Rohan's transform\n    # NOTE: it's not a real transform...it was a tip from a friend\n    scale_factor = 0.5*(param_stim[\"max_value_first\"]+1)\n    sin_mask_scaled = scale_factor * (sin_mask + 1) - 1\n   \n    grating_left[:,:,0] = sin_mask_scaled\n# If no difference in brightness is required, apply the grating as above\nelse:\n    grating_left[:, :, 0] = sin_mask\n\n\n#---Vergence Gratings---#\n\n# Create a gray texture (Psychopy [0,0,0] is gray)...\ngrating_vergence = np.zeros((param_stim[\"resolution\"], param_stim[\"resolution\"], 3))\n\n#...then overimpose a grid on all the three RGB channels\ngrating_vergence[:, :, 0] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 2] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\n\n#---Circle Mask---#\n\n# Generate a nice smooth (at least almost) circle mask\nmask = filters.makeMask(matrixSize=param_stim[\"mask_resolution\"],\n                        shape=\"circle\")\n\n#---Generate Stimuli with Psychopy---#\n\n# left grating stimulus\nstim_left = visual.GratingStim(\n    name=\"stimL\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_left,\n    mask=mask,\n    units=\"pix\")\n# Right grating stimulus\nstim_right = visual.GratingStim(\n    name=\"stimR\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_right,\n    mask=mask,\n    units=\"pix\")\n# Left vergence pattern\nvergence_left = visual.GratingStim(\n    name=\"vergL\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n# Right vergence pattern\nvergence_right = visual.GratingStim(\n    name=\"vergR\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n\n# Left fixation dot\nfixation_left = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_left\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\nfixation_right = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_right\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\n\n###############################################################################\n#                                    DRAW\n###############################################################################\n\nif WHAT_TO_DRAW == \"both\":\n# Draw stimuli on buffer\n    vergence_left.draw()\n    vergence_right.draw()\n    stim_left.draw()\n    stim_right.draw()\n    fixation_left.draw()\n    fixation_right.draw()\nelif WHAT_TO_DRAW == \"left\":\n    vergence_left.draw()\n    stim_left.draw()\n    fixation_left.draw()\nelse:\n    vergence_right.draw()\n    stim_right.draw()\n    fixation_right.draw()\n\n# Present stimuli on the window\nwin.flip()\n# Save stimuli if requested\nif SAVE_IMAGE:\n    frame = win.getMovieFrame()\n    frame.save(os.path.join(this_dir, image_name))\n# Terminate when a key is pressed\nevent.waitKeys()\n# Close the Psychopy window\nwin.close()"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html",
    "title": "EEG preprocessing functions",
    "section": "",
    "text": "A collection of functions and code-snippets useful for EEG preprocessing with EEGLAB (Matlab). Note, I distribute these functions under the GNU General Public License. I removed the statement to slim the code and the page. However, if you use or modify any of these functions in your projects, please add the GNU licence statement at the end of the function help. You can find the statement at the end of this page."
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#counting-triggers",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#counting-triggers",
    "title": "EEG preprocessing functions",
    "section": "Counting triggers",
    "text": "Counting triggers\nThis function takes an EEGLAB EEG structure and returns a cell array containing the number of triggers (aka events) divided for each trigger type. I use it as a sanity check to ensure that my data does not contain missing triggers and that the participant completed the correct number of trials/blocks.\n% nTrig = count_triggers(EEG)\n%\n%    Count the number of unique triggers contained in the EEG.event \n%    structure.\n%\n% In:\n%    EEG   - An EEGLAB data structure\n%\n% Out:\n%    nTrig - Cell array containing the unique trigger names and how many\n%            times they occur in the recording.\n\n% Author: Daniele Scanzi\n\nfunction nTrig = count_triggers(EEG)\n\n    % Check that event structure is in EEG structure\n    if ~isfield(EEG, \"event\")\n        error(\"Cannot find event structure. Is EEG an EEGLAB structure?\")\n    end\n    \n    % Find unique events\n    nTrig = unique({EEG.event.type})';\n    if isempty(nTrig)\n        error(\"Cannot find any events in the event structure. Do you need to load them?\")\n    end\n\n    % Create cell array storing name of event and their number\n    for iEvent = 1:size(nTrig,1)\n        nTrig{iEvent, 2} = sum(strcmp({EEG.event.type}, nTrig{iEvent, 1}));\n    end\n\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#add-extra-trigger-information",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#add-extra-trigger-information",
    "title": "EEG preprocessing functions",
    "section": "Add extra trigger information",
    "text": "Add extra trigger information\nThis function is useful if you have triggers represented as codes (eg. T100) and you would like to add their meaning in the EEG.event structure. However, you can add any other information.\n% EEG = add_trigger_info(EEG, trigInfo, varargin)\n%\n%    Add extra information to the EEG.event structure. It is useful to add\n%    trigger information for each trigger. For instance, you can add the\n%    meaning of each trigger if trigger codes have been used.\n%\n% In:\n%    EEG      - An EEGLAB data structure\n%    trigInfo - Cell array of size NxM. N should reflect the number of\n%               triggers for which extra information should be added. M\n%               should reflect the nuber of extra information to add. M new\n%               columns will be added to the EEG.event structure. M must be\n%               at least 2, where the first column should contain the\n%               names of the triggers (one for each row) and the second\n%               column contains the information to add. For instance, if\n%               you have three triggers and you want to add their meaning,\n%               you could pass a cell array such as:\n%               {'T100' 'start_recording'; \n%                'T150' 'pause_recording';\n%                'T200' 'stop_recording'}\n%\n% Optional:\n%     fieldName - Cell array containing the names of the new columns to add\n%                 to the EEG.event structure. The cell array should have\n%                 size of 1xM, where M is the number of the extra\n%                 information columns to add. M must match the number of\n%                 columns of trigInfo. If no names are provided, the new\n%                 columns will be named 'X1', 'X2',..., 'XM'\n%\n% Out:\n%    EEG        - Modified copy of the EEG structure\n\n% Author: Daniele Scanzi\n\nfunction EEG = add_trigger_info(EEG, trigInfo, varargin)\n\n    p = inputParser;\n    \n    % Mandatory\n    addRequired(p, 'EEG', @isstruct);     % EEG structure\n    addRequired(p, 'trigInfo', @iscell);  % Cell array containing tirgger info\n    \n    % Optional\n    addParameter(p, 'fieldName', [], @iscell);         % Name/s to provide to the fields to add\n    \n    % Parse user input\n    parse(p, EEG, trigInfo, varargin{:});\n\n    EEG = p.Results.EEG;\n    triggInfo   = p.Results.trigInfo;\n    fieldName   = p.Results.fieldName;\n\n    %% Checks\n   \n   if size(trigInfo, 2) &lt;= 1\n       error('triggerInfo should be a cell array with at least two columns: Trigger names - New field1 \\n');\n   end\n\n   % Create names for field if not provided, check otherwise\n   if isempty(fieldName)\n       fieldName = cell(size(trigInfo, 2) - 1, 1);\n       for iField = 1:( size(trigInfo, 2) - 1 )\n           fieldName{iField, 1} = strcat('X', num2str(iField));\n       end\n   else\n       if length(fieldName) ~= ( size(trigInfo, 2) - 1 )\n           error('fieldNames should match the number of new fields contained in trigInfo \\n');\n       end\n   end\n   \n   for iEvent = 1:length(EEG.event)\n       currentEvent       = EEG.event(iEvent).type;\n       currentTrigInfoIdx = find(strcmp(trigInfo(:, 1), currentEvent));\n\n       % Add info\n       for iInfo = 1:length(fieldName)\n           if isempty(currentTrigInfoIdx)\n               EEG.event(iEvent).(fieldName{iInfo}) = 'n/a';\n           else\n               EEG.event(iEvent).(fieldName{iInfo}) = trigInfo{currentTrigInfoIdx, iInfo+1};\n           end\n       end\n   end\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#compute-latencies-with-photosensor",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#compute-latencies-with-photosensor",
    "title": "EEG preprocessing functions",
    "section": "Compute latencies with photosensor",
    "text": "Compute latencies with photosensor\nIn our current EEG setup, we have a Brainproducts system where triggers are sent through a triggerbox. We also have a photosensor, which allows us to detect with precision when a stimulus is presented on the screen. Unfortunately, we do not have the StimTrack used to convert the photosensor data into triggers. So, we need to do this ourselves.\nThe photosensor data is stored as an extra channel in the EEG data. If the photosensor is set up to detect a switch from a dark to a light stimulus, then it creates a trace with a spike (NOTE: not a square wave) when the switch happens. The spike begins at the onset of the light stimulus and peaks just after that.\nI did not test this function with other systems, but I would assume it would work (grating that the photosensor records luminance and not just the change in luminance).\nBriefly, this function does the following: 1. Detect the photosensor spikes onsets 2. Add triggers at the detected onset latencies 3. For each trigger, check whether there is an associated photosensor trigger, accounting for the fact that the photosensor could appear before or after the associated trigger.\nThe order of photosensor-trigger seems to depend on how the experiment is coded and reflects whether the trigger lags behind the stimulus or not. I’m still discussing this point with friends, but we don’t have a conclusive explanation for this. Any insight on this is welcomed.\n% [eventLatenciesInfo, EEG] = trigger_photo_latency(EEG, eventNames, varargin)\n%\n%         Align the recorded triggers to the onset times recorded by\n%         Brainproducts' photosensor. Conversion follows the procedure:\n%         1. Extract data from photosensor\n%         2. Find onset of spikes - spikes reflect changes in contrast\n%         3. Add found onsets to EEG.event structure in chronological order\n%         4. For eah trigger, find closest spike (within defined tolerance)\n%         5. Convert the original onset time of each trigger to its\n%            assciated spike onset time\n%\n% In:\n%    EEG        - An EEGLAB data structure\n%    eventNames - Cell array of strings represenitng the names of the\n%                 triggers to align\n%\n% Optional:\n%    photoTrigger       - Name to provide to the triggers representing the\n%                         photosensor event onsets. Default: 'photo'\n%    photoChannel       - Channel number (int) or channel name (string) of \n%                         the channel containing the photosensor data\n%    normalisePhotoData - Logical (true|false). Whether to normalise the \n%                         photosensor data prior to find the peaks. If true, \n%                         data is normalised in range [0,1]. Default: true\n%\n%    peakHeightThresh   - Minimum peak height for a photosensor spike to be\n%                         considered an event. This value is empirical and\n%                         it depends on multiple factors (eg. colour of the\n%                         photosensor stimulus). Can be useful to evoid \n%                         considering events changes in diplay contrast not\n%                         associated with the experiment (eg. experiment \n%                         window opening). Deafult: 0\n%    missedTrigTresh    - Maximum acceptable lag (in ms) between recorded \n%                         trigger and photosensor spike. If the lag is\n%                         higher than thism a warning is produced. This\n%                         often occurs if you try to align a trigger that\n%                         does not have an associated photosensor.\n%    modifyOriginal     - Logical (true|false). Whether to return a modified\n%                         copy of the EEG structure containin the\n%                         re-aligned events. Default: true\n%\n% Out:\n%    eventLatenciesInfo - Cell array containing delays information for each\n%                         trigger included in eventNames. Each row\n%                         represents a different trigger. Columns\n%                         represent: \n%                         1. Trigger name\n%                         2. Trigger idx in modified EEG.event structure\n%                         3. Array of delays (ms) for each trigger\n%                         4. Average delay (ms) for each trigger\n%    EEG                - Modified copy of the EEG structure where the \n%                         EEG.event structure now contains:\n%                         1. Photosensor events as triggers\n%                         2. Onset of triggers alligned with the\n%                            photosensor events\n\n% Author: Daniele Scanzi\n\nfunction [eventLatenciesInfo, EEG] = trigger_photo_latency(EEG, eventNames, varargin)\n    \n    % INPUTS\n    p = inputParser;\n    \n    % Mandatory\n    addRequired(p, 'EEG', @isstruct);      % EEG structure with EEG data\n    addRequired(p, 'eventNames', @iscellstr); % Signal matrix\n    \n    % Optional\n    addParameter(p, 'photoTrigger', 'photo', @ischar);       % Name of the trigger to add\n    addParameter(p, 'photoChannel', 64);                     % Channel containing the photosensor data\n    addParameter(p, 'normalisePhotoData', true, @islogical); % Whether to normalise the data or not\n    addParameter(p, 'peakHeightThresh', 0, @isnumeric);      % Minimum height to consider something a peak\n    addParameter(p, 'missedTrigTresh', 16, @isnumeric);      % Max delay (in ms) acceptable\n    addParameter(p, 'modifyOriginal', true, @islogical);     % Whether to modify the original EEG structure or not\n    \n    parse(p, EEG, eventNames, varargin{:});\n    \n    EEG                = p.Results.EEG;\n    eventNames         = p.Results.eventNames;\n    photoTrigger       = p.Results.photoTrigger;\n    photoChannel       = p.Results.photoChannel;\n    normalisePhotoData = p.Results.normalisePhotoData;\n    peakHeightThresh   = p.Results.peakHeightThresh;\n    missedTrigTresh    = p.Results.missedTrigTresh;\n    modifyOriginal     = p.Results.modifyOriginal;\n\n    %% MAIN FUNCTION\n\n    % Check that event structure is in EEG structure\n    if ~isfield(EEG, \"event\")\n        error(\"Cannot find event structure. Is EEG an EEGLAB structure?\")\n    end\n\n    % Check that provided events exist\n    for iEvent = 1:length(eventNames)\n        if ~any(strcmp({EEG.event.type}, eventNames{iEvent}))\n            error(\"Cannot find %s in EEG.event structure\", eventNames{iEvent})\n        end\n    end\n\n    % Check that data contains the channel requested\n    if isinteger(photoChannel)\n        if ~size(EEG.data, 1) &lt; photoChannel\n            error(\"Channel %i out of bound of data size %i\", photoChannel, size(EEG.data, 1))\n        end\n    elseif ischar(photoChannel) || isstring(photoChannel)\n        if ~any(strcmp({EEG.chanlocs.labels}, photoChannel))\n            error(\"Channel %s not found\", photoChannel)\n        else\n            % find channel number and overwrite string\n            photoChannel = find(strcmp({EEG.chanlocs.labels}, photoChannel));\n        end\n    end\n\n    % The function should run even with epoched data, but this has not been\n    % tested yet. There are no many reasons for using this function with\n    % epoched data anyway\n    if ndims(EEG.data) &gt; 2\n        warning(\"Function not tested with epoched data (or data with more \" + ...\n            \"than two dimensions in general\" )\n    end\n\n    % Extract photsensor data (account for possibility of data being\n    % epoched). Correct peaks so to find photosensor onset\n    photoData = EEG.data(photoChannel, :, :);\n    photoData = diff(photoData);\n    \n    if normalisePhotoData\n        photoData = normalize(photoData, 'range');\n    end\n\n    % Find peaKs\n    [~, peaksLocs] = findpeaks(photoData, 'MinPeakHeight', peakHeightThresh);\n\n    % Add peaks to the event structure data\n    fprintf(\"Adding photosensor events to EEG.event structure\")\n    % Add peaks to chanloc values\n    for iPeak = 1:length(peaksLocs)\n        EEG.event(end+1).latency = peaksLocs(iPeak);\n        EEG.event(end).type = photoTrigger;\n    end\n\n    % Reorder events by latency\n    EEG = eeg_checkset(EEG,'eventconsistency');\n\n    % First find triggers indices\n    eventLatenciesInfo = {length(eventNames), 4};\n    for iEvent = 1:length(eventNames)\n        eventLatenciesInfo{iEvent, 1} = eventNames{iEvent};\n        eventLatenciesInfo{iEvent, 2} = find(strcmp({EEG.event.type}, eventNames{iEvent}));\n    end\n    \n    % Compute event latencies finding the nearest photosensor event\n    for iEvent = 1:size(eventLatenciesInfo, 1)\n        % create arry to store latencies\n        currentLatencies = nan(length(eventLatenciesInfo{iEvent, 2}), 1);\n        for iTrig = 1:length(eventLatenciesInfo{iEvent, 2})\n\n            % Check previous event\n            previousTrigger = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)-1);\n            nextTrigger     = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)+1);\n            if strcmp({previousTrigger.type}, photoTrigger)\n                previousLatencyDiff = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency - EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)-1).latency;\n            else\n                % if there is no photosensor trigger before, set this diff\n                % tp -Inf\n                previousLatencyDiff = nan;\n            end\n\n            % Check next event\n            if strcmp({nextTrigger.type}, photoTrigger)\n                nextLatencyDiff = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)+1).latency - EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency;\n            else\n                % if there is no photosensor trigger before, set this diff\n                % tp -Inf\n                nextLatencyDiff = nan;\n            end\n\n            % Find minimum between the two\n            [currentMin, minIdx] = min([previousLatencyDiff nextLatencyDiff]);\n            if  currentMin &gt; missedTrigTresh\n                warning(\"Possible missing triggers around trigger number %i. Event not included in latency calculation \\n\", eventLatenciesInfo{iEvent, 2}(iTrig));\n                currentLatencies(iTrig) = nan;\n            else\n                currentLatencies(iTrig) = currentMin;\n            end\n            \n            % Modify the original dataset event structure if requested\n            if modifyOriginal\n                if minIdx == 1\n                    EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency = previousTrigger.latency;\n                elseif minIdx == 2\n                    EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency = nextTrigger.latency;\n                end\n            end\n\n        end\n\n        % Compute average latency excluding missing triggers\n        currentLatencies = currentLatencies(~isnan(currentLatencies));\n        eventLatenciesInfo{iEvent, 3} = currentLatencies;\n        eventLatenciesInfo{iEvent, 4} = mean(currentLatencies);\n    end\n    EEG = eeg_checkset(EEG);\nend\n\n\n\n% Helper\nfunction isText(myVar)\n    isstring(myVar) || ischar(myVar);\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#gnu-statement",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#gnu-statement",
    "title": "EEG preprocessing functions",
    "section": "GNU statement",
    "text": "GNU statement\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details (https://www.gnu.org/licenses)."
  },
  {
    "objectID": "projects/bayesian_stat_material/bayes_intro.html",
    "href": "projects/bayesian_stat_material/bayes_intro.html",
    "title": "Overview",
    "section": "",
    "text": "A student-led group to learn Bayesian statistics - For students by students\n\n\nThis project was initially born when a friend of mine, Dylan Taylor (PhD student at the University of Auckland), and I wanted to meet over lunch to work through the book Statistical Rethinking. Talking with other PhD students, we soon realised that many researchers are interested in applying bayesian stats. However, most students didn’t have the opportunity to learn this and feel like learning it alone can be too difficult and time-consuming - a feeling we share too. So, we expanded the group to other interested people, and we began a series of weekly meetups. During these, we go through the 2022 lecture series accompanying the book, we have discussions and work through the exercises together. The group has expanded to around 15 people and has become a great opportunity to learn something useful together and create new connections between like-minded people.\nIn this section, you can find the slides and material we create to provide direction during the meetups.\n\nThis project has now been awarded a Creating Connections Grant which will help to provide refreshment during the meetups"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Stop treating participants as passive subjects\n\n\n\n\n\n\nopinions\n\n\nacademia\n\n\n\nReflections on the difference between participants and subjects in research\n\n\n\n\n\nOctober 14, 2024\n\n\nDaniele Scanzi\n\n\n\n\n\n\n\n\n\n\n\n\nYou don’t need to be competitive if you know you’re competent\n\n\n\n\n\n\nopinions\n\n\nacademia\n\n\n\nMusical reflections on academic competitiveness\n\n\n\n\n\nJune 8, 2023\n\n\nDaniele Scanzi\n\n\n\n\n\n\n\n\n\n\n\n\nK means clustering\n\n\n\n\n\n\nR\n\n\ncoding\n\n\nmicrostates\n\n\n\nK means algortihm walk through presented at the lab meeting to explain this technique.\n\n\n\n\n\nOctober 3, 2022\n\n\nDaniele Scanzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "href": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "title": "You don’t need to be competitive if you know you’re competent",
    "section": "",
    "text": "In the past two months, I’ve been obsessed with the new album of Pinguini Tattici Nucleari. Every song deals with a different theme in a powerful way. I cried multiple times to Ricordi and its earthly depiction of dealing with a parent suffering from Alzheimer’s. I reflect on the concept of faith with Fede and the Italian socio-political situation with Coca Zero. However, Zen is the single I go back to over and over again.\nThe reason for this is simple. It describes something I often feel, being often worried about the future and looking for a moment of Zen. But one sentence got stuck in my mind, and it has nothing to do with my psychological state and everything with academia.\n\n Essere competitivo non serve se sai di еssere competеnte\n\n\nWhich translates to:\n\nYou don’t need to be competitive if you know you’re competent\n\n\nIt’s just one sentence, a simple one. Still, it captures so well the academic environment, where people feel the need to show their knowledge, achievement and prizes just for the sake of establishing their position. Something exacerbated by the Universities themselves, putting people against each to obtain one position (often underpaid), a breadcrumb of notoriety in the field. So much so that we are still here having well-established academics that have to act as the “bad cop” in recruitment panels for the sake of it. Or you need to fight for years for an opportunity to become a clinical psychologist because, you know, we need psychologists, but only if they are elitists.\nBut do I need to take part in this? Do I need to play the game?\nNo, I don’t. I know my worth, I know my strengths and weaknesses. I don’t want to be competitive. It’s useless. It’s worthless. It’s damaging to others.\nA simple song, a verse sentence. Yet so much to think about."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniele Scanzi",
    "section": "",
    "text": "I spend my day looking at squiggly lines produced by your brain. I would love to know how a bat sees the world. One day I ate five razorblades in front of 400 people."
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html",
    "href": "exploring/waihaha_2023_09_04.html",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "",
    "text": "Location: Pureora forest\nDate: from 2023/09/02 to 2023/09/04\nThree of us finally managed to squeeze two nights out for an end-of-winter camping trip. We headed to the Pureora Forest Park to walk the Waihāhā hut Track. The track follows the Waihāhā River and it offers a nice and easy walk across a variety of vegetation.\nWe spent the night at the Waihāhā hut, and the next day, we made our way back to the car. We decided to take the day slow, visiting Kinloch and then relaxing at the Wairakei Terraces.\nGiven that the weather was nice and would have remained clear until the next afternoon, we decided to camp out for another night. We headed to the Kakaho Campsite. The site was desert, we were the only one there. The night was clear, and we got treated to an amazing starry sky while trying to keep warm next to a campfire.\nAfter a quite cold night, we packed up and completed the short Rimu Track Loop within the Pureora Forest."
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "href": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "Notes for the future",
    "text": "Notes for the future\n\nFor winterish camping, source an Italian Army Blanket (or similar) or bring a hot water bottle. The night was fine, but it would have been nicer to have an extra source of warmth.\nInstant polenta is a great camping food. It packs compact and it’s filling. It might be better than pasta.\nNeed to learn some handy knots"
  },
  {
    "objectID": "exploring/monte_avaro.html",
    "href": "exploring/monte_avaro.html",
    "title": "Monte Avaro collection",
    "section": "",
    "text": "My sister and I with Boletus Pinicola\n\n\n\n\n\nRiding a goat"
  },
  {
    "objectID": "exploring/monte_avaro.html#early-photos",
    "href": "exploring/monte_avaro.html#early-photos",
    "title": "Monte Avaro collection",
    "section": "",
    "text": "My sister and I with Boletus Pinicola\n\n\n\n\n\nRiding a goat"
  },
  {
    "objectID": "exploring/monte_avaro.html#december-2022",
    "href": "exploring/monte_avaro.html#december-2022",
    "title": "Monte Avaro collection",
    "section": "December 2022",
    "text": "December 2022\n\n\n\nEating panettone in the snow\n\n\n\n\n\nDrinking from a natural spring\n\n\n\n\n\nWinter view\n\n\n\n\n\nGoing into the woods"
  },
  {
    "objectID": "exploring/monte_avaro.html#videos",
    "href": "exploring/monte_avaro.html#videos",
    "title": "Monte Avaro collection",
    "section": "Videos",
    "text": "Videos\n\n\nVideo\nLaura (wife) decided she had enough of me\n\n\n\n\nVideo\nI decided to slide too\n\n\n\n\nVideo\nBad attempt to a front flip\n\n\n\n\nVideo\nBad attempt to stay bare feet in the snow\n\n\n\n\nVideo\nWoods with snow\n\n\n\n\nVideo\nBob sleading first attempt\n\n\n\n\nVideo\nLaura’s first time on a bob\n\n\n\n\nVideo\nA steeper but better bob ride"
  },
  {
    "objectID": "exploring/exploring_intro.html",
    "href": "exploring/exploring_intro.html",
    "title": "Exploring B/Log",
    "section": "",
    "text": "This is me (on the left), my sister (eating the bread) and my cousins at Monte Avaro. For most people this name means nothing, but this place is probably one of the most important for me. Still today, some of the most important and memories I have comes from there.\nMy grandparents used to bring us four up to this mountain every summer. However, this was not just a one day family hike, this was a month-long stay. A month-long stay in a van that my grandfather repourposed as a self-contained van, without running water, toilet or anything else. We slept all together, some years all inside the van, some other years we got a tent on top of it. We collected water from a spring a couple of hundred meters down the road, my grandmother used to cook us with a small gas stove or on a campfire, where some nights we prepared vin brulé (the Italian version of mulled wine).\nThis place was amazing. It was not popular, there was hardly anyone around, just us, an alpine cow farmer who became a family friend (you see, my grandparents used to bring our parent up here as well when they young) and a couple of people owning a hut up there. Electricity was not a thing, I remember drinking hot chocolate in the hut lighted up by oil lamps.\n\n\n\n\n\n\nSo how do we spend the majority of our time? Well, hiking for hours with my grandfather looking for mushrooms: boletus pinicola, boletus luridus, Lactarius deliciosus (but only if the sape was orange), mazze di tamburo (lit. drumsticks, or macrolepiota procera) amanita vaginata, russola virescens, russola cyanoxantha and a few more. These excursions were always fun for us and scary for our parents. The reason? My grandfather never used tracks, we always explored the woods, away from the areas where other people would go. So imagine, I was 6 and I was walking 8 hours next to cliffs or underneath steep woods. Surely a hard work, but fun and rewarding.\n\n\n\n\n\n\nboletus pinicola\n\n\n\n\n\n\n\nboletus luridus\n\n\n\n\n\n\n\n\n\namanita vaginata\n\n\n\n\n\n\n\nmacrolepiota procera\n\n\n\n\n\nThroughout the years I acquired some specific knowledge. First of all, how to navigate that terrain safely while keeping up with my grandfather. He’s a fast walker, something I acquired too. Then, how to recognise good mushrooms (above), from those that we should avoid (amanita phalloidis, amanita muscaria, russola emetica). How to deal without the comforts of our homes for a long time. But most importantly, the love for the mountain and nature.\nRecently, I went back home in Bergamo. It was a fantastic time and I managed to squeeze in a few days in the mountains. It was winter there, so found some snow (not as mush as I would have imagined, but still fun). My family and I went up to the rifugio Vodala equipped with crampons. After a great lunch, we decided to keep going up and my sister, my cousin (Luca, the one not looking at the camera in the first picture), my mum and I attempted to reach Cima Timogno. However, fog started to build up and we needed enough time to go down. So we stopped half way and turned back. I am not really experienced in winter alpine terrain, so that was a nice challenge. It made me want some training and experience in winter alpine excursions.\n\n\n\n\n\n\nI also brought my wife up to Monte Avaro twice. Once to explore around and once to try bob sleading. Not the one on the ice track, but the kid version on a plastic sled. We climbed up a steep section of Monte Avaro where there were no people around and we went down a few times. It was fun to try this again after many years.\n\n\n\n\n\n\nMy family going up to Rifugio Vodala\n\n\n\n\n\n\n\nTurn around point with my cousin\n\n\n\n\n\n\n\n\n\nMonte avaro winter 2022\n\n\n\n\n\n\n\nDescending from monte tetta - Avaro 2022\n\n\n\n\n\nNow I am back in New Zealand, and I have a strong wanting to explore and reconnect with this part of my life I have put a bit aside over the past few years since I moved here. For a draw of luck, a friend of mine and his partner are into hiking and camping too, so it would be nice to have people around here to explore with. As said at the very beginning, this section will be a collection of the different excursions, short or long, I’ll do. Each page will have a short description (definitely shorter than this), reporting the major elements of the hike.\n\nThis section is dedicated to my grandfather that, now 88 years old, keeps asking me when we will go to the mountain to look for mushrooms. He always jokes that he might not be able to hike much anymore, but every time we end up walking 6 hours no stop…and he leads the whole way"
  },
  {
    "objectID": "educating/signal_detection_theory.html",
    "href": "educating/signal_detection_theory.html",
    "title": "Introduction to Signal Detection Theory",
    "section": "",
    "text": "An introduction to signal detection theory for the lab meeting hold on the 22nd of July 2024."
  },
  {
    "objectID": "educating/signal_detection_theory.html#is-wally-there",
    "href": "educating/signal_detection_theory.html#is-wally-there",
    "title": "Introduction to Signal Detection Theory",
    "section": "Is Wally there?",
    "text": "Is Wally there?\n\nHere is a fun experiment. I’ll show you a picture from the puzzle Where is Wally? and you need to tell me, as quickly as possible, whether Wally is in there or not. That’s correct, I don’t want to know where Wally is - that’s the standard game - I want to know if he’s present in the picture. You can give me only two possible answers, “YES” or “NO”, and your time is limited.\nClick on the link here and write down your answer for each image (or just answer in the moment, there are no solutions for this small example).\nHow was it? I bet some “trials” were simple, while others were hard, so much so you have randomly guessed - or at least this is what you think. Crucially, however, how hard each trial was might vary from person to person, especially for those images that were somewhat in the middle. Some of you might be serial Where is Wally players, others might have seen these pictures for the very first time now and have no clue what is happening here. This is interesting! It means that the same exact images can be perceived and processed differently by different people. Ok, this observation is not ground-braking but it allows me to introduce the question we want to tackle today:\n\nHow do we assess and quantify someone’s performance in a given task?\n\n\nWe will tackle this question with Signal Detection Theory (SDT)"
  },
  {
    "objectID": "educating/signal_detection_theory.html#setting-some-boundaries",
    "href": "educating/signal_detection_theory.html#setting-some-boundaries",
    "title": "Introduction to Signal Detection Theory",
    "section": "Setting some boundaries",
    "text": "Setting some boundaries\nThe question is large in scope. What do I mean by performance? Which tasks are we talking about? The time we have is limited (and my brain as well), and even if SDT can be employed in a variety of cases, we will focus only on the two most common and basic tasks: (1 Alternative Forced Choice Task (1AFC) (aka YES-NO tasks, but I think this name is misleading) and 2 Alternative Forced Choice Tasks (2AFC).\n\n1AFC: As described in Hautus, Macmillan and Creelman, these tasks aim to distinguish between stimuli. Modifying the book a bit, an example is deciding whether an MRI brain scan shows abnormalities or not. If we want to stay more in the cognitive psychology realm, whether the Müller-Lyer line on the right is longer than the one on the left.\n\n\n\n\n\nIt’s not - trust me. As hinted above, these tasks are commonly known as YES-NO tasks, because they often allow only two answers, maybe and perhaps. However, this is not necessarily the case and many other tasks that are not Yes-No tasks require a yes-no answer. So, I prefer the name 1AFC (unfortunately, I don’t remember where I read about this definition, as it’s not mine). The 1 in the definition represents the number of stimuli you present at any given time. One MRI scan, one line (other than the comparison), and one image with or without Wally.\n2AFC: If 1AFC are clear, 2AFC are simply their expansion. Here, you present two stimuli within the same task. If we modify our Wally experiment, we could ask Is Wally present in the image on the right or on the left?\n\n\n\n\n\n\n\n\n\n\nIs the MRI of person A to have abnormalities or the MRI of person B? Is the line on the left or the line on the right the longest? Now you see where I think can be confusing. The line example can be either a yes-no task or a 2AFC task, depending on what you ask the person to do. Note that you can expand these tasks even further, with a 3AFC, 4AFC, 5AFC… with each number representing your score on a sadism scale."
  },
  {
    "objectID": "educating/signal_detection_theory.html#sensitivity",
    "href": "educating/signal_detection_theory.html#sensitivity",
    "title": "Introduction to Signal Detection Theory",
    "section": "Sensitivity",
    "text": "Sensitivity\nWhat are the interesting aspects of these tasks? Well, firstly, their goal is to test a person’s sensitivity to something. In other words, their ability to discriminate something. In our Wally experiment, whether Wally is present or not. Someone with high sensitivity to Wally would be able to tell quickly and accurately whether Wally is in a picture or in which of two pictures he is. People that suck at this game, instead, have low sensitivity and struggle to answer correctly even with simple images.\nObviously, saying that someone is good at something is not a very scientific way to quantify sensitivity. So, let’s think about how we can measure your sensitivity to Wally. The first and most obvious step is to count how many times you correctly found Wally (you need to see him to say that he is there). Because this value depends on the number of pictures you have been presented with, we divide it by the number of pictures that contained Wally. This way, we can compare this value across studies, and our measure is independent of the number of trials. This measure is called the hit rate.\n\nHit rate: proportion of trials where the person correctly identified the presence of a feature of interest\n\nThis measure is nice and easy to interpret. You scored a hit rate of 90%, well done! You are terrific at finding Wally. You scored a hit rate of 50%. Well, you were probably guessing. You scored a hit rate of 20%… mmm I’m not sure what you were doing there… the opposite of what you have been asked? Hooowwwwever… looking only at your hit rate is problematic. Think about this: what if you could not be bothered to do a task, but you had to complete it anyway? What’s the fastest way you can achieve your freedom? Perhaps you could provide the same answer over and over.\nImagine this: if you say “Wally is there” every single time, you will get a hit rate of 100%. Every time Wally was in a picture, you “found” it. Here is where the pictures without Wally (lure trials, catch trials… call them as you like, I like to call them igotchya trials) become important. Using your “always say yes” strategy, you end up saying that Wally was there every time he wasn’t.\nSo, what we ALSO want to look at is the number of trials without Wally where you said you saw him. Again, we divide this number by the total number of Wally-less trials, and we obtain your false alarm rate.\n\nFalse alarms: proportion of trials where the person incorrectly stated the presence of a feature of interest where the feature was not there\n\nIf we want to be precise, we can split your answers into four categories:\n\n\n\n\nWally is there\nWally is not there\n\n\n\n\n\n\nYou say “yes”\nHIT\nFALSE ALARM\n\n\n\n\nYou say “no”\nMISS\nCORRECT REJECTION\n\n\n\n\n\nWe can now formalise our definition of hit and false alarm rate.\n\\[\\text{HIT RATE} = \\frac{\\text{hit}}{\\text{hit} + \\text{miss}}\\] \\[\\text{FALSE ALARM RATE} = \\frac{\\text{false alarm}}{\\text{false alarm} + \\text{correct rejection}}\\]\nNote that, by the definition above, hit and miss rates are complementary. If your hit rate is 85%, your miss rate is 25%. The reason for this is that they are both computed on the number of trials that contained Wally. The same goes for the false alarm and the correct rejection rates.\n\n\n\n\n\n\n\n\n\n\n\nWally is there\nWally is not there\n\n\n\n\n\n\nYou say “yes”\n\\(\\frac{\\text{hit}}{\\text{hit} + \\text{miss}}\\)\n\\(\\frac{\\text{false alarm}}{\\text{false alarm} + \\text{correct rejection}}\\)\n\n\n\n\nYou say “no”\n\\(\\frac{\\text{miss}}{\\text{hit} + \\text{miss}}\\)\n\\(\\frac{\\text{correct rejection}}{\\text{false alarm} + \\text{correct rejection}}\\)\n\n\n\n\n\nBecause hits and false alarms include information regarding all the possible types of answers, we can just use those to compute, where were we? … oh yes, a measure of sensitivity.\n\nd-prime\nIf you have high sensitivity to finding Wally, you are either very good at (1) finding when Wally is present, (2) finding when Wally is not there, or (3) both. (1) is indexed by your hit rate, and (2) by your false alarm rate. This means that we should expect our sensitivity measure to increase if (1) the hit rate increases, (2) the false alarm rate decreases, or (3) both. A measure with these characteristics can be obtained by subtracting the false alarm rate from the hit rate (for 1AFC tasks, an adjustment of $\\frac{\\sqrt{2}}{2}$ needed for 2AFC tasks), but the concept is similar).\nThink about this. If your hit rate is high and your false alarm rate is low, the result of the subtraction would be high. Vice versa, if your hit rate is low and your false alarm rate is high, the subtraction will be (in absolute value) high. If your hit rate is high and your false alarm rate is high too, the result will be low. Finally, if you have the same hit and false alarm rate, then the result will be 0.\nIn signal detection theory, this measure is called d-prime or d’ and it is computed on the standardised hit and false alarm rates - where standardised means that they have been converted into Z-scores:\n\\[d' = Z(\\text{hit rate}) - Z(\\text{false alarm rate})\\] The interesting thing about d’ is that the same d’ value can be achieved with different proportions of hit and false alarm rates. One way to visualise this, is through the Receiver Operating Characteristic curves.\n\n\nCode\nd_prime &lt;- seq(-3, 3, by=0.01)\nfa      &lt;- seq(0, 1, by=0.01)\n\n# Create data to plot\nroc_curves &lt;- list()\nfor (d in d_prime) {\n    # Compute hit rate\n    current_hit &lt;- pnorm(d + qnorm(fa))\n    # Create dataframe containing all relevant info\n    current_roc_data &lt;- data.frame(\n        dprime    = rep(d, length(fa)),\n        hit     = current_hit,\n        fa      = fa\n    )\n    \n    roc_curves &lt;- append(roc_curves, list(current_roc_data))\n}\n\nroc_data &lt;- Reduce(rbind, roc_curves)\n\nroc_plot &lt;- ggplot(roc_data, aes(x=fa, y=hit, frame=dprime)) +\n    geom_line(color=\"purple\", linewidth=1.5) +\n    geom_segment(aes(x=0, y=0, xend=1, yend=1)) +\n    labs(\n        x = \"FA RATE\",\n        y = \"HIT RATE\",\n        title = \"d'\"\n        \n    ) +\n    theme_minimal() +\n            coord_fixed(xlim = c(0,1), \n                        ylim = c(0,1), \n                        expand = TRUE)\nggplotly(roc_plot)\n\n\n\n\n\n\nThe purple curve above represents all the combinations of hit and false alarm rates that result in the same d’. The diagonal black line represents a d’ of 0. As said above, you achieve this value every time the hit and false alarm rates are the same. Usually, only ROC curves above the positive diagonal are reported. These represent d’ values above 0. It is extremely unlikely that you will deal with d’ below zero (ROCs below the positive diagonal), as they reflect scenarios where someone had fewer hits than false alarms. To achieve that, a person needs to do the opposite of what is asked. However, it can happen that on a small number of trials, the participant’s performance falls to chance level and, just by chance, you get a d’ just below 0. You might see this, for instance, in difficult tasks if you analyse blocks independently.\nSo, to recap, d’ measures a person’s sensitivity in a task by accounting for correct and incorrect responses in trials containing the target characteristic and lapse/ catch trials."
  },
  {
    "objectID": "educating/signal_detection_theory.html#bias",
    "href": "educating/signal_detection_theory.html#bias",
    "title": "Introduction to Signal Detection Theory",
    "section": "Bias",
    "text": "Bias\nAnother aspect of performance we might be interested in investigating is whether someone has a tendency to provide one specific answer instead of another. For instance, in conditions of uncertainty (like the majority of psychological tasks), you might be more prone to report something - maybe you believe that this will make the experimenter happy (it doesn’t). Or maybe you are left-handed, and you tend to report more with your left hand - randomisation is key in experiments, isn’t it? This is problematic if we want to assess the sensitivity of a person. Because the same d’ can be achieved with multiple combinations of hits and false alarms, it is possible that two people can score the same sensitivity, even when one is really trying their best in the task and the other has a strong bias for providing one specific response. Obviously, the two situations are not the same and we should be aware of that. Let’s see another example.\nTwo people complete our Is Wally there? task. Here are their hit rate, false alarm rate and d’ scores:\n\n\n\n\nPerson A\nPerson B\n\n\n\n\nHIT\n0.73\n0.91\n\n\nFALSE ALARM\n0.08\n0.25\n\n\nd’\n2.02\n2.02\n\n\n\nWow, two different performances but the same sensitivity values!\nAlright, you probably got the point now. So, what can we do about this? The answer is simple: we want to find a measure that represents whether a person’s sensitivity is unbiased or not. If not, towards which answer the bias is. Thankfully, SDT provides a simple answer to this question: the criterion or c. c is derived again from hits and false alarms and, for our simple case of 1AFC tasks, it is computed as:\n\\[c = - \\frac{Z(\\text{hit rate}) + Z(\\text{false alarm rate})}{2}\\] c assumes a value of 0 where the responder is unbiased. Positive values indicate a bias towards not reporting something. Negative values indicate a bias towards reporting something. The reason for this is the relationship between false alarms and misses and between hit rate and correct rejection. We won’t get into these details now, but we can build some intuition and connect the discussion back to sensitivity by looking at an updated version of our ROC plot.\n\n\nCode\n# Look at code above to understand this\ntriangle_vertex_low &lt;- data.frame(\n    x = c(0, 0, 0.5),\n    y = c(0.01, 1, 0.51)\n)\n\ntriangle_vertex_high &lt;- data.frame(\n    x = c(0.5, 0, 1),\n    y = c(0.51, 1, 1)\n)\n\nroc_data_c &lt;- roc_data %&gt;% \n    filter(dprime &gt;= 0)\n\nroc_c_plot &lt;- ggplot(roc_data_c, aes(x=fa, y=hit, frame=dprime)) +\n    geom_polygon(data = triangle_vertex_low, \n                 aes(x=x, y=y), \n                 inherit.aes = FALSE,\n                 fill = \"#fc7b54\",\n                 alpha = 0.25) +\n        geom_polygon(data = triangle_vertex_high, \n                 aes(x=x, y=y), \n                 inherit.aes = FALSE,\n                 fill = \"#008080\",\n                 alpha = 0.25) +\n    annotate(geom = \"text\", x=0.15, y=0.25, label=\"c&gt;0\") +\n    annotate(geom = \"text\", x=0.65, y=0.75, label=\"c&lt;0\") +\n    geom_segment(aes(x=0.5, y=0.5, xend=0, yend=1)) +\n    geom_line(color=\"purple\", linewidth=1.5) +\n    geom_segment(aes(x=0, y=0, xend=1, yend=1)) +\n    labs(\n        x = \"FA RATE\",\n        y = \"HIT RATE\",\n        title = \"d'\"\n        \n    ) +\n    theme_minimal() +\n            coord_fixed(xlim = c(0,1), \n                        ylim = c(0,1), \n                        expand = TRUE)\n\n\nggplotly(roc_c_plot)\n\n\n\n\n\n\nSet the d’ to 2.02, from our example above. Now, if we look at participant A, we see that it falls within the orange area. This indicates a tendency for this participant to report that Wally is not in a picture. Participant B, instead, falls within the green area, which indicates a tendency to report that Wally is there. Indeed, their hit rate is high, but their false alarm rate is too! In other words, while the first participant has a NO bias, the second one has a YES bias."
  },
  {
    "objectID": "educating/signal_detection_theory.html#noise-and-models",
    "href": "educating/signal_detection_theory.html#noise-and-models",
    "title": "Introduction to Signal Detection Theory",
    "section": "NOISE and MODELS",
    "text": "NOISE and MODELS\nAfter this brief intro to the two main SDT measures, we need to talk about how people make decisions. In doing so, we will build more insight into d’ and c. To keep the discussion simple, we will solely focus on 1AFC tasks, as they are one-dimensional and easy to understand. Just know that 2AFC tasks are simply a 2-dimensional version of 1AFC tasks.\nIn 1AFC tasks, the person is asked to judge one stimulus at a time and to provide one of two possible answers - commonly YES or NO, though not always. Because our Wally task is a YES-NO task, I’ll go with this, though you can swap these answers for anything you want: LEFT and RIGHT, NEW and OLD, REMEMBERED and FORGOTTEN, etc… Look at the picture below\n Wally is just on the right of the white and green umbrella. How did we find it? Well, we needed to sieve through the wall of information presented to our eyes. Specifically, irrelevant information. Or, as we like to call it, noise. To say that Wally is here means that we processed the noise within the image as long as Wally, the signal, and decided that the evidence favouring the presence of the signal is higher enough for us to say YES, Wally is here! Another way to say this is that we have collected enough evidence for us to state that we saw something relevant.\nTwo things are at play here:\n\nNoise vs Signal\nEnough evidence\n\nThese are intertwined, as are most things in life. This is a good enough reason for me to start discussing the second point, completely disregarding the order of the list I wrote. So… enough evidence. How much is enough? Well, it depends. All of us have a different threshold. Some of you might have originally said that Wally was not in that picture. The strength of the signal (Wally standing there creepily wearing a beanie on a beach) was not strong enough to overpower the noise of all those red togs. Others, instead, might have said yes, their eyes are well attuned to spot people who are going to get a heat stroke. Or, some of you might be biased towards saying they did not see anything (don’t worry, I’m not the police, you can tell me where Wally is). Others might have a bias for YES.\nSweet, the threshold for enough evidence is determined by (a) the stimulus itself and (b) the personal bias. The stimulus is a mix of noise and signal so that the closer the noise is to the signal (e.g. all the red and stripes in the picture) and the weaker the signal itself (imagine a teeny-tiny Wally), the more difficult it is to separate the two. We are now at the first point on the list. Obviously, noise and signal are two very broad terms, but this is good for us because we can do what every good scientist does when dealing with something vague: create a normal distribution. The simplest - but powerful - SDT model to explain performance in a 1AFC task is a Gaussian Model, where the probability of classifying something as noise (NO) and the probability of classifying something a signal (YES) are described by two Gaussian.\n\nOpen app in new page\nPlay around with this app. The two curves represent noise and signal. These curves, specifically, tell you the probability of something being noise (or signal) given a specific value of one dimension of your stimulus (x-axis: here can be anything, image contrast, familiarity, etc…). The simplest model implies that the two curves have equal variance, though this assumption can be relaxed. Try to modify the noise spread or the signal spread.\nBy modifying the d’ value, you see that the curves get closer or farther away. Why the distance between the centres of these two distributions reflects sensitivity? Well, higher sensitivity means that you can correctly discriminate a signal (Wally) even when it looks like noise (everything else in the scene). That is, you are able to obtain a high hit rate and a low false alarm rate even when the two distributions overlap.\nWhen do you report a signal? When you have gathered enough evidence in support of the signal. Enough here is determined by c, which is the vertical line. There are a couple of things to note about c. Firstly, note that its placement determines the proportion of hit rate and false alarms. The hit rate is represented by the area under the noise+signal curve above and beyond the c line (how many times something that is a signal is defined as a signal). Similarly, The false alarm rate is defined by the area under the noise curve above and beyond the c line (how many times noise has been classified as signal). Toggle the Noise and Noise+ Signal distributions. Moreover, note that the value of c defines where you end up on a d’ ROC line. That is all the different combinations that give rise to the same d’ reflect different levels of bias.\nHope this is useful!"
  },
  {
    "objectID": "educating/page-reveal.html#is-wally-there",
    "href": "educating/page-reveal.html#is-wally-there",
    "title": "Introduction to Signal Detection Theory",
    "section": "Is Wally there?",
    "text": "Is Wally there?\n\nHere is a fun experiment. I’ll show you a picture from the puzzle Where is Wally? and you need to tell me, as quickly as possible, whether Wally is in there or not. That’s correct, I don’t want to know where Wally is - that’s the standard game - I want to know if he’s present in the picture. You can give me only two possible answers, “YES” or “NO”, and your time is limited.\nClick on the link here and write down your answer for each image (or just answer in the moment, there are no solutions for this small example).\nHow was it? I bet some “trials” were simple, while others were hard, so much so you have randomly guessed - or at least this is what you think. Crucially, however, how hard each trial was might vary from person to person, especially for those images that were somewhat in the middle. Some of you might be serial Where is Wally players, others might have seen these pictures for the very first time now and have no clue what is happening here. This is interesting! It means that the same exact images can be perceived and processed differently by different people. Ok, this observation is not ground-braking but it allows me to introduce the question we want to tackle today:\n\nHow do we assess and quantify someone’s performance in a given task?\n\n\nWe will tackle this question with Signal Detection Theory (SDT)"
  },
  {
    "objectID": "educating/page-reveal.html#setting-some-boundaries",
    "href": "educating/page-reveal.html#setting-some-boundaries",
    "title": "Introduction to Signal Detection Theory",
    "section": "Setting some boundaries",
    "text": "Setting some boundaries\nThe question is large in scope. What do I mean by performance? Which tasks are we talking about? The time we have is limited (and my brain as well), and even if SDT can be employed in a variety of cases, we will focus only on the two most common and basic tasks: (1 Alternative Forced Choice Task (1AFC) (aka YES-NO tasks, but I think this name is misleading) and 2 Alternative Forced Choice Tasks (2AFC).\n\n1AFC: As described in Hautus, Macmillan and Creelman, these tasks aim to distinguish between stimuli. Modifying the book a bit, an example is deciding whether an MRI brain scan shows abnormalities or not. If we want to stay more in the cognitive psychology realm, whether the Müller-Lyer line on the right is longer than the one on the left.\n\n\n\n\n\nIt’s not - trust me. As hinted above, these tasks are commonly known as YES-NO tasks, because they often allow only two answers, maybe and perhaps. However, this is not necessarily the case and many other tasks that are not Yes-No tasks require a yes-no answer. So, I prefer the name 1AFC (unfortunately, I don’t remember where I read about this definition, as it’s not mine). The 1 in the definition represents the number of stimuli you present at any given time. One MRI scan, one line (other than the comparison), and one image with or without Wally.\n2AFC: If 1AFC are clear, 2AFC are simply their expansion. Here, you present two stimuli within the same task. If we modify our Wally experiment, we could ask Is Wally present in the image on the right or on the left?\n\n\n\n\n\n\n\n\n\n\nIs the MRI of person A to have abnormalities or the MRI of person B? Is the line on the left or the line on the right the longest? Now you see where I think can be confusing. The line example can be either a yes-no task or a 2AFC task, depending on what you ask the person to do. Note that you can expand these tasks even further, with a 3AFC, 4AFC, 5AFC… with each number representing your score on a sadism scale."
  },
  {
    "objectID": "educating/page-reveal.html#sensitivity",
    "href": "educating/page-reveal.html#sensitivity",
    "title": "Introduction to Signal Detection Theory",
    "section": "Sensitivity",
    "text": "Sensitivity\nWhat are the interesting aspects of these tasks? Well, firstly, their goal is to test a person’s sensitivity to something. In other words, their ability to discriminate something. In our Wally experiment, whether Wally is present or not. Someone with high sensitivity to Wally would be able to tell quickly and accurately whether Wally is in a picture or in which of two pictures he is. People that suck at this game, instead, have low sensitivity and struggle to answer correctly even with simple images.\nObviously, saying that someone is good at something is not a very scientific way to quantify sensitivity. So, let’s think about how we can measure your sensitivity to Wally. The first and most obvious step is to count how many times you correctly found Wally (you need to see him to say that he is there). Because this value depends on the number of pictures you have been presented with, we divide it by the number of pictures that contained Wally. This way, we can compare this value across studies, and our measure is independent of the number of trials. This measure is called the hit rate.\n\nHit rate: proportion of trials where the person correctly identified the presence of a feature of interest\n\nThis measure is nice and easy to interpret. You scored a hit rate of 90%, well done! You are terrific at finding Wally. You scored a hit rate of 50%. Well, you were probably guessing. You scored a hit rate of 20%… mmm I’m not sure what you were doing there… the opposite of what you have been asked? Hooowwwwever… looking only at your hit rate is problematic. Think about this: what if you could not be bothered to do a task, but you had to complete it anyway? What’s the fastest way you can achieve your freedom? Perhaps you could provide the same answer over and over.\nImagine this: if you say “Wally is there” every single time, you will get a hit rate of 100%. Every time Wally was in a picture, you “found” it. Here is where the pictures without Wally (lure trials, catch trials… call them as you like, I like to call them igotchya trials) become important. Using your “always say yes” strategy, you end up saying that Wally was there every time he wasn’t.\nSo, what we ALSO want to look at is the number of trials without Wally where you said you saw him. Again, we divide this number by the total number of Wally-less trials, and we obtain your false alarm rate.\n\nFalse alarms: proportion of trials where the person incorrectly stated the presence of a feature of interest where the feature was not there\n\nIf we want to be precise, we can split your answers into four categories:\n\n\n\n\nWally is there\nWally is not there\n\n\n\n\n\n\nYou say “yes”\nHIT\nFALSE ALARM\n\n\n\n\nYou say “no”\nMISS\nCORRECT REJECTION\n\n\n\n\n\nWe can now formalise our definition of hit and false alarm rate.\n\\[\\text{HIT RATE} = \\frac{\\text{hit}}{\\text{hit} + \\text{miss}}\\] \\[\\text{FALSE ALARM RATE} = \\frac{\\text{false alarm}}{\\text{false alarm} + \\text{correct rejection}}\\]\nNote that, by the definition above, hit and miss rates are complementary. If your hit rate is 85%, your miss rate is 25%. The reason for this is that they are both computed on the number of trials that contained Wally. The same goes for the false alarm and the correct rejection rates.\n\n\n\n\n\n\n\n\n\n\n\nWally is there\nWally is not there\n\n\n\n\n\n\nYou say “yes”\n\\(\\frac{\\text{hit}}{\\text{hit} + \\text{miss}}\\)\n\\(\\frac{\\text{false alarm}}{\\text{false alarm} + \\text{correct rejection}}\\)\n\n\n\n\nYou say “no”\n\\(\\frac{\\text{miss}}{\\text{hit} + \\text{miss}}\\)\n\\(\\frac{\\text{correct rejection}}{\\text{false alarm} + \\text{correct rejection}}\\)\n\n\n\n\n\nBecause hits and false alarms include information regarding all the possible types of answers, we can just use those to compute, where were we? … oh yes, a measure of sensitivity.\nd-prime\nIf you have high sensitivity to finding Wally, you are either very good at (1) finding when Wally is present, (2) finding when Wally is not there, or (3) both. (1) is indexed by your hit rate, and (2) by your false alarm rate. This means that we should expect our sensitivity measure to increase if (1) the hit rate increases, (2) the false alarm rate decreases, or (3) both. A measure with these characteristics can be obtained by subtracting the false alarm rate from the hit rate (for 1AFC tasks, an adjustment of $\\frac{\\sqrt{2}}{2}$ needed for 2AFC tasks), but the concept is similar).\nThink about this. If your hit rate is high and your false alarm rate is low, the result of the subtraction would be high. Vice versa, if your hit rate is low and your false alarm rate is high, the subtraction will be (in absolute value) high. If your hit rate is high and your false alarm rate is high too, the result will be low. Finally, if you have the same hit and false alarm rate, then the result will be 0.\nIn signal detection theory, this measure is called d-prime or d’ and it is computed on the standardised hit and false alarm rates - where standardised means that they have been converted into Z-scores:\n\\[d' = Z(\\text{hit rate}) - Z(\\text{false alarm rate})\\] The interesting thing about d’ is that the same d’ value can be achieved with different proportions of hit and false alarm rates. One way to visualise this, is through the Receiver Operating Characteristic curves.\n\n\nCode\nd_prime &lt;- seq(-3, 3, by=0.01)\nfa      &lt;- seq(0, 1, by=0.01)\n\n# Create data to plot\nroc_curves &lt;- list()\nfor (d in d_prime) {\n    # Compute hit rate\n    current_hit &lt;- pnorm(d + qnorm(fa))\n    # Create dataframe containing all relevant info\n    current_roc_data &lt;- data.frame(\n        dprime    = rep(d, length(fa)),\n        hit     = current_hit,\n        fa      = fa\n    )\n    \n    roc_curves &lt;- append(roc_curves, list(current_roc_data))\n}\n\nroc_data &lt;- Reduce(rbind, roc_curves)\n\nroc_plot &lt;- ggplot(roc_data, aes(x=fa, y=hit, frame=dprime)) +\n    geom_line(color=\"purple\", linewidth=1.5) +\n    geom_segment(aes(x=0, y=0, xend=1, yend=1)) +\n    labs(\n        x = \"FA RATE\",\n        y = \"HIT RATE\",\n        title = \"d'\"\n        \n    ) +\n    theme_minimal() +\n            coord_fixed(xlim = c(0,1), \n                        ylim = c(0,1), \n                        expand = TRUE)\nggplotly(roc_plot)\n\n\n\n\n\n\nThe purple curve above represents all the combinations of hit and false alarm rates that result in the same d’. The diagonal black line represents a d’ of 0. As said above, you achieve this value every time the hit and false alarm rates are the same. Usually, only ROC curves above the positive diagonal are reported. These represent d’ values above 0. It is extremely unlikely that you will deal with d’ below zero (ROCs below the positive diagonal), as they reflect scenarios where someone had fewer hits than false alarms. To achieve that, a person needs to do the opposite of what is asked. However, it can happen that on a small number of trials, the participant’s performance falls to chance level and, just by chance, you get a d’ just below 0. You might see this, for instance, in difficult tasks if you analyse blocks independently.\nSo, to recap, d’ measures a person’s sensitivity in a task by accounting for correct and incorrect responses in trials containing the target characteristic and lapse/ catch trials."
  },
  {
    "objectID": "educating/page-reveal.html#bias",
    "href": "educating/page-reveal.html#bias",
    "title": "Introduction to Signal Detection Theory",
    "section": "Bias",
    "text": "Bias\nAnother aspect of performance we might be interested in investigating is whether someone has a tendency to provide one specific answer instead of another. For instance, in conditions of uncertainty (like the majority of psychological tasks), you might be more prone to report something - maybe you believe that this will make the experimenter happy (it doesn’t). Or maybe you are left-handed, and you tend to report more with your left hand - randomisation is key in experiments, isn’t it? This is problematic if we want to assess the sensitivity of a person. Because the same d’ can be achieved with multiple combinations of hits and false alarms, it is possible that two people can score the same sensitivity, even when one is really trying their best in the task and the other has a strong bias for providing one specific response. Obviously, the two situations are not the same and we should be aware of that. Let’s see another example.\nTwo people complete our Is Wally there? task. Here are their hit rate, false alarm rate and d’ scores:\n\n\n\n\nPerson A\nPerson B\n\n\n\n\nHIT\n0.73\n0.91\n\n\nFALSE ALARM\n0.08\n0.25\n\n\nd’\n2.02\n2.02\n\n\n\nWow, two different performances but the same sensitivity values!\nAlright, you probably got the point now. So, what can we do about this? The answer is simple: we want to find a measure that represents whether a person’s sensitivity is unbiased or not. If not, towards which answer the bias is. Thankfully, SDT provides a simple answer to this question: the criterion or c. c is derived again from hits and false alarms and, for our simple case of 1AFC tasks, it is computed as:\n\\[c = - \\frac{Z(\\text{hit rate}) + Z(\\text{false alarm rate})}{2}\\] c assumes a value of 0 where the responder is unbiased. Positive values indicate a bias towards not reporting something. Negative values indicate a bias towards reporting something. The reason for this is the relationship between false alarms and misses and between hit rate and correct rejection. We won’t get into these details now, but we can build some intuition and connect the discussion back to sensitivity by looking at an updated version of our ROC plot.\n\n\nCode\n# Look at code above to understand this\ntriangle_vertex_low &lt;- data.frame(\n    x = c(0, 0, 0.5),\n    y = c(0.01, 1, 0.51)\n)\n\ntriangle_vertex_high &lt;- data.frame(\n    x = c(0.5, 0, 1),\n    y = c(0.51, 1, 1)\n)\n\nroc_data_c &lt;- roc_data %&gt;% \n    filter(dprime &gt;= 0)\n\nroc_c_plot &lt;- ggplot(roc_data_c, aes(x=fa, y=hit, frame=dprime)) +\n    geom_polygon(data = triangle_vertex_low, \n                 aes(x=x, y=y), \n                 inherit.aes = FALSE,\n                 fill = \"#fc7b54\",\n                 alpha = 0.25) +\n        geom_polygon(data = triangle_vertex_high, \n                 aes(x=x, y=y), \n                 inherit.aes = FALSE,\n                 fill = \"#008080\",\n                 alpha = 0.25) +\n    annotate(geom = \"text\", x=0.15, y=0.25, label=\"c&gt;0\") +\n    annotate(geom = \"text\", x=0.65, y=0.75, label=\"c&lt;0\") +\n    geom_segment(aes(x=0.5, y=0.5, xend=0, yend=1)) +\n    geom_line(color=\"purple\", linewidth=1.5) +\n    geom_segment(aes(x=0, y=0, xend=1, yend=1)) +\n    labs(\n        x = \"FA RATE\",\n        y = \"HIT RATE\",\n        title = \"d'\"\n        \n    ) +\n    theme_minimal() +\n            coord_fixed(xlim = c(0,1), \n                        ylim = c(0,1), \n                        expand = TRUE)\n\n\nggplotly(roc_c_plot)\n\n\n\n\n\n\nSet the d’ to 2.02, from our example above. Now, if we look at participant A, we see that it falls within the orange area. This indicates a tendency for this participant to report that Wally is not in a picture. Participant B, instead, falls within the green area, which indicates a tendency to report that Wally is there. Indeed, their hit rate is high, but their false alarm rate is too! In other words, while the first participant has a NO bias, the second one has a YES bias."
  },
  {
    "objectID": "educating/page-reveal.html#noise-and-models",
    "href": "educating/page-reveal.html#noise-and-models",
    "title": "Introduction to Signal Detection Theory",
    "section": "NOISE and MODELS",
    "text": "NOISE and MODELS\nAfter this brief intro to the two main SDT measures, we need to talk about how people make decisions. In doing so, we will build more insight into d’ and c. To keep the discussion simple, we will solely focus on 1AFC tasks, as they are one-dimensional and easy to understand. Just know that 2AFC tasks are simply a 2-dimensional version of 1AFC tasks.\nIn 1AFC tasks, the person is asked to judge one stimulus at a time and to provide one of two possible answers - commonly YES or NO, though not always. Because our Wally task is a YES-NO task, I’ll go with this, though you can swap these answers for anything you want: LEFT and RIGHT, NEW and OLD, REMEMBERED and FORGOTTEN, etc… Look at the picture below\n Wally is just on the right of the white and green umbrella. How did we find it? Well, we needed to sieve through the wall of information presented to our eyes. Specifically, irrelevant information. Or, as we like to call it, noise. To say that Wally is here means that we processed the noise within the image as long as Wally, the signal, and decided that the evidence favouring the presence of the signal is higher enough for us to say YES, Wally is here! Another way to say this is that we have collected enough evidence for us to state that we saw something relevant.\nTwo things are at play here:\n\nNoise vs Signal\nEnough evidence\n\nThese are intertwined, as are most things in life. This is a good enough reason for me to start discussing the second point, completely disregarding the order of the list I wrote. So… enough evidence. How much is enough? Well, it depends. All of us have a different threshold. Some of you might have originally said that Wally was not in that picture. The strength of the signal (Wally standing there creepily wearing a beanie on a beach) was not strong enough to overpower the noise of all those red togs. Others, instead, might have said yes, their eyes are well attuned to spot people who are going to get a heat stroke. Or, some of you might be biased towards saying they did not see anything (don’t worry, I’m not the police, you can tell me where Wally is). Others might have a bias for YES.\nSweet, the threshold for enough evidence is determined by (a) the stimulus itself and (b) the personal bias. The stimulus is a mix of noise and signal so that the closer the noise is to the signal (e.g. all the red and stripes in the picture) and the weaker the signal itself (imagine a teeny-tiny Wally), the more difficult it is to separate the two. We are now at the first point on the list. Obviously, noise and signal are two very broad terms, but this is good for us because we can do what every good scientist does when dealing with something vague: create a normal distribution. The simplest - but powerful - SDT model to explain performance in a 1AFC task is a Gaussian Model, where the probability of classifying something as noise (NO) and the probability of classifying something a signal (YES) are described by two Gaussian.\n\nOpen app in new page\nPlay around with this app. The two curves represent noise and signal. These curves, specifically, tell you the probability of something being noise (or signal) given a specific value of one dimension of your stimulus (x-axis: here can be anything, image contrast, familiarity, etc…). The simplest model implies that the two curves have equal variance, though this assumption can be relaxed. Try to modify the noise spread or the signal spread.\nBy modifying the d’ value, you see that the curves get closer or farther away. Why the distance between the centres of these two distributions reflects sensitivity? Well, higher sensitivity means that you can correctly discriminate a signal (Wally) even when it looks like noise (everything else in the scene). That is, you are able to obtain a high hit rate and a low false alarm rate even when the two distributions overlap.\nWhen do you report a signal? When you have gathered enough evidence in support of the signal. Enough here is determined by c, which is the vertical line. There are a couple of things to note about c. Firstly, note that its placement determines the proportion of hit rate and false alarms. The hit rate is represented by the area under the noise+signal curve above and beyond the c line (how many times something that is a signal is defined as a signal). Similarly, The false alarm rate is defined by the area under the noise curve above and beyond the c line (how many times noise has been classified as signal). Toggle the Noise and Noise+ Signal distributions. Moreover, note that the value of c defines where you end up on a d’ ROC line. That is all the different combinations that give rise to the same d’ reflect different levels of bias.\nHope this is useful!"
  },
  {
    "objectID": "educating/eeg_preprocessing.html",
    "href": "educating/eeg_preprocessing.html",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "This page will be a work-in progress project. I will update when as the opportunity comes\nThe goal of today’s lab meeting is to gain insight into the most common EEG data cleaning (preprocessing) steps. I’m going to use MNE (Python) for a couple of reasons. Firstly, we will introduce an alternative to MATLAB in the hope that we can all move away from it for our studies. Secondly, because the point of today is not to learn the “how-to” preprocess data (how to make a script, what functions to use, how to code, etc…). I believe that is the easiest part, and there are many tutorials around. Instead, we will try to gain a deeper understanding of what we are doing to the data every time we perform specific cleaning steps. I believe this is a more interesting and useful approach, especially in an hour.\nWe will start by loading the EEG data. This is a pilot recording I have conducted on myself while performing Simon’s task.\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nprint('hey')\n\n\nThe first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()\n\n\n\n\nIf the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample\nnew_sampling_rate = 50\n\ndef g(srate=500):\n    number_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * srate)\n    step_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\n    downsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    downsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    \n    print(f\"Retaining: {number_of_points_to_retain} out of {len(sine_signal)}\")\n    \n    plt.plot(samples_vector, sine_signal, '-k', linewidth=8, alpha=0.3)\n    plt.scatter(downsampled_time, downsampled_signal, c='r', s=6, alpha=0.6)\n    plt.plot(downsampled_time, downsampled_signal, '-r', linewidth=1)\n    plt.rcParams['figure.figsize'] = [10, 5]\n    plt.show()\n    \n\n    \ninteractive(g, srate=widgets.IntSlider(min=5, max=100, step=1, value=50))\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied to the data to avoid distortions (aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)\n\n\n\nThis is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, 'k', linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), 'r')\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors='k', linestyles='dashed', linewidth=3);\n\nHere we filtered only the second half of the signal. Let’s see it’s frequency decompostion\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to='magnitude', mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin='lower', aspect='auto', cmap='viridis', extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color='gray', linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors='r', linestyles='dashed', linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2));\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 30\ndef filter_order_plot(filter_order=4):\n    b,a = signal.butter(filter_order, cutoff_frequency, 'low', analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    \n    fig, axs = plt.subplots(2, 1)\n    \n    axs[0].plot(w, 20 * np.log10(abs(h)), c='r', label=f\"Order: {filter_order}\")\n    axs[0].vlines(30, -50, 5, colors='k')\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    \n    axs[1].plot(t, y, c=\"orange\")\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n    plt.tight_layout()\n\n    \ninteractive(filter_order_plot, filter_order=widgets.IntSlider(min=1, max=20, step=1, value=4))\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\n\n# Explore filter lenght\nsample_rate      = 1000\ncutoff_frequency = 50\ntrans_bandwidth  = 10\n\ndef filter_components(filter_lenght_s=0.2):\n    filter_order        = int(round((sample_rate * filter_lenght_s))) + 1\n    kaiser_coefficients = signal.firwin(filter_order, 50, fs=1000, window=(\"blackman\"))\n    kaiser_w, kaiser_h  = signal.freqz(kaiser_coefficients, fs=1000)\n    fig, ax = plt.subplots()\n    ax.plot(kaiser_w, 20 * np.log10(abs(kaiser_h)))\n    ax.fill_between((cutoff_frequency, cutoff_frequency+trans_bandwidth), 5, -150, alpha=0.35, color='red')\n    ax.set_xlim((0, 175))\n    ax.set_ylim((-150, 5))\n\ninteractive(filter_components, filter_lenght_s=widgets.FloatSlider(min=0, max=1.5, step=0.1, value=0.2, description=\"FL(s)\"))\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.fill_between((50, 50+10), 5, -200, alpha=0.1, color='red')\n    ax.set_xlim((0, 200))\n    ax.legend()\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None) raw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)\n\n\n\nLots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section).\n\n\n\nOnce you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "educating/eeg_preprocessing.html#looking-at-the-data",
    "href": "educating/eeg_preprocessing.html#looking-at-the-data",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "The first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()"
  },
  {
    "objectID": "educating/eeg_preprocessing.html#downsampling",
    "href": "educating/eeg_preprocessing.html#downsampling",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "If the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample\nnew_sampling_rate = 50\n\ndef g(srate=500):\n    number_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * srate)\n    step_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\n    downsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    downsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    \n    print(f\"Retaining: {number_of_points_to_retain} out of {len(sine_signal)}\")\n    \n    plt.plot(samples_vector, sine_signal, '-k', linewidth=8, alpha=0.3)\n    plt.scatter(downsampled_time, downsampled_signal, c='r', s=6, alpha=0.6)\n    plt.plot(downsampled_time, downsampled_signal, '-r', linewidth=1)\n    plt.rcParams['figure.figsize'] = [10, 5]\n    plt.show()\n    \n\n    \ninteractive(g, srate=widgets.IntSlider(min=5, max=100, step=1, value=50))\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied to the data to avoid distortions (aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)"
  },
  {
    "objectID": "educating/eeg_preprocessing.html#filtering",
    "href": "educating/eeg_preprocessing.html#filtering",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "This is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, 'k', linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), 'r')\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors='k', linestyles='dashed', linewidth=3);\n\nHere we filtered only the second half of the signal. Let’s see it’s frequency decompostion\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to='magnitude', mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin='lower', aspect='auto', cmap='viridis', extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color='gray', linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors='r', linestyles='dashed', linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2));\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 30\ndef filter_order_plot(filter_order=4):\n    b,a = signal.butter(filter_order, cutoff_frequency, 'low', analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    \n    fig, axs = plt.subplots(2, 1)\n    \n    axs[0].plot(w, 20 * np.log10(abs(h)), c='r', label=f\"Order: {filter_order}\")\n    axs[0].vlines(30, -50, 5, colors='k')\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    \n    axs[1].plot(t, y, c=\"orange\")\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n    plt.tight_layout()\n\n    \ninteractive(filter_order_plot, filter_order=widgets.IntSlider(min=1, max=20, step=1, value=4))\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\n\n# Explore filter lenght\nsample_rate      = 1000\ncutoff_frequency = 50\ntrans_bandwidth  = 10\n\ndef filter_components(filter_lenght_s=0.2):\n    filter_order        = int(round((sample_rate * filter_lenght_s))) + 1\n    kaiser_coefficients = signal.firwin(filter_order, 50, fs=1000, window=(\"blackman\"))\n    kaiser_w, kaiser_h  = signal.freqz(kaiser_coefficients, fs=1000)\n    fig, ax = plt.subplots()\n    ax.plot(kaiser_w, 20 * np.log10(abs(kaiser_h)))\n    ax.fill_between((cutoff_frequency, cutoff_frequency+trans_bandwidth), 5, -150, alpha=0.35, color='red')\n    ax.set_xlim((0, 175))\n    ax.set_ylim((-150, 5))\n\ninteractive(filter_components, filter_lenght_s=widgets.FloatSlider(min=0, max=1.5, step=0.1, value=0.2, description=\"FL(s)\"))\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.fill_between((50, 50+10), 5, -200, alpha=0.1, color='red')\n    ax.set_xlim((0, 200))\n    ax.legend()\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None) raw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)"
  },
  {
    "objectID": "educating/eeg_preprocessing.html#find-bad-channels",
    "href": "educating/eeg_preprocessing.html#find-bad-channels",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "Lots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section)."
  },
  {
    "objectID": "educating/eeg_preprocessing.html#interpolation",
    "href": "educating/eeg_preprocessing.html#interpolation",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "Once you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "educating/educating_intro.html",
    "href": "educating/educating_intro.html",
    "title": "Educating",
    "section": "",
    "text": "One of my favorite aspects of doing a PhD is teaching. I enjoy the educational aspect of academia, and I focus a lot on how to be a better educator. In this section, you’ll find a set of materials, projects, and data all related to my experience as an educator."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "~/About",
    "section": "",
    "text": "Daniele completed his Bachelor of Science in Psychology and Communication at the University of Milan - Bicocca in 2018. He joined the University of Auckland in 2021 as an Honours student. He is now starting a PhD working with Paul Corballis, director of the Cognitive Psychophysiology Lab and co-advised by Christopher Erb. He is interested in investigating the neural correlates of consciousness using EEG, eye-tracking and whatever machine he can get his hands on. He has a second identity as a magician and his assistant is a tiny blue plunger."
  },
  {
    "objectID": "educating/eeg_insights.html",
    "href": "educating/eeg_insights.html",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "This page will be a work-in progress project. I will update as the opportunity comes. An interactive Jupyter notebook and a Python script can be found here\nThe goal of today’s lab meeting is to gain insight into the most common EEG data cleaning (preprocessing) steps. I’m going to use MNE (Python) for a couple of reasons. Firstly, we will introduce an alternative to MATLAB in the hope that we can all move away from it for our studies. Secondly, because the point of today is not to learn the “how-to” preprocess data (how to make a script, what functions to use, how to code, etc…). I believe that is the easiest part, and there are many tutorials around. Instead, we will try to gain a deeper understanding of what we are doing to the data every time we perform specific cleaning steps. I believe this is a more interesting and useful approach, especially in an hour.\nWe will start by loading the EEG data. This is a pilot recording I have conducted on myself while performing Simon’s task.\n\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n\n\nThe first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()\n\n\n\n\nIf the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample (MODIFY THE NEXT LINE)\nnew_sampling_rate = 50\n\nnumber_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * new_sampling_rate)\nstep_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\ndownsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\ndownsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n\nplt.plot(samples_vector, sine_signal, \"-k\", linewidth=8, alpha=0.3)\nplt.scatter(downsampled_time, downsampled_signal, c=\"r\", s=6, alpha=0.6)\nplt.plot(downsampled_time, downsampled_signal, \"-r\", linewidth=1)\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.show()\n\n\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied and a sliding average is computed to avoid distortions (eg. aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)\n\n\n\nThis is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, \"k\", linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), \"r\")\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors=\"k\", linestyles=\"dashed\", linewidth=3)\n\n&lt;matplotlib.collections.LineCollection at 0x11cca28f6e0&gt;\n\n\n\n\n\nHere we filtered only the second half of the signal. Let’s see it’s time-frequency decomposition.\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to=\"magnitude\", mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\", extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color=\"gray\", linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors=\"r\", linestyles=\"dashed\", linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2))\n\n(0.0, 2.0)\n\n\n\n\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 50\nfilter_orders    = [2, 3, 4, 8, 10, 12]\n\n\nfig, axs = plt.subplots(2, 1)\nplt.tight_layout()\nfor filter_order in filter_orders:\n    b,a = signal.butter(filter_order, cutoff_frequency, \"low\", analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    axs[0].plot(w, 20 * np.log10(abs(h)), label=f\"Order: {filter_order}\")\n    axs[0].vlines(50, -50, 5, colors=\"k\")\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    axs[0].legend()\n    \n    axs[1].plot(t, y)\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n\n\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\n# FIR\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.set_xlim((0, 200))\n    ax.legend()\n\n\n\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None)\nraw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)\n\n\n\nLots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section).\n\n\n\nOnce you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "educating/eeg_insights.html#looking-at-the-data",
    "href": "educating/eeg_insights.html#looking-at-the-data",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "The first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()"
  },
  {
    "objectID": "educating/eeg_insights.html#downsampling",
    "href": "educating/eeg_insights.html#downsampling",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "If the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample (MODIFY THE NEXT LINE)\nnew_sampling_rate = 50\n\nnumber_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * new_sampling_rate)\nstep_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\ndownsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\ndownsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n\nplt.plot(samples_vector, sine_signal, \"-k\", linewidth=8, alpha=0.3)\nplt.scatter(downsampled_time, downsampled_signal, c=\"r\", s=6, alpha=0.6)\nplt.plot(downsampled_time, downsampled_signal, \"-r\", linewidth=1)\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.show()\n\n\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied and a sliding average is computed to avoid distortions (eg. aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)"
  },
  {
    "objectID": "educating/eeg_insights.html#filtering",
    "href": "educating/eeg_insights.html#filtering",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "This is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, \"k\", linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), \"r\")\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors=\"k\", linestyles=\"dashed\", linewidth=3)\n\n&lt;matplotlib.collections.LineCollection at 0x11cca28f6e0&gt;\n\n\n\n\n\nHere we filtered only the second half of the signal. Let’s see it’s time-frequency decomposition.\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to=\"magnitude\", mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\", extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color=\"gray\", linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors=\"r\", linestyles=\"dashed\", linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2))\n\n(0.0, 2.0)\n\n\n\n\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 50\nfilter_orders    = [2, 3, 4, 8, 10, 12]\n\n\nfig, axs = plt.subplots(2, 1)\nplt.tight_layout()\nfor filter_order in filter_orders:\n    b,a = signal.butter(filter_order, cutoff_frequency, \"low\", analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    axs[0].plot(w, 20 * np.log10(abs(h)), label=f\"Order: {filter_order}\")\n    axs[0].vlines(50, -50, 5, colors=\"k\")\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    axs[0].legend()\n    \n    axs[1].plot(t, y)\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n\n\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\n# FIR\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.set_xlim((0, 200))\n    ax.legend()\n\n\n\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None)\nraw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)"
  },
  {
    "objectID": "educating/eeg_insights.html#find-bad-channels",
    "href": "educating/eeg_insights.html#find-bad-channels",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "Lots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section)."
  },
  {
    "objectID": "educating/eeg_insights.html#interpolation",
    "href": "educating/eeg_insights.html#interpolation",
    "title": "EEG Preprocessing Insights",
    "section": "",
    "text": "Once you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#task",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#task",
    "title": "Is Wally There?",
    "section": "Task",
    "text": "Task\nEach image will be presented for 5 seconds. Write down whether you Wally is in the image or not"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-1",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-1",
    "title": "Is Wally There?",
    "section": "Trial 1",
    "text": "Trial 1"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-2",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-2",
    "title": "Is Wally There?",
    "section": "Trial 2",
    "text": "Trial 2"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-3",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-3",
    "title": "Is Wally There?",
    "section": "Trial 3",
    "text": "Trial 3"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-4",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-4",
    "title": "Is Wally There?",
    "section": "Trial 4",
    "text": "Trial 4"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-5",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-5",
    "title": "Is Wally There?",
    "section": "Trial 5",
    "text": "Trial 5"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-6",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-6",
    "title": "Is Wally There?",
    "section": "Trial 6",
    "text": "Trial 6"
  },
  {
    "objectID": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-7",
    "href": "educating/signal_Detection_files/is_waldo_there_slides.html#trial-7",
    "title": "Is Wally There?",
    "section": "Trial 7",
    "text": "Trial 7"
  },
  {
    "objectID": "educating/students_review.html",
    "href": "educating/students_review.html",
    "title": "Students Review",
    "section": "",
    "text": "When I started my PhD, I have also begun tutoring every semester. I enjoy teaching, interacting with students and sharing the knowledge I am passionate about. I often think about how to be a better educator, how to set up my tutorials and how to create a positive environment in my classes. I like to look at teaching through the lens of my past as a magician. Teaching is an act with a story to convey, attention to grab, and audience participation should be encouraged.\nMy idea of teaching may be far away from what the students want and need, I know. Unfortunately, the courses I teach do not ask students to provide feedback on their tutors, and I find this absurd. We are an integral part of their education experience, we should be able to know how we are doing and what we should work on. So, I have been actively collecting students’ evaluations for all the courses I teach. I do this at the end of each semester through an anonymous survey that students can voluntarily fill out.\nIn the name of transparency, I have decided to post all the anonymous feedback here. I will update this page each semester, and I will address the comments and reflect on what I need to work on.\nAs a side note, I try to encourage students to provide constructive feedback, and I try to stress that negative feedback is more than welcome. I really want to improve and work on how to be better, and negative feedback, if constructive, helps highlight things to change and work on. As such, I won’t hide any comments except if they are un-constructively negative or contain some sensitive information. I will add a note if this happens.\nOne of the survey’s question is to provide 3 words that describe me as a tutor. Above is a summary of the words provided so far",
    "crumbs": [
      "Educating",
      "Students Review",
      "Students Review"
    ]
  },
  {
    "objectID": "educating/students_review.html#students-marks",
    "href": "educating/students_review.html#students-marks",
    "title": "Students Review",
    "section": "Students’ marks",
    "text": "Students’ marks\nLet’s start with a simple and fun (hopefully) section where I swap the roles with the students and they are able to grade my work.\n\n\n\n\n\n\n\n\n\nEach square represents 1% of the overall scores received by students. I’m glad to see that the vast majority considered my tutorial “Above average” and there are no Below average.\nIn terms of marks, the grades below are scaled by their relative proportion. The larger the letter, the higher the number of students that selected that grade. Again, I’m happy to see the A+ and A being large!",
    "crumbs": [
      "Educating",
      "Students Review",
      "Students Review"
    ]
  },
  {
    "objectID": "educating/students_review.html#what-students-appreciate",
    "href": "educating/students_review.html#what-students-appreciate",
    "title": "Students Review",
    "section": "What students appreciate",
    "text": "What students appreciate\nNow, we get to the core of the feedback. I will highlight at the top the comments that I think represent the major ideas. Nonetheless, at the end of the section, you can find an exhaustive table with all the feedback.\n\nYou were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping.\n\n\nYou did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it.\n\n\nTutorials are fun and friendly environments.\n\nThere are three main elements I think students appreciate:\n1- The content should be presented differently from the lecturers. This makes the tutorial feel less like a simple repetition and more like an opportunity to learn more and/or understand better.\n2- Give people time to ask questions. We all vary with how comfortable we are in asking questions in class. I encourage discussions, debates and questions since the very first tutorial to create an environment where everyone feels comfortable being an active part.\n3- Engage. We all agree that listening to one person talking for two hours is boring. So why do this? Showing a more fun and natural side helps create a connection, keep the attention high and make the tutorial time more enjoyable.\n\nI felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good.\n\nI agree on this point. One of the aspects I need to work more on is structure (more on this in the next session). I do not want to have everything predefined, as this is counterproductive, but I will try to define at the beginning of the tutorial the key areas I will discuss.\n\n\n\n\n\n\nFeedback\n\n\n\n\nInteractiveness, enthusiasm and motivation\n\n\nI really loved your passion and the time you took to check everybody understood. Also appreciated when you would draw diagrams to help us understand when we were all struggling to understand.\n\n\nI enjoyed the activities we did!! It was fun especially with how the content was pretty full on but the fact we got to discuss often with our peers was a good aspect for me. I also liked how you went through the content first explaining everything we similarly learnt in lectures before we started because hearing it from the way you talked about it helped me understand lecture content a bit easier. And the white board examples and the pictures helped immensely!!!! It made things so much easier to grasp so please continue that because reading of PowerPoint slides for me personally doesn’t feel engaging\n\n\nEngaged with smaller group discussions by walking around the room Answered questions directly and was ready to do so in the middle of a presentation\n\n\nTutorials felt well prepared and always were well delivered, the content was explained clearly and sometimes having clarification of lecture material was super helpful. Your delivery was engaging and even content I'm not super into wasn't boring so thank you! You seem to be pretty into the topics that we covered which really helps. The feedback on the essay was pretty insightful too.\n\n\nTrying to engage the class rather than lecturing, despite a lack of input form us 😆\n\n\nAll content was explained clearly, and questions from the class were answered swiftly and very well. The way we walked through different programmes was extremely helpful for understanding the way they work and how data is arranged and understood.\n\n\nYou were fun and engaging, incredibly knowledgeable about the topics you were teaching, spent time with us one-on-one or in pairs when able and provided individual feedback + guidance\n\n\nConstructive feedback allowing us to critically think\n\n\nAlways trying to make things fun and using interactive ways of learning, checking that everyone is understanding was really nice\n\n\nI find it really helpful that you go in depth into the concepts as it helps me understand more. i also really appreciate in the tips you provide in our academic writing etc finding sources, i find them really helpful :)\n\n\nI appreciated the quality of the content and how it was useful and complementary to the lectures\n\n\nEverything is explained very well, you’re very patient with answering questions no matter how many are asked or if you get interrupted, you know the content well and relate it to us in a very simple and good way as well, you give us a very nice breakdown of how to write things and what to include in assignments (for the TBI essay) and in general you make it a very nice environment to be in and your chill/relaxed and funny vibe makes the tutorials far more engaging\n\n\nAsked us questions that helped go to actively revise.\n\n\nSupport! You were very supportive and accurately explained what was required to do well in this class, and explained what we did not need to know as extensively - which was much appreciated as often times labs can be seen as just an overload of information.\n\n\ni liked how you explained things in a different way to the lecturers which often made more sense to me. or having the content presented in a different way deepened my understanding of it. i liked how you took the time to explain something fully to me again if i still didn’t understand it\n\n\nGiving supports to every student, really engaging with the class\n\n\ntutorials are fun and friendly environments\n\n\nLiterally so fun for 9am on a Friday! You are just so engaging and knowledgeable and really get us thinking. Thanks for being such a legend! I really enjoy these labs\n\n\nAlways try to make tutorials fun, friendly demeanour in general, and genuinely trying to help us, thank you!\n\n\nAlways have interesting teaching way\n\n\nGood at explaining thoroughly and gives a real opportunity for students to ask questions. Also answers questions as best as possible. Takes time to check in on each student/group individually and welcoming to ask questions to. Seems like you genuinely want to help your students understand everything clearly and do well.\n\n\nYou have always been very transparent and supportive of us. You helped us with everything you could and always gave us the right advice, pushing us to do so much more. The feedback on our assignments has been very thorough which also personally has helped me to incorporate into assignments from my other courses.\n\n\nSupport was great! feedback on the assignments was super helpful\n\n\nReally encouraging and great with positive feedback. I really liked the feedback I received on my assignments throughout the course as it guided me as to how to improve in my work.\n\n\nExplained alot of concepts and answers thoroughly and appreciated that he took the time to go through everything. Feedback felt subjective and catered to our learning\n\n\nI felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good.\n\n\nThe feedback you gave on my blog post was super helpful! I didn't realise how much blog was lacking something big and your feedback helped me see it but you also provided specific tips to integrate so I didn't feel lost knowing I had alot of work to improve on :) You were also super on top of replying in teams and being there for support so that was something else that made you such an awesome tutor.\n\n\nI appreciate how friendly and easy to approach you were. Unlike some other tutors, you really made it easy for us to ask questions - esp questions that we found hard to ask to other lecturers. Assignment feedbacks were extremely detailed and helpful and also very fair I personally think. I also like how chill you were - as long as we followed the rubric and did everything, you were very fair with your marking - literally made capstone life extremely enjoyable and rewarding to have a tutor who understood what you were doing!\n\n\nFriendly manner, always willing to help\n\n\nFeedback on assignments are good, you take the time to listen and help and make sure we understand\n\n\nVery helpful, compassionate, supportive and understanding, explained really well\n\n\nExplaining things throughly, questioning us and making us work things through\n\n\nGood feedback & ways of trying to improve our learning.\n\n\nYou were very helpful and understanding of our places in knowledge. Your feedback was helpful in allowing for a better understanding.\n\n\nThe content covered was always relevant and helpful. Trying to engage the class\n\n\nYou were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping.\n\n\nAppreciated how patient you were, especially re-explaining something over and over again so our class understood. Your feedback and also going over and beyond.\n\n\ni appreciated fast reply’s on email, and also giving feedback on assignments and work. friendly work environment\n\n\nYou did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it.",
    "crumbs": [
      "Educating",
      "Students Review",
      "Students Review"
    ]
  },
  {
    "objectID": "educating/students_review.html#what-students-think-i-should-do-better",
    "href": "educating/students_review.html#what-students-think-i-should-do-better",
    "title": "Students Review",
    "section": "What students think I should do better",
    "text": "What students think I should do better\n\nSometimes the discussion time in tutorials felt too long and often we’d finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway. Occasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes.\n\nI will try next year to have more class discussions and less small group work. A couple more people pointed this out. I believe that small groups would have encouraged people to talk more by reducing the pressure of having to speak out loud in front of everyone. However, doing this takes time that could be used differently.\n\nFound the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc.\n\nThis is a fair point. I almost always take the full 2 hours, most likely because I like to digress. As pointed out above, next year I will try to structure each tutorial a bit more. Specifically, I will highlight at the beginning the major point we will discuss, and I will add a recap at the end. However, I will keep some improvisation. I believe that having just a general structure of what to talk about allows for the exploration of topics, concepts and ideas that otherwise won’t be addressed. Tutorials, in my view, are not just a recap of the lectures but a moment where we can dive deeper into a topic or create connections across different areas. They are useful to broaden our understanding of the concepts discussed in the lectures and, by doing so, learn the material better.\n\nWould appreciate more concise marking, and felt marking scores were abit harsh sometimes compared to other students or the feedback received.\n\nGiven the multiple comments appreciating the lengthy feedback on the essays, I will not plan to change this. Assignments and exams are not just a test but an opportunity to improve and gain skills. By providing detailed feedback, students are able to improve their future work. I agree that I can be strict with my marking. This is partly caused by my study background and the way I have been assessed. Partly because I try to mark focusing hardly on the reasoning. That is, I don’t see just repeating the content from a book or lectures as being enough for an A+. The reasoning behind an answer is important. Nonetheless, in the class I teach, us tutors try to match our criteria, so if I am being too harsh, other tutors and professors will call me out on this before the marks are finalized.\n\n\n\n\n\n\nanswers\n\n\n\n\nNothing really\n\n\nOnly suggestion I have is to speak slower. Your excitement and English are amazing, but when you speak really fast with excitement, it made it difficult to understand and keep up sometimes.\n\n\nThis isn't really a you thing but it was better communication with the lecturers (not really your fault) as it's kind of disheartening when tutors don't know whata's going on with what lecturers are doing for example when lab quizzes results are released or even whats going on with exams. I also wish we got to discuss with more of our peers around the room, not just the people we sat with but I guess that’s kind of hard with the layout of the room\n\n\nSometimes skipped over points too quick.\n\n\nSometimes the discussion time in tutorials felt too long and often we'd finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway.\n\n\nOccasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes\n\n\nHonestly, thought you were amazing. Maybe you could just slow down sometimes with your explanations because it can be a bit hard to keep up with\n\n\nGenuinely cannot think of anything sorry\n\n\nI am happy with the way the tutorials are going :)\n\n\nI don't believe there is anything you could do better :) keep up the great work!\n\n\nHonestly nothing, the way you do things is amazing\n\n\nSummarise the lab at the end\n\n\nI think maybe the first few labs had a LOT of information to be taught and not so much time for questions to be asked. This isn’t so much a tutor specific issue, rather it is a course coordination issue of how much is taught in those labs\n\n\noccasionally (not often) i couldn’t understand what you were saying when you spoke fast\n\n\nsome tutorials cover ground that has already been covered in previous years, such as essay writing basics, though this is likely more of an issue with the general course coordination\n\n\nMore magic please!\n\n\nKeep doing what you're doing :)\n\n\nMaybe speaking a little bit slower is better\n\n\nMore of feedback for tutorials in general rather than the tutor but Maybe, especially in the first lesson, have more focus on having students introduce themselves in the class and have opportunities to work together and discuss with each other so that students can build relationships to each other and to tutor.\n\n\nI honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done.\n\n\nI honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done.\n\n\ncommunication could've been better, at times was difficult to get our points across as our research differed greatly from yours, which is completely okay but having a tutor who knows the topic and understands the specific research would have made it a wee bit easier for you and us.\n\n\nWould appreciate more concise marking, and felt markinf scores were abit harsh sometimes compared to other students or the feedback received\n\n\nMaybe provide more tips on what we should be careful of when doing each assignment - like providing common places where student often lose marks or mess up\n\n\nNothing much - everything was all good! Don't really have much feedback\n\n\ndeeper explanation of that weeks lecture content\n\n\nI think you are good\n\n\nMore magic and pizza\n\n\nNothing I love this tutor\n\n\nDon’t overestimate students understanding of topics\n\n\nI think maybe just more in depth feedback and being super critical, even though people might not like it it’s actually very helpful.\n\n\nFound the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc.",
    "crumbs": [
      "Educating",
      "Students Review",
      "Students Review"
    ]
  },
  {
    "objectID": "educating/students_review.html#extra-comments",
    "href": "educating/students_review.html#extra-comments",
    "title": "Students Review",
    "section": "Extra comments",
    "text": "Extra comments\nAs my survey is meant to be quick, I leave the opportunity to express other ideas freely.\n\n\n\n\n\n\nanswers\n\n\n\n\nGreat job! Thanks for being a great tutor!\n\n\nThanks so much for your help and enthusiasm! I found Psych 305 to be a really difficult class, but having you as a tutor helped me so much with the work content because your excitement made me get more excited about it, which helped me engage and learn better! I really loved having you teach us! All the best. :)\n\n\nThanks for a great semester!!\n\n\nThank you for teaching us over the past semester, and good luck with your PhD!\n\n\nGreat job. I personally hate tutorials, and you made them fun and engaging and made me look forward to coming :)\n\n\nThank you for a lovely lab!\n\n\nI think you are a great tutor (:\n\n\nthank you for all your hard work :)\n\n\nThank you!\n\n\nI'm not actually in your normal stream but joined it once when I couldn’t attend my normal one and I enjoyed your tutorial so much that I made it my regular stream and have been coming along ever since\n\n\nLOVE YOU DANIELE!!! You are an awesome tutor\n\n\nyou never gave us the pizza making class :(\n\n\nPlease keep tutoring! You're literally my fav!\n\n\nThank you for being such a lovely tutor this semester, I wish you luck on all your future eneavours.\n\n\nNo\n\n\nGreat mustache! And you did a great job giving students opportunity to ask questions individually (rather than to whole class) so felt more comfortable and open. Thanks!\n\n\nDaniele is an amazing tutor and an amazing person. With great professionalism that can be seen on our assignments feedback and advice, he is super friendly and ensures that you are not being negatively criticised. He is highly supportive and always pushes you to perform to the best of your ability.\n\n\nDaniele is an amazing tutor, who is thoughtful and empathetic. He is a great help to those who are stuck and need a direction.\n\n\nDaniele was my tutor for my year 3 capstone course of my degree. He was an amazing communicator who was ways able to convey feedback in an depth manner - because of this I was motivated improve my work rather than seeing it as a daunting task. He also always came to meetings with a positive attitude and willingness to help!\n\n\nDaniele is an extremely friendly tutor. Very thoughtful and always eager to help his students. His tutorial sessions were always helpful and his assignment feedbacks were very detailed and accurate. Would love to have him as my tutor for other classes too.\n\n\nYou are a great tutor. Continue to do the awesome work of teaching your students.\n\n\nDaniele is very calm in nature and provides a positive environment that allows for learning and is very entertaining when adding his magic into his tutorials.\n\n\nDaniele was a great tutor to have as he was always so nice to everyone and helpful when questions were asked. I learnt a lot in his class and it made me enjoy going to my tutorial because he was a good teacher.",
    "crumbs": [
      "Educating",
      "Students Review",
      "Students Review"
    ]
  },
  {
    "objectID": "exploring/kawakawa_2023_12_26.html",
    "href": "exploring/kawakawa_2023_12_26.html",
    "title": "Kinlock to Kawakawa track",
    "section": "",
    "text": "Location: Kinlock - Taupo Area\nDate: 2023/12/26\nIn the early morning of boxing day I decided to go out for a trail run on the Kinlock to Kawakawa bay track (K2K). The overall run was about 20Km, 10Km in each direction, which I completed in 2 hours. The track was easy, with not too much elevation (418m overall) and some nice viewing areas (but not too many). Very quiet and relaxing - although I know that later on in the day it gets busier with mountain bikers and climbers going over to Kawakawa."
  },
  {
    "objectID": "exploring/kawakawa_2023_12_26.html#notes-for-the-future",
    "href": "exploring/kawakawa_2023_12_26.html#notes-for-the-future",
    "title": "Kinlock to Kawakawa track",
    "section": "Notes for the future",
    "text": "Notes for the future\nNot much for this one. Trail running is great! Being immerse in the nature and not knowing exactly how much terrain you have covered (less reference points compared to running in the city) is a nice sensation."
  },
  {
    "objectID": "exploring/trash_running.html",
    "href": "exploring/trash_running.html",
    "title": "Trash Running",
    "section": "",
    "text": "I started running again recently and wanted to incorporate something different in my runs. Inspired by Beau Miles and by a growing and stronger environmental awareness, I decided to dedicate some of my runs to picking up some trash. As some studies have shown (Dur & Vollaard, 2014), a clean area tend to remain clean, while a littered area tends to induce more littering, probably in line with the Broken Windows Theory. The area where I live is not the most upkeep, so I thought doing some cleaning would help reduce littering. Even if it doesn’t, there are at least two benefits (excluding my personal well-being):\nI keep the plan simple and low-key. I collect as much trash as I can, starting by collecting everything I spot, no matter the size. Once I’m closer to capacity, I start to focus only on recyclable materials, like cans, plastic bottles, etc… Currently, I bring with me a few canvas bags that I fill up with trash. During one of the runs, I found a long and sturdy bamboo stick discarded on a pathway. I now attach the bags to it and distribute the load over my shoulders. This allows me to carry heavier bags and, in turn, pick up more trash. Plus, it’s funny to see people’s reaction to a guy running with a cane and bags loaded with trash. At the end of each run, I unload everything in my garden, sort every piece of litter and dispose of it in the proper bins.\nBelow, I will keep a diary of each (or most) run as a way to quantify how much litter I have picked up and to keep up with the motivation.\nI’m bad with photos, so I’ll try to take more and more often."
  },
  {
    "objectID": "exploring/trash_running.html#section",
    "href": "exploring/trash_running.html#section",
    "title": "Trash Running",
    "section": "2023/10/29",
    "text": "2023/10/29\n4 bags filled along with a box I found on the road over a 10Km run. This was the first trash-run I did. Funny thing, I picked up a very smelly bag at some point end realized only at home that it was filled with dead crabs.\n\n\n\nDead crabs and some trash\n\n\n\n\n\nDead crabs and smashed glass"
  },
  {
    "objectID": "exploring/trash_running.html#section-1",
    "href": "exploring/trash_running.html#section-1",
    "title": "Trash Running",
    "section": "2023/11/12",
    "text": "2023/11/12\nFilled up half of my recycling bin, mainly with cans, glass bottles, and takeaways material (bags, drink glasses, burger containers, etc…). Also, a few boxes and cardboard that have been discarded behind some bushes. The funny thing for this run were two bibles, in completely different sites. All found over 10Km.\n\n\n\nFilled bin"
  },
  {
    "objectID": "exploring/trash_running.html#section-2",
    "href": "exploring/trash_running.html#section-2",
    "title": "Trash Running",
    "section": "2023/12/30",
    "text": "2023/12/30\nUnfortunately I lost the photos for this one. Huge amount of cans for just a 6km run. basically 2 entire bags were just filled with cans. I suspect the holidays made the situation worst. I’ve also found a broken telephone that I need to bring to a proper recycling centre."
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "This page will be a work-in progress project. I will update when as the opportunity comes\nThe goal of today’s lab meeting is to gain insight into the most common EEG data cleaning (preprocessing) steps. I’m going to use MNE (Python) for a couple of reasons. Firstly, we will introduce an alternative to MATLAB in the hope that we can all move away from it for our studies. Secondly, because the point of today is not to learn the “how-to” preprocess data (how to make a script, what functions to use, how to code, etc…). I believe that is the easiest part, and there are many tutorials around. Instead, we will try to gain a deeper understanding of what we are doing to the data every time we perform specific cleaning steps. I believe this is a more interesting and useful approach, especially in an hour.\nWe will start by loading the EEG data. This is a pilot recording I have conducted on myself while performing Simon’s task.\n\n## Modules, directories and import data\nimport os\nfrom ipywidgets import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n\n\nThe first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()\n\n\n\n\nIf the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample\nnew_sampling_rate = 50\n\ndef g(srate=500):\n    number_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * srate)\n    step_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\n    downsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    downsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    \n    print(f\"Retaining: {number_of_points_to_retain} out of {len(sine_signal)}\")\n    \n    plt.plot(samples_vector, sine_signal, '-k', linewidth=8, alpha=0.3)\n    plt.scatter(downsampled_time, downsampled_signal, c='r', s=6, alpha=0.6)\n    plt.plot(downsampled_time, downsampled_signal, '-r', linewidth=1)\n    plt.rcParams['figure.figsize'] = [10, 5]\n    plt.show()\n    \n\n    \ninteractive(g, srate=widgets.IntSlider(min=5, max=100, step=1, value=50))\n\n\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied to the data to avoid distortions (aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)\n\n\n\nThis is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, 'k', linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), 'r')\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors='k', linestyles='dashed', linewidth=3);\n\n\n\n\nHere we filtered only the second half of the signal. Let’s see it’s frequency decompostion\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to='magnitude', mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin='lower', aspect='auto', cmap='viridis', extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color='gray', linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors='r', linestyles='dashed', linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2));\n\n\n\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 30\ndef filter_order_plot(filter_order=4):\n    b,a = signal.butter(filter_order, cutoff_frequency, 'low', analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    \n    fig, axs = plt.subplots(2, 1)\n    \n    axs[0].plot(w, 20 * np.log10(abs(h)), c='r', label=f\"Order: {filter_order}\")\n    axs[0].vlines(30, -50, 5, colors='k')\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    \n    axs[1].plot(t, y, c=\"orange\")\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n    plt.tight_layout()\n\n    \ninteractive(filter_order_plot, filter_order=widgets.IntSlider(min=1, max=20, step=1, value=4))\n\n\n\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\n\n# Explore filter lenght\nsample_rate      = 1000\ncutoff_frequency = 50\ntrans_bandwidth  = 10\n\ndef filter_components(filter_lenght_s=0.2):\n    filter_order        = int(round((sample_rate * filter_lenght_s))) + 1\n    kaiser_coefficients = signal.firwin(filter_order, 50, fs=1000, window=(\"blackman\"))\n    kaiser_w, kaiser_h  = signal.freqz(kaiser_coefficients, fs=1000)\n    fig, ax = plt.subplots()\n    ax.plot(kaiser_w, 20 * np.log10(abs(kaiser_h)))\n    ax.fill_between((cutoff_frequency, cutoff_frequency+trans_bandwidth), 5, -150, alpha=0.35, color='red')\n    ax.set_xlim((0, 175))\n    ax.set_ylim((-150, 5))\n\ninteractive(filter_components, filter_lenght_s=widgets.FloatSlider(min=0, max=1.5, step=0.1, value=0.2, description=\"FL(s)\"))\n\n\n\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.fill_between((50, 50+10), 5, -200, alpha=0.1, color='red')\n    ax.set_xlim((0, 200))\n    ax.legend()\n\n\n\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None) raw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)\n\n\n\nLots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section).\n\n\n\nOnce you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html#looking-at-the-data",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html#looking-at-the-data",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "The first thing you should do when starting to work with EEG data is to look at it. This is useful to get accustomed to how clean EEG data looks like, how common artefacts appear (e.g. blinks, muscle noise) and, in general, to ensure that your recording is as you expect it.\nraw_data.plot()"
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html#downsampling",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html#downsampling",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "If the EEG data was collected with a high sample rate (commonly 1000 Hz), then you can decide to downsample the data to a lower sampling rate, like 500 Hz or 250 Hz. To my understanding, the primary reason for downsampling is to reduce the size of the data and speed up the analyses. This was especially important back in the days when computers did not have much memory and computing power. In my opinion, nowadays, this is not really necessary as most computers are able to handle large datasets. Moreover, by downsampling, we remove some information from the data. Unless downsampling is necessary (e.g. it would take months for your PC to preprocess your data), then I would not do it. However, some online discussions suggest that downsampling might be useful to obtain a better ICA decomposition (I don’t think this has been formally tested).\nAt its core, downsampling is an easy step that simply requires selecting one point for every N. How many N points you skip is defined by the sampling rate you want to obtain. Let’s check this.\n\n# Recorded signal\noriginal_sampling_rate = 100\nsignal_duration_s      = 2\nsamples_vector         = np.arange(0, signal_duration_s, 1/original_sampling_rate)\nsine_signal = 2*np.sin(2*np.pi*2*samples_vector)\n\n# Downsample\nnew_sampling_rate = 50\n\ndef g(srate=500):\n    number_of_points_to_retain = int((len(sine_signal)/original_sampling_rate) * srate)\n    step_downsampled_signal    = int(len(sine_signal) / number_of_points_to_retain)\n    downsampled_signal         = sine_signal[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    downsampled_time           = samples_vector[np.arange(0, len(sine_signal), step_downsampled_signal)]\n    \n    print(f\"Retaining: {number_of_points_to_retain} out of {len(sine_signal)}\")\n    \n    plt.plot(samples_vector, sine_signal, '-k', linewidth=8, alpha=0.3)\n    plt.scatter(downsampled_time, downsampled_signal, c='r', s=6, alpha=0.6)\n    plt.plot(downsampled_time, downsampled_signal, '-r', linewidth=1)\n    plt.rcParams['figure.figsize'] = [10, 5]\n    plt.show()\n    \n\n    \ninteractive(g, srate=widgets.IntSlider(min=5, max=100, step=1, value=50))\n\n\n\n\nAs you can see, the lower the sampling rate, the lower the resolution of our signal. Although here we see distortions only at very low values, we need to think about how a lower resolution could affect a complex signal as the EEG data. For instance, by downsampling, we are likely to remove information represented as high-frequency oscillatory activity.\nNOTE: this is a simplified representation of downsampling. In reality, filters are applied to the data to avoid distortions (aliasing). In other words, do not use this code on your data! Consider what you have observed and what we said about frequencies being distorted, and you can understand why filtering is important here\nNow let’s downsample the data.\nraw_data.resample(sfreq=250)"
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html#filtering",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html#filtering",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "This is an essential step, but I think it’s probably one of the most technical and complex topics in signal processing. Filtering aims to remove parts of the data that are likely to contain artefacts or not to represent brain activity. For the sake of simplicity, we will only discuss high-pass filters and low-pass filterswithout stopping on any specific type of filter (eg. Finite Impulse Response vs Infinite Impulse Response). What we will introduce, though, are some of the parameters you will definitely encounter and will be relevant to your data.\nIn general, a filter can be described as a mathematical procedure that allows the removal of specific frequencies from the data. It is important to understand this point. When a filter is applied, you modify the frequency information of your signal. Although downsampling partly affects the frequency domain too, with filters we do not trim the data. In other words, the time resolution of your data is left untouched.\nLet’s see.\n\n# Create signal as sum of sines\nsampling_rate     = 1000\nsignal_duration_s = 2\ntimes             = np.arange(0, signal_duration_s, 1/sampling_rate)\n\nsignal_frequencies = np.arange(10, 25, 10)\nsignal_sines       = np.sin(2*np.pi*signal_frequencies[0]*times)\nfor frequency in signal_frequencies[1:]:\n    signal_sines = np.vstack((signal_sines, np.sin(2*np.pi*frequency*times)))\n\ncomposite_signal = np.sum(signal_sines, axis=0)\n\n# Define Butterworth lowpass filter\npolinomial_numerator, polinomial_denominator = signal.butter(10, 15, fs=sampling_rate)\nfiltered_half_signal = signal.filtfilt(polinomial_numerator, polinomial_denominator, composite_signal[int(len(composite_signal)/2):])\n\nfig, ax = plt.subplots()\nax.plot(times, composite_signal, 'k', linewidth=2, alpha=0.5)\nax.plot(times, np.append(composite_signal[0:int(len(composite_signal)/2)], filtered_half_signal), 'r')\nax.vlines(times[int(len(times)/2)], min(composite_signal), max(composite_signal), colors='k', linestyles='dashed', linewidth=3);\n\n\n\n\nHere we filtered only the second half of the signal. Let’s see it’s frequency decompostion\n\n# Replace second half of signal with filtered version\ncomposite_signal[int(len(composite_signal)/2):] = filtered_half_signal\n\n\nfft_window = signal.windows.blackman(500)\nfft_hop    = int(len(fft_window)/20)\nSFT = signal.ShortTimeFFT(fft_window, hop=fft_hop, fs=sampling_rate, scale_to='magnitude', mfft=2**11)\nSx = SFT.stft(composite_signal)  # perform the STFT\n\nfig,ax = plt.subplots()\nax.imshow(abs(Sx), origin='lower', aspect='auto', cmap='viridis', extent=SFT.extent(len(composite_signal)))\nax.hlines((10, 20), 0, (2, 1), color='gray', linewidth=3, alpha=0.5)\nax.vlines(1, 0, 30, colors='r', linestyles='dashed', linewidth=3)\nax.set_ylim((0, 30))\nax.set_xlim((0, 2));\n\n\n\n\nWe can see here that the 20 Hz component of our signal disappears after the filter is applied.\nCool, now to the technical part. As I said, filters are not so straightforward, and when you decide which one to apply to your data, you should pause and think about it. To begin understanding why this is the case, let’s look at how a commonly used filter (Butterworth) attenuates frequencies.\n\ncutoff_frequency = 30\ndef filter_order_plot(filter_order=4):\n    b,a = signal.butter(filter_order, cutoff_frequency, 'low', analog=True)\n    w,h = signal.freqs(b, a)\n    t, y = signal.impulse((b,a))\n    \n    fig, axs = plt.subplots(2, 1)\n    \n    axs[0].plot(w, 20 * np.log10(abs(h)), c='r', label=f\"Order: {filter_order}\")\n    axs[0].vlines(30, -50, 5, colors='k')\n    axs[0].set_ylabel(\"Amplitude (dB)\")\n    axs[0].set_xlabel(\"Frequency (Hz)\")\n    axs[0].set_xlim((0, 100))\n    axs[0].set_ylim((-50, 5))\n    \n    axs[1].plot(t, y, c=\"orange\")\n    axs[1].set_xlabel(\"Time (s)\")\n    axs[1].set_ylabel(\"Amplitude (AU)\")\n\n    plt.tight_layout()\n\n    \ninteractive(filter_order_plot, filter_order=widgets.IntSlider(min=1, max=20, step=1, value=4))\n\n\n\n\nTo understand filters and the information you might find online or published in some papers, we need to define a few terms. Note that these terms should be reported in a publication to ensure the correct replicability of your analyses - though not many people do this.\n\nCutoff Frequency: frequency threshold from/to where (ideally) the filter starts acting\nPassband: Frequency region (ideally) unaffected by the filter\nStopband: Frequency region (ideally) suppressed by the filter\nTransition band: Frequency region where the filter attenuates the amplitude without complete suppression\nLowpass filter: a filter that reduces/suppresses high frequencies (beyond the defined cutoff + passband)\nHighpass filter: a filter that reduces/suppresses low frequencies (before the cutoff + passband)\n\nNOTE: if you use EEGLAB default filter function pop_eegfiltnew, you will need to provide, at least, the passband limits and not the cutoff frequency!\n\n\n# Explore filter lenght\nsample_rate      = 1000\ncutoff_frequency = 50\ntrans_bandwidth  = 10\n\ndef filter_components(filter_lenght_s=0.2):\n    filter_order        = int(round((sample_rate * filter_lenght_s))) + 1\n    kaiser_coefficients = signal.firwin(filter_order, 50, fs=1000, window=(\"blackman\"))\n    kaiser_w, kaiser_h  = signal.freqz(kaiser_coefficients, fs=1000)\n    fig, ax = plt.subplots()\n    ax.plot(kaiser_w, 20 * np.log10(abs(kaiser_h)))\n    ax.fill_between((cutoff_frequency, cutoff_frequency+trans_bandwidth), 5, -150, alpha=0.35, color='red')\n    ax.set_xlim((0, 175))\n    ax.set_ylim((-150, 5))\n\ninteractive(filter_components, filter_lenght_s=widgets.FloatSlider(min=0, max=1.5, step=0.1, value=0.2, description=\"FL(s)\"))\n\n\n\n\nAs you can see here, by increasing the filter length, we make the filter less steep, and we push the ringing outside the transition band. The downside is that we are now keeping more frequencies that are beyond the cutoff. By modifying the filter length, as well as the type of filter and its specific parameters, we can modify how long the transition band is, how steep the filter is and how many of those ripples you can see above we get.\nTalking about ripples. You need to be aware of them as they can introduce artefacts in your data (the data itself can become “ripply”). Usually, if they appear in the stopband, you should be mostly fine, as their effects will affect a dimension of the data you supposedly are not interested in (otherwise, why would you filter out frequencies you want to analyse). However, you need to pay attention to filters that introduce ripples in the passband. Below, you can see that the rectangular filter induces some oscillations before the transition band (&lt;50 Hz). Because these ripples affect frequencies that will be contained in your filtered data, they will alter the data itself.\n\nfilter_types  = [\"hamming\", \"kaiser\", \"blackman\", \"rectangular\"]\nfilter_lenght = 80\nfig, ax = plt.subplots()\nfor filter_type in filter_types:\n    if filter_type != \"kaiser\":\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type))\n    else:\n        fir_coefficients = signal.firwin(filter_lenght, 50, fs=1000, window=(filter_type, 8))\n\n    \n    fir_w, fir_h = signal.freqz(fir_coefficients, fs=1000)\n    ax.plot(fir_w, 20 * np.log10(abs(fir_h)), label=filter_type)\n    ax.fill_between((50, 50+10), 5, -200, alpha=0.1, color='red')\n    ax.set_xlim((0, 200))\n    ax.legend()\n\n\n\n\nUsually, you won’t need to create filters from scratch. Most EEG packages (MNE, EEGLAB, Brainstorm, etc…) provide functions with (usually) sensible default values. However, always check your filters and their outputs!\nraw_filtered = raw_data.copy().filter(l_freq=0.5, h_freq=None) raw_filtered = raw_filtered.filter(h_freq=30, l_freq=None)"
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html#find-bad-channels",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html#find-bad-channels",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "Lots can happen during an EEG session and one of the most common events is that one or more electrodes do not record the signal properly. The causes for this can vary, incorectly setting up the electode cap, the participant touching the electordes, electrical noise, or some electrodes dying. Independently from the cause, we want to capture these electrode and remove them from the data. If we do not do so, we run the risk to retain or even introduce noise and artefact in the data we will analyse (see Rereferencing to average section).\nSo, how do we go about finiding bad channels? I don;t have a set answer for this. There are multiple procedures and the choice on how you want to go about this step is yours. The simplest thing you can do is to look at the data manually. If you open the data scroll (or plot the channels), you might be able to pick up channels that have unrealistic high or low voltages throughout the recording or for part of it.\n\nHere, for instance, you can see that one channel oscillates all over the place. That doesn’t look like brain at all and we might want to flag this channel to remove it. If you don’t want to look at every channel, then you need to decide on one or more rules to flag noisy channels. EEGLAB has a pervasive set of algorithms to achieve this (way too many for my taste). Their default now is to use a function called clean_rawdata, which would remove a channel if:\n\nIt is flat for longer than a threshold\nIt is not correlated with other electrodes\nIt contains excessive high-frequency noise\n\nIf you use EEGLAB and decide to use this algorithm, then I would suggest you run it a few times on the same dataset and ensure that the removed channels are always the same (or almost always). In my experience, sometimes the results are not consistent, especially when you have a short recording (short in terms of samples) or a low number of electrodes.\nNOTE: I haven’t seen this detail being reported much, but there is one important element (to me) that you should consider when removing channels: their location. One thing is to remove a few channels here and there; another thing is to remove a cluster of adjacent channels. If bad channels are close to each other, there might be a systematic issue that affects that cluster specifically. Moreover, it would be dangerous to proceed with their reconstruction through interpolation (next section)."
  },
  {
    "objectID": "extra_material/eeg_insights_extra/eeg_preprocessing.html#interpolation",
    "href": "extra_material/eeg_insights_extra/eeg_preprocessing.html#interpolation",
    "title": "EEG Preporcessing Insights",
    "section": "",
    "text": "Once you have removed bad channels, what should you do? If you are a purist, you might want to leave these channels out. Hopefully you do not need them for your analyses. Otherwise, you could reconstruct them using the information in the other channels. This step is called interpolation, and it is based on the idea that EEG electrodes contain data that is correlated. That is, two nearby channels partly pick up the same signal. Thus, if we remove one channel, we can use the other to reconstruct what the signal in the removed channel should look like.\nFor EEG data, this is usually done through a spherical interpolation, a procedure that projects the channels to the surface of a sphere and then uses the information contained in the retained channels to reconstruct the removed one."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html",
    "href": "posts/2022-10-03-k-means-clustering/index.html",
    "title": "K means clustering",
    "section": "",
    "text": "K-means is a class of unsupervised clustering algorithms that was developed within the field of signal processing. Given a set of data points \\(X={x_{1}, x_{2}, x_{3}, ..., x_{N}}\\), the aim is to find k clusters of points so that the Euclidean mean of the points within each cluster is minimised. Conversely, the distance between clusters is maximised.\nAlthough there are many different k-means algorithms, the general procedure is the following:\n\nDefine the number of clusters (k)\nInitialise the center point (centroid) for each cluster\nCompute the distance between each point and every centroid\nAssign points to the cluster whose centroid is minimally distant\nUpdate the centroid location\nRepeat assignment and updates until the difference between iterations in negligible\n\nFor this simulation we are going to use the data provided here\nThe dataframe looks like this.\n\n\n# A tibble: 6 × 2\n       x     y\n   &lt;dbl&gt; &lt;dbl&gt;\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#brief-overlook",
    "href": "posts/2022-10-03-k-means-clustering/index.html#brief-overlook",
    "title": "K means clustering",
    "section": "",
    "text": "K-means is a class of unsupervised clustering algorithms that was developed within the field of signal processing. Given a set of data points \\(X={x_{1}, x_{2}, x_{3}, ..., x_{N}}\\), the aim is to find k clusters of points so that the Euclidean mean of the points within each cluster is minimised. Conversely, the distance between clusters is maximised.\nAlthough there are many different k-means algorithms, the general procedure is the following:\n\nDefine the number of clusters (k)\nInitialise the center point (centroid) for each cluster\nCompute the distance between each point and every centroid\nAssign points to the cluster whose centroid is minimally distant\nUpdate the centroid location\nRepeat assignment and updates until the difference between iterations in negligible\n\nFor this simulation we are going to use the data provided here\nThe dataframe looks like this.\n\n\n# A tibble: 6 × 2\n       x     y\n   &lt;dbl&gt; &lt;dbl&gt;\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "title": "K means clustering",
    "section": "Define the number of clusters",
    "text": "Define the number of clusters\nThe number of clusters depends on the specific application. For instance, in EEG microstate analysis one common practice is to define the use of 4 clusters, which are descriptively called A, B, C, and D. However, note that defining a number a priori is a drawback of this technique. Ideally we would like to find a value that allows explaining the greatest proportion of variability in the data (without assigning each data point to a different group). Consequently, forcing the use of 4 - or any other number - of clusters might be a suboptimal option. We are going to discuss this a bit more later on. For the moment let’s be sneaky and look at the data.\nOur data contains X and Y coordinates for 60 points which are so distributed:\n\n\n\n\n\nFrom the plot we can reasonably say that there are 3 clusters, so we are going to work with that.\n\n#Define number of clusters (k)\nk &lt;- 3"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "title": "K means clustering",
    "section": "Initialise centroids",
    "text": "Initialise centroids\nThe first step of this algorithm is to select the k points that are stereotypes of the clusters. In other words, points that are representative of the groups we want to create. These points are called centroids or prototypes. Obviously, we do not know what groups we will end up with, so the simplest way to select the first centroids is to pick k points at random. Let’s define a function that does exactly this.\n\n# Define function that select k centroids from a dataset.By default the function will select 2 centroids if no k is provided\n\npick_centroids &lt;- function(data, k, seed=1234){\n  # Randomly select k rows from the dataset provided\n  set.seed(seed)\n  centroids &lt;- data[sample(nrow(data),k), ]\n  # Add a unique letter label\n  centroids &lt;- cbind(centroids, 'label'=LETTERS[1:k])\n  return(centroids)\n}\n\nWe can now pick 3 random centroids and visualize them.\n\n# Select first centroids\ncentroids_1 &lt;- pick_centroids(df, k=3, seed=19)\n\n# Visualise them\ndf %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point(size=2, alpha=0.5, colour='gray') +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=5, shape=15) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "href": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "title": "K means clustering",
    "section": "Compute the distance between each point and each cluster",
    "text": "Compute the distance between each point and each cluster\nOnce the first centroids have been selected, we can start to divide all the other points into the corresponding clusters. Each point will be assigned to the cluster represented by the centroid that is its closest geometrically. To do so, we need to compute the Euclidean distance between every point and every centroid. Then, we select the minimum distance and assign the point to that centroid’s group. The Euclidean formula is:\n\\[ \\bar{A,B} = \\sqrt{(x_{A} - x_{B})^{2} + (y_{A} - y_{B})^{2}} \\] The following function returns two pieces of information for each point. Firstly, the assigned group as defined by the minimum Euclidean distance from the corresponding centroid. Secondly, an “error’ value defined as the distance between the point and its closest centroid. We will use this error to set up a stopping rule for our k-means algorithm later on.\n\n# Define function to compute the Euclidean distance\neuclidean_distance &lt;- function(data, centroid){\n  distance &lt;- sapply(1:nrow(data), function(i){\n    sqrt(sum((data[i,] - centroid)^2))\n  })\n  return(distance)\n}\n\n\n# Define a function that applies the euclidean distance to each point and returns the minimum \n# Note that this function presupposes that the centroids have a x and y coordinates columns\nfind_min_distance &lt;- function(data, centroids, c_coord){\n  # Firstly we compute the distance between each point and each centroid\n  distances &lt;- sapply(1:nrow(centroids), function(i){\n    euclidean_distance(data, centroids[i, c_coord])\n  })\n  \n  # For each point let's find the centroid with the minimum distance\n  min_idx &lt;- apply(distances, 1, which.min)\n  \n  # We also extract the minimum distance so we can return it\n  min_distance &lt;- apply(distances, 1, FUN = min)\n  \n  # Extract the associated labels\n  min_labels &lt;- sapply(1:length(min_idx), function(i){\n    centroids$label[min_idx[i]]\n  })\n  \n  return(list('error'=min_distance, 'labels'=min_labels))\n}\n\nNow we can apply this to every point in our dataset.\n\nfirst_iter &lt;- find_min_distance(df, centroids_1, c('x', 'y'))\n\n# Let's plot this\ncbind(df, 'label' = first_iter$labels) %&gt;% \n  ggplot(aes(x=x, y=y, colour = label)) +\n  geom_point(size=2, alpha=0.7) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThat looks like a good starting point, although the B group dominates most of the data. To improve the categorisation, we can build from here by repeating this process over and over. Each time we will select new centroids and assign the points to the group represented by the closest centroid. We do this until there are no more significant changes in the groups."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "title": "K means clustering",
    "section": "Update centroids",
    "text": "Update centroids\nAfter one iteration, we need to update the centroids. A simple way to do this is by computing the mean coordinate values for each group. The new centroids will be defined by these mean coordinates.\n\nupdate_centroids &lt;- function(df, labels){\n  new &lt;- cbind(df, 'label' = labels) %&gt;% \n    group_by(label) %&gt;% \n    summarise(x = mean(x), \n              y = mean(y)) %&gt;% \n    relocate(label)\n    \n  return(new)\n}\n\n# Compute new centroids\ncentroids_2 &lt;- update_centroids(df, first_iter$labels)\n\n# Plot old and new centroids\ncbind(df, 'label'=first_iter$labels) %&gt;% \n  ggplot(aes(x=x, y=y, colour=label)) +\n  geom_point(size=2, alpha=0.5) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=3, shape=15) +\n  geom_point(data=centroids_2, aes(x=x, y=y, colour=label), size=5, shape=4) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe squares represent the original centroids, the Xs represent the new ones, and the points are still coloured according to their original categorisation. Notice how the blue centroid is now in the centre of its group."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "href": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "title": "K means clustering",
    "section": "Reiterate",
    "text": "Reiterate\nWe are ready to reiterate the update and assignation process N times. As said above, we will stop when there are no more significant differences between one categorisation and the next one. To quantify this, we will use the error value we introduce just before. For each point, this is represented by the distance of the point from its closest centroid. Thus, we can sum these error values and use this sum as the stopping rule. When the sum of errors is reduced below a predetermined threshold, then we can stop.\n\nmy_kmeans &lt;- function(data, k=2, c_coord= c('x', 'y'), tolerance=1e-4, seed=1234){\n  # Firstly we find the first centroids\n  current_centroids &lt;- pick_centroids(data, k=k, seed=seed)\n  \n  # Create datasets were to store results\n  labelling &lt;- c()\n  centroids &lt;- current_centroids\n  \n  # Reiterate labelling - assignment - update centroids\n  continue &lt;- TRUE\n  iter &lt;- 0\n  previous_error &lt;- 0\n  while(continue){\n    \n    # Assign data to centroids\n    current_groups &lt;- find_min_distance(data, current_centroids, c_coord)\n    \n    # Store assigned labels with column name as the iteration number\n    iter &lt;- iter + 1\n    labelling &lt;- cbind(labelling, current_groups$labels)\n    \n    # Update centroids\n    current_centroids &lt;- update_centroids(data, current_groups$labels)\n    centroids &lt;- rbind(centroids, current_centroids)\n    \n    # Check if we have minimizes the error below the threshold\n    current_error &lt;- sum(current_groups$error)\n    current_err_diff &lt;- abs(previous_error - current_error)\n    print(sprintf('Iteration %s -&gt; Error: %s', iter, current_err_diff))\n    if(current_err_diff &lt;= tolerance){\n      continue = FALSE\n    }\n    # If we did not reach the tolerance, update the current error\n    previous_error &lt;- current_error\n    \n  }\n  colnames(labelling) &lt;- 1:iter\n  # remove last centroid data as it has not been used and assign iter values\n  centroids &lt;- centroids[1:(nrow(centroids)-k), ]\n  centroids &lt;- cbind(centroids, 'iter'=rep(1:iter, each=k))\n  return(list('lables'=labelling, 'centroids'=centroids, 'error'=current_groups$error))\n}\n\nLet’s iterate on our data.\n\nresults1 &lt;- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=4)\n\n[1] \"Iteration 1 -&gt; Error: 458.171667198993\"\n[1] \"Iteration 2 -&gt; Error: 72.4112246075529\"\n[1] \"Iteration 3 -&gt; Error: 0.655622083297658\"\n[1] \"Iteration 4 -&gt; Error: 0\"\n\n\nSweet, for this particular case the algorithm converged in 4 iterations. Let’s see the final result.\n\ncbind(df, 'group'=results1$lables[, ncol(results1$lables)]) %&gt;%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThis looks all good! Yay!"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "title": "K means clustering",
    "section": "Initialization problems",
    "text": "Initialization problems\nThe previous result might make you think that this algorithm is amazing. It categorised our data in just four iterations and with a perfect division. However, things are always more complicated than they initially appear. Indeed, a drawback of the k-means approach is that the final result is highly dependent on the initial centroids. We can demonstrate this by starting the algorithm with different initial centroids. We will exploit the seed argument we provided to our functions.\n\nresults2 &lt;- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=19)\n\n[1] \"Iteration 1 -&gt; Error: 1278.04142581076\"\n[1] \"Iteration 2 -&gt; Error: 549.175741899334\"\n[1] \"Iteration 3 -&gt; Error: 120.573809438913\"\n[1] \"Iteration 4 -&gt; Error: 7.0347771777997\"\n[1] \"Iteration 5 -&gt; Error: 10.0949551678153\"\n[1] \"Iteration 6 -&gt; Error: 5.53498901037312\"\n[1] \"Iteration 7 -&gt; Error: 4.606053850374\"\n[1] \"Iteration 8 -&gt; Error: 1.49594562166419\"\n[1] \"Iteration 9 -&gt; Error: 2.36620189065968\"\n[1] \"Iteration 10 -&gt; Error: 0\"\n\ncbind(df, 'group'=results2$lables[, ncol(results2$lables)]) %&gt;%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2, alpha=.5) +\n  geom_point(data=results2$centroids %&gt;% filter(iter==max(iter)), aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nHere the algorithm converged to a suboptimal solution. During the years different solutions have been created to address this problem, with the most popular and reliable being the kmeans++ algorithm created by David Arthur and Sergei Vassilvitskii. If you are interested, the procedure is presented here"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "title": "K means clustering",
    "section": "How many clusters?",
    "text": "How many clusters?\nAs stated in the introduction, one obvious limitation of this paradigm is that the number of clusters needs to be defined a priori. Thus, we need a system that would allow us to select the optimal number of clusters that reduces the classification error as much as possible without “overfitting”. One simple method to do so is to run the algorithm with different number of clusters and use the scree plot of the error as a guide. To explain this let’s change dataset and pick something with a greater number of observations and a less clear number of clusters. We will use the iris dataset provided in R. Our task is to cluster the flowers into species based on the sepal length and the petal width.\nIf we look at the raw data we can see two possible groups, but now the situation is more complex than before.\n\niris %&gt;% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nTo establish the optimal amount of clusters, we are going to run our k-means algorithm 10 times adding one cluster at each iteration. Each time we will store the final error, so we can plot it later. Here’s the code:\n\nks &lt;- 1:10\nerrors &lt;- rep(0, length(ks))\niris_df &lt;- iris[, c('Sepal.Length', 'Petal.Width')]\ncolnames(iris_df) &lt;- c('x', 'y')\nfor(r in ks){\n  errors[r] &lt;- sum(my_kmeans(iris_df, k=r)$error)\n}\n\nNow we can create the scree plot by visualising the final error for each iteration.\n\n# Make scree plot of the errors\ndata.frame('error'=errors, 'k'=1:length(errors)) %&gt;% \n  ggplot(aes(x=k, y=error)) +\n  geom_point(size=2) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:length(errors)) +\n  theme_minimal()\n\n\n\n\nThe scree plot is a “descriptive tools” so it won’t tell specifically the correct number of clusters. The main idea here is to look at the elbow of the plot, that is the point at which the trend plateau. This flexion point indicates the value after which increasing the number of groups does not provide a significant decrease in the error. Looking at the plot we can say that the elbow is in between k=3 or k=4. As a starting point we can visualise both of them:\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 &lt;- my_kmeans(iris_df, k = 3)\n\n[1] \"Iteration 1 -&gt; Error: 84.5340503544893\"\n[1] \"Iteration 2 -&gt; Error: 22.5320451284015\"\n[1] \"Iteration 3 -&gt; Error: 0.12222997427822\"\n[1] \"Iteration 4 -&gt; Error: 0.194408399479336\"\n[1] \"Iteration 5 -&gt; Error: 0\"\n\niris_k4 &lt;- my_kmeans(iris_df, k = 4)\n\n[1] \"Iteration 1 -&gt; Error: 67.9944121319274\"\n[1] \"Iteration 2 -&gt; Error: 9.13499108999842\"\n[1] \"Iteration 3 -&gt; Error: 1.30457150911509\"\n[1] \"Iteration 4 -&gt; Error: 0.932100341554694\"\n[1] \"Iteration 5 -&gt; Error: 1.0714446191438\"\n[1] \"Iteration 6 -&gt; Error: 0.716841774592375\"\n[1] \"Iteration 7 -&gt; Error: 0.675081843250041\"\n[1] \"Iteration 8 -&gt; Error: 0.318333061193755\"\n[1] \"Iteration 9 -&gt; Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean &lt;- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %&gt;% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %&gt;% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nThe groups are very similar, or at least there are no weird differences. This is a good thing. To better investigate possible difference between the two clustering systems, we could re-run them with a different seed and see if they are consistent (although we have discussed the implications that the first initialization can have).\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 &lt;- my_kmeans(iris_df, k = 3, seed = 0905)\n\n[1] \"Iteration 1 -&gt; Error: 99.0311396766634\"\n[1] \"Iteration 2 -&gt; Error: 24.4105692091305\"\n[1] \"Iteration 3 -&gt; Error: 4.88652621704111\"\n[1] \"Iteration 4 -&gt; Error: 3.47693331688762\"\n[1] \"Iteration 5 -&gt; Error: 2.56320926311736\"\n[1] \"Iteration 6 -&gt; Error: 1.62704674573919\"\n[1] \"Iteration 7 -&gt; Error: 0.0724143145959744\"\n[1] \"Iteration 8 -&gt; Error: 0.057240636048725\"\n[1] \"Iteration 9 -&gt; Error: 0.0225024050884883\"\n[1] \"Iteration 10 -&gt; Error: 0\"\n\niris_k4 &lt;- my_kmeans(iris_df, k = 4, seed = 0905)\n\n[1] \"Iteration 1 -&gt; Error: 97.5708407922748\"\n[1] \"Iteration 2 -&gt; Error: 29.7874124595873\"\n[1] \"Iteration 3 -&gt; Error: 7.83693472432838\"\n[1] \"Iteration 4 -&gt; Error: 2.47590455673429\"\n[1] \"Iteration 5 -&gt; Error: 0.153319685392383\"\n[1] \"Iteration 6 -&gt; Error: 0.0496830708143321\"\n[1] \"Iteration 7 -&gt; Error: 0.0799826359733871\"\n[1] \"Iteration 8 -&gt; Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean &lt;- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %&gt;% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %&gt;% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nNote that the colours can change between iterations. What we are interested in are the patterns of groups. The results are interesting. k=3 produced the same results as before. Conversely, k=4 did something different; it divided the lower cluster of points into two groups. Although this is not a formal assessment, from here we might want to say that k=3 is more reliable and go with it. As a final test, let’s compare these results agains the real division by species.\n\niris %&gt;% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour=Species)) +\n  geom_point(size=2) + \n  theme_minimal()\n\n\n\n\nLet’s hihglight the differences:\n\n# Let's assign a species label to to A, B, D clusters\nspecies &lt;- unique(iris$Species)\n\n# Merge datasets and code whether there groups are the same\niris_binded &lt;- cbind(iris, 'my_kmeans' = iris_k3$lables[, ncol(iris_k3$lables)]) %&gt;% \n  mutate(\n    my_species = case_when(\n      my_kmeans == 'A' ~ species[3],\n      my_kmeans == 'B' ~ species[1], \n      TRUE ~ species[2]),\n    equal = case_when(my_species != Species ~ 0,\n                      TRUE ~ 1))\n\n# Plot highlighting difference\niris_binded %&gt;% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour = factor(equal))) +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c('red', 'lightgray'), name = 'Difference', labels = c('different', 'equal')) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe red dots are the observations that our kmeans algorithm categorised differently compared to the original division into species. Overall, the result can be improved but it’s not too bad considering that the majority of points were correctly identified."
  },
  {
    "objectID": "posts/2024-10-14-participants-vs-subjects/index.html",
    "href": "posts/2024-10-14-participants-vs-subjects/index.html",
    "title": "Stop treating participants as passive subjects",
    "section": "",
    "text": "During my undergrad I spent lots of time learning about classical psychological experiments, like the Stanford Prison Experiment or the Milgram’s Experiment. These are influential studies not only for the role they played in the psychological discussion but for their impact on the ethics of science. These experiments potentially created undue distress (this is an understatement for the Stanford Experiment) in the participants, which resulted in the introduction of stricter and stricter ethical protocols. As a researcher (for legal reasons I need to say I am a PhD student), I see the effects of our past every time I start a new study. A lengthy ethics form must be filled, highlighting, among other things, all the steps I will take to avoid inducing unjustified distress to those who voluntarily participate in my research.\nAlthough filling out lengthy and repetitive paperwork causes me distress, being forced to think about how I will protect the participants’ privacy, acquire informed consent, and ensure everyone is informed and comfortable being in the study is important. It reminds you that part of your job as a researcher is “customer service” - you work with people. Not only do you work with people, but people work for you. For whatever reason, people decide to engage in scientific studies. They offer some of their free time and effort to complete our studies, without asking anything back except for the experience and a 20$ voucher.\nWith all these considerations, I always try to make the experimental experience an enjoyable time. My studies are already quite boring (cognitive neuroscience of vision and attention sound cool until you sit in front of a computer looking at a square for an hour), and EEG requires a certain level of proximity, especially when we need to touch your head and face to apply the electrodes. Therefore, the first thing I usually do is get to know the person in front of me, run them through what’s going to happen, make sure they are fine with it, and have small talk whenever possible. This is a collaborative moment, and I want to have a nice relationship with my collaborators.\nI have never consciously thought much about all of this until one day. One day, I started preparing my data to be stored in BIDS format. Specifically, I was converting the file names into the required standard sub-0X_task, and it hit me. Do you see? Do you see what’s happening? The standard is to use sub, subject, and not participant. Ok, now that I have written this down, I realise it doesn’t seem much, but bear with me and let’s go down this rabbit hole together.\nThe Merriam-Webster dictionary define subject as:\nIt implies subordination, hierarchy, passivity. Subject is a term that captures the views and beliefs that underlie studies like the Stanford Prison and Milgram’s experiments. A subject is someone that you can use to collect data for your research. You, the researcher, are the one offering a unique opportunity to see how science works. And they, the subjects, are better off taking this opportunity because the greater cause of science needs their data. Don’t think twice, don’t complain, just come and do your part.\nOK, OK, maybe that was an overreaction. “Or was it?” (Vsauce, anytime). Something about that initially seemingly insignificant difference between the two words made me pay more attention to how researchers talk about participants and behave with them. And well, researchers like subjects. The first thing I noticed was a somewhat widespread feeling that dealing with participants was annoying. It is so annoying that in some more funded universities, primary researchers do not even have to interact with them directly. Research assistants, usually undergrad and grad students, do the work for them. Everything the researcher sees is the stream of data they are provided with. The idea is that data collection is not a high-priority step in the study. It takes time from other more important activities, like writing as many papers as you can because “you publish or perish”, or understanding how to use statistics to fix a bug in the experiment, a bug that no one caught on time because those in charge were writing as many articles as they could… Obviously, the students collecting data do not necessarily end up as authors on the papers because spending time with participants and creating datasets is clearly not a necessary step in the research process. I’m sure we all agree with that.\nIn this view, a subject is an entity that is detached from the study. Although no cognitive neuroscience or psychology experiment would be done without their time and work, they are seen just as a source of data. This view is similar to how big AI companies see content creators (writers, YouTubers, photographers, actors, etc… ) as a necessity whose work should be exploited with disinterest. Because subjects’s data is necessary, it’s annoying when they do not turn up for an experiment, even though the consent form we provide states that they are free to withdraw whenever. But researchers can be late for a session (you know, all those papers to write) and that’s fine. I saw cases of people being left waiting for their session for over an hour. But this is all good, subjects are entities and entities have no concept of time or work to do.\nA related issue is the idea of interpersonal relationship some researchers have if they are so unlucky to have to collect their own data. There is no need to talk to subjects to make them feel comfortable, welcomed and/or interested. You just recite some protocol detailing the necessary information for the session, and that’s pretty much it. In an EEG study, this means that the subject will sit mostly in silence for anywhere between 15 and 45 minutes while the researcher is touching their head and staying close to them. We can all agree that nothing relaxes more than a stranger touching your head for a prolonged period of time. This type of researcher-subject relationship seems to be rooted in the idea of science engagement as a business transaction. It requires formality. It requires professionalism. It requires awkward silence. To me, this is simply unacceptable. If dentists try to do small talk to keep us comfortable when they are chucking their fingers in our mouths, I’m sure we can do the same. The small talk, no the fingers part.\nProbably the most telling issue on this topic is the amount of time and money researchers spend trying to optimise data collection. The standard practice is through some flyers displayed around the university - cheap and easy. But this is not enough. Flyers won’t attract the masses, the story goes. We need other tools. We need other ways that maximise the capital gain in subjects and minimise the time frame of data collection. And what we came up with? Course credits for completing a learning experience module. The idea is simple. A university is full of students. If we encourage them to participate in studies, we can collect data at a fast rate. But you know what’s even better? We can also avoid reimbursing them with money or vouchers! The only thing we need to do is to tell them that part of their course credits is obtained by participating in one or more studies1. Researchers save money and students learn something through a practical activity. Unless they don’t. I cannot remember participating in a study where I truly learned something. True, the experience might be interesting. You can see some cool equipment, but in most cases, the learning component stops there. As researchers, we can show you something, like the differences in your EEG when you keep your eyes open or closed, or we can give you a picture of your fMRI brain scan. But for most people, these are interesting things that mean nothing. Seeing a wiggly line on a screen is exciting if you know its meaning and implications. A picture of your brain is a nice token, but is it a learning component? I’d say no. The issue is exacerbated by the points discussed above, by the treatment of participants as subjects. If data collection is always delegated to research assistants, they might not know the background details of the study, meaning that they might be unable to provide more insight into the reason for the study. If the leading researchers are uninterested in properly collaborating with the participants, they create an environment that does not support participants asking questions they are interested in.\nThe sad part of this situation is that, from personal experience, most participants are truly interested in what they are doing in a study, even when the study is tedious or requires many different sessions. And this is the core issue I have with the subject mentality. It fails people who engage with science. The people who play an essential part in human research. There is considerable discussion and pressure within the scientific community to perform more community engagement. Science needs to connect more with people who are not necessarily part of the scientific community. Scientists need to be able to communicate the meaning of their research so that everyone can understand what they do. However, this pressure is mainly expressed in words and implemented in policies but is not necessarily enforced. The situation is one where researchers express in ethics applications, grant proposals, and other documents how well participants will be treated, how much they will learn from engaging in a study, how good a study is for the community. However, the sentiment is not expressed in a change of research culture. Participants are seen as subjects, and they are treated as such.\nI understand that this is a multifaceted problem that encompasses many other academic issues. Researchers are dealing with low funds, extreme pressure to publish “innovative” research, time constraints, low or non-existent wages, and extreme competition. In my view, these issues begin at the higher levels of academia and trickle down to researchers (PhD, master and undergrad students). However, we should not let them trickle down even further, down to people who want to help science out of their free time. Not only this, these issues are not an excuse for treating people as subjects. We researchers really need to think hard about how we treat the people who participate in our study. A first step is to realise that we are not the most important element in the research room. Yes, we hold the responsibility for the safety and well-being of the other person, but they are our collaborators. Our co-pilots for the duration of the study. We provide the needed guidance, and they collaborate with us to complete the research task. As such, we need to create an environment with a flat power-gradient2. An environment that is friendly and welcoming and makes people comfortable not only with being there but also with asking questions, making comments, or simply getting to know each other for a moment. If we truly believe in what we write on official documents, we need to start to behave accordingly.\nThe post image was generated through DALL-E. The content was written without the use of generative AI systems. Grammarly which was used to spell-check the final document."
  },
  {
    "objectID": "posts/2024-10-14-participants-vs-subjects/index.html#footnotes",
    "href": "posts/2024-10-14-participants-vs-subjects/index.html#footnotes",
    "title": "Stop treating participants as passive subjects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe situation is actually worse. Students can choose whether to participate in studies or write a short essay. The topic and related material are provided by the researchers who also grade the essays. The problem is that researchers are not paid for this, meaning that they are doing course marking for free and for a course they don’t even know. If you want to know more, look up the SONA System. Already the homepage statement on participation rates and the prices hidden behind a call-to-action capture the marketing and capitalistic view of science.↩︎\nThe topic of a flat power-gradient is an interesting one in academia. It plays an essential role in class, where the passive view encapsulated by subjects is translated into the passive view some educators have of students. The view of educators (professors, teaching assistants, techs, etc…) playing the most important role in a class is pretty much still alive, meaning that students are seen as an entity that should quietly listen to anything it is told to. I not only see this happening in some classes and tutorials, but it is also evident in the way the university administrators run universities. As a business whose primary goal is to maximise some random rankings at the expanses of the tuition paid by students. Much could be said about the ways universities have been reducing the number of teaching roles, diminishing the educational offer to students while increasing tuition prices, university accommodation rents, etc…↩︎"
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html",
    "href": "projects/a_paper_a_day/papers_summaries.html",
    "title": "A Paper a day",
    "section": "",
    "text": "Detection or discrimination tasks often require the estimation of each participant’s psychometric function to control behavioral performance to a specific threshold. However, obtaining a psychometric function usually requires a high number of trials, as the participant’s performance needs to be tested multiple times at a variety of stimulus levels. A way around this is to use an adaptive procedure that incorporates the knowledge acquired from previous trials, as well as the experimenters’ knowledge, to estimate the psychometric function’s parameters more quickly. Here, Watson and Pelli introduce an adaptive staircase (QUEST), which uses a Bayesian framework to speed up the titration process while retaining high estimation accuracy."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#watson-pelli-1983",
    "href": "projects/a_paper_a_day/papers_summaries.html#watson-pelli-1983",
    "title": "A Paper a day",
    "section": "",
    "text": "Detection or discrimination tasks often require the estimation of each participant’s psychometric function to control behavioral performance to a specific threshold. However, obtaining a psychometric function usually requires a high number of trials, as the participant’s performance needs to be tested multiple times at a variety of stimulus levels. A way around this is to use an adaptive procedure that incorporates the knowledge acquired from previous trials, as well as the experimenters’ knowledge, to estimate the psychometric function’s parameters more quickly. Here, Watson and Pelli introduce an adaptive staircase (QUEST), which uses a Bayesian framework to speed up the titration process while retaining high estimation accuracy."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#shin-et-al.-2017",
    "href": "projects/a_paper_a_day/papers_summaries.html#shin-et-al.-2017",
    "title": "A Paper a day",
    "section": "2024-13-06:Shin et al., 2017",
    "text": "2024-13-06:Shin et al., 2017\nPre-stimulus beta power has been associated with detection and attention in tactile tasks. Generally, increased sustained pre-stimulus power is negatively correlated with detection and attention. However, the authors show that the observed pre-stimulus power likely does not represent sustained activity. When single-trial time-frequency activity is analyzed, short bursts of beta activity (beta events) are visible in the pre-stimulus window. These beta events are not time-locked to a stimulus and do not appear to be generated by a rhythmic generator. Thus, their number and timing vary across trials. Because of this, averaging trials in standard analyses creates the illusion of a sustained beta rhythm. By analyzing two MEG human datasets and one intracranial recording in mice, the authors show that the number and timing of the last beta event are negatively correlated with detection and attention: a higher number of events corresponds to lower detection, and the closer the last beta event is to stimulus onset, the lower the detection and attention. Importantly, the number of events and the timing of the last event independently affect behavior."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#voytek-knight-2015",
    "href": "projects/a_paper_a_day/papers_summaries.html#voytek-knight-2015",
    "title": "A Paper a day",
    "section": "2024-12-06: Voytek & Knight, 2015",
    "text": "2024-12-06: Voytek & Knight, 2015\nOne of the major features of EEG activity is the 1/f slope visible in the power spectrum of the data. Although attention is predominantly given to the oscillatory activity visible in the spectrum (i.e., peaks), the 1/f slope might contain relevant information reflecting the excitability state and connectivity of neuronal populations. Here, the authors start by considering the complex coupling between extracellular local field potentials (LFP) and firing rates. Specifically, neuronal spiking is modulated by oscillatory LFPs. LFPs modify the membrane potentials of postsynaptic neurons, rendering them closer or further from firing (essentially, LFPs modify the likelihood of a postsynaptic potential inducing an action potential). The authors propose that this micro-scale mechanism could be reflected at the meso-scale by phase-amplitude coupling, where the amplitude of a signal is modulated by the phase of the oscillatory activity the signal is embedded in. Moreover, they argue that the effects of LFP activity are reflected in the 1/f slope. When the LFP activity is “stable” and oscillatory, neurons are more likely to go through an excitability phase, where action potentials can be triggered. In this context, a stimulus could trigger a cascade of action potentials in a neuronal population. If this happens, many neurons in the population would quickly fire and enter a refractory period. As this activity is synchronized by the LFP oscillations, the net refractory period in the population would be short, resulting in a steep 1/f slope (the aperiodic activity seems to reflect local population excitation). Conversely, a weak but prolonged stimulus would induce a higher degree of noise in the population field, as neurons would not be synchronized, thus inducing a non-coordinated firing pattern. The result is a net prolonged state of inhibition of the population, which is reflected as a flattened spectrum. To conclude, a flattened power spectrum could be a sign of decreased coherence across cortical areas, as local firing patterns are not synchronized by stable oscillatory LFP activity. Note, however, that excessive coherence can be problematic too, as it could create a situation where neuronal firing is locked to the LFP, resulting in a feedback process that perpetuates itself. Therefore, a certain amount of “noise,” defined as firing decoupled from the background carrier frequency, is needed for proper communication across brain areas."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#yuasa-et-al.-2023",
    "href": "projects/a_paper_a_day/papers_summaries.html#yuasa-et-al.-2023",
    "title": "A Paper a day",
    "section": "2024-11-06: Yuasa et al., 2023",
    "text": "2024-11-06: Yuasa et al., 2023\nPresenting visual stimuli usually induces a decrease in occipital [[alpha power]], which is believed to represent increased cortical excitation. However, both oscillatory and broadband aperiodic activities are affected by the stimuli. Changes in periodic activity power could be masked by changes in aperiodic power (e.g., alpha power might increase, but broadband aperiodic activity might decrease, creating a false null effect). To address this, the authors used a retinotopic mapping paradigm (checkerboards moving across space) with ECoG. They separated the periodic and aperiodic components and observed that stimulus presentation decreases alpha power at electrodes over visual cortical areas processing the stimulus. This suggests a retinotopic distribution of alpha power. Further analysis with pRF showed that both components have similar pRF centers in the parafovea, but the pRF for alpha power is larger than for aperiodic activity. This indicates that stimuli reduce alpha power even when presented peripherally, increasing cortical excitability and tuning the cortex to process visual information. Importantly, the stimulus drives [[alpha power]] changes, not the other way around (bottom-up, not top-down)."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#van-der-werf-et-al.-2022",
    "href": "projects/a_paper_a_day/papers_summaries.html#van-der-werf-et-al.-2022",
    "title": "A Paper a day",
    "section": "2024-10-06: van der Werf et al., 2022",
    "text": "2024-10-06: van der Werf et al., 2022\nElectrophysiological studies have observed that the deployment of attention is a rhythmic process, often associated with neuronal oscillatory activity around 8 Hz. These observations have been expanded with behavioural analyses to determine whether the detection of a target follows an oscillatory pattern. Here, the authors build on previous studies by estimating behavioural oscillations and relating hit rates to the length of the cue-target interval. Specifically, they compute the hit rate in a 50 ms sliding window to obtain a time-resolved hit-rate measure. The frequency spectrum of the time-resolved hit rate is then computed to investigate whether the behavioural representation of attention deployment is dominated by specific frequencies. However, in this study, they do not observe any effect of frequency on target detection (i.e., detection is not more common at specific delays from cue onset). Similarly, they did not find any oscillatory effects at non-cued locations."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#wei-et-al.-2022",
    "href": "projects/a_paper_a_day/papers_summaries.html#wei-et-al.-2022",
    "title": "A Paper a day",
    "section": "2024-08-06: Wei et al., 2022",
    "text": "2024-08-06: Wei et al., 2022\nThis study employed an uncued lateralised detection task to investigate whether the alpha desynchronisation usually observed in these types of tasks is associated with frontal activity. The assumption is that, although occipital alpha power might play a role in detecting visual stimuli, it could be modulated by frontal activity. They observed that occipital alpha power in the 200ms preceding the stimulus correlates with hits and misses (low power -&gt; hit; high power -&gt; miss). Some results pointed towards a frontal-to-occipital sweep of alpha power correlated with hits and misses, moving from a time window further away from the stimulus towards stimulus presentation (e.g., parieto-central activity correlates with hits and misses around 500ms before stimulus onset). However, these correlations were not significant or approached significance after FDR correction, making this trend difficult to interpret. Nevertheless, they also observed synchronisation between frontal theta activity and occipital alpha power, with this synchronisation being stronger for high alpha power compared to low power. Moreover, occipital power could be predicted by frontal theta power."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#boncompte-et-al.-2016",
    "href": "projects/a_paper_a_day/papers_summaries.html#boncompte-et-al.-2016",
    "title": "A Paper a day",
    "section": "2024-07-06: Boncompte et al., 2016",
    "text": "2024-07-06: Boncompte et al., 2016\nStudies on the lateralisation of alpha power commonly use cued designs. However, hypotheses have been made regarding the possibility of this lateralisation being generated non-stochastically. To address this, the authors employed a lateralised design (target appears either in the left or right hemifield) without using a cue. They compared alpha power between seen trials (participant saw the target) and unseen trials (participant missed the target). They observed that, although the overall occipital alpha power preceding target onset remains constant between seen and unseen trials, there is significant lateralisation that distinguishes the two conditions. Specifically, there is higher lateralisation in seen trials (higher power in the ipsilateral hemisphere) compared to unseen trials. The crucial aspect here is that this lateralisation is endogenously driven, as no cues were presented."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#liu-et-al.-2023",
    "href": "projects/a_paper_a_day/papers_summaries.html#liu-et-al.-2023",
    "title": "A Paper a day",
    "section": "2024-06-06: Liu et al., 2023",
    "text": "2024-06-06: Liu et al., 2023\nProbably one of the most reliable observations in attentional studies (and working memory) is the lateralization of alpha power — a modulation in power so that there is lower power in the hemisphere contralateral to the hemifield where attention was deployed. Here, they show that one potentially important confound is eye movements, specifically microsaccades. These small deviations from fixations create a lateralization of occipital alpha power even in the absence of a task (here they analyzed the retention period of a working memory paradigm), meaning that microsaccades alone are able to drive changes in power. Importantly, they observed that the direction of the alpha power lateralization is such that power increases ipsilateral to the direction of the microsaccades, while no effects were observable in the contralateral hemisphere. Moreover, the effect was greater at P07/P08, but only weak at central electrodes. These observations pose questions about how to properly investigate the relationship between alpha power and attention in lateralized presentations."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#gao-et-al.-2020",
    "href": "projects/a_paper_a_day/papers_summaries.html#gao-et-al.-2020",
    "title": "A Paper a day",
    "section": "2024-06-05: Gao et al., 2020",
    "text": "2024-06-05: Gao et al., 2020\nIn this paper, Gao and colleagues investigate the time-scale property of neurons. Specifically, neurons across the cortex must be able to maintain and process information at different time scales: from brief moments necessary to react to a stimulus, to seconds, minutes, and even years, as in working memory and memory. The time scale of different cortical areas was estimated from the PSD of ECoG recordings, exploiting the fact that the time-scale factor can be calculated from the knee frequency in the power spectrum. They then correlated the time-scale values with measures of myelination, cortical thickness, gene expression, and working memory performance. They observed that time scales are correlated with all these measures, suggesting that the micro- and macro-scale structural and functional organization of the brain allows for the emergence of an organized time-scale structure. Indeed, sensorimotor and visual areas showed faster time scales, while association regions showed longer time scales, reflecting the different types of information processing they perform. Note that this hierarchy has been observed in both humans and macaques. This time scale also varies as a function of cognitive activity (e.g., time scale increases with increased cognitive load) and aging (on average, the time scale decreases)."
  },
  {
    "objectID": "projects/a_paper_a_day/papers_summaries.html#keesey-nichols-1967",
    "href": "projects/a_paper_a_day/papers_summaries.html#keesey-nichols-1967",
    "title": "A Paper a day",
    "section": "2024-06-04: Keesey & Nichols, 1967",
    "text": "2024-06-04: Keesey & Nichols, 1967\nThe authors investigate the relationship between the time dynamics of individual alpha power and the fading out of stabilized fixated images. They used a collimator system mounted on a contact lens to stabilize a patch of light on the participant’s retina while recording EEG. They observed that alpha power increases over a 1-second period before the participant reports the subjective disappearing of the image from conscious perception. Alpha power decreases after the participant reports the reappearing of the stimulus (though the change in alpha is more variable in time). They also observed that reappearance of the image was preceded by large eye movements or blinks in 40% of the trials. In a control condition where the image was not stabilized and the disappearance of the stimulus was mimicked by either defocusing the stimulus or lowering its luminance to zero, they observed that alpha powerchanges only after the participant reports the change in subjective perception. Putting it all together, it seems that alpha power might control the perception of luminous stimuli when the visual system cannot rely on eye movements to update the retinal image. Here, the authors suggest the possibility that in this condition, alpha replaces the missing signal associated with the visual input."
  },
  {
    "objectID": "projects/bayesian_stat_material/slides_list.html",
    "href": "projects/bayesian_stat_material/slides_list.html",
    "title": "Slides",
    "section": "",
    "text": "Here you can find the link to the slides openly available on RPubs.\n\nIntroduction to Bayesian Stat\nBayes Theorem\nBayesian Engines"
  },
  {
    "objectID": "projects/projects_intro.html",
    "href": "projects/projects_intro.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a collection of material related to different projects I have been working on, either alone or in collaboration with other people."
  }
]