[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "~/About",
    "section": "",
    "text": "Daniele completed his Bachelor of Science in Psychology and Communication at the University of Milan - Bicocca in 2018. He joined the University of Auckland in 2021 as an Honours student. He is now starting a PhD working with Paul Corballis, director of the Cognitive Psychophysiology Lab and co-advised by Christopher Erb. He is interested in investigating the neural correlates of consciousness using EEG, eye-tracking and whatever machine he can get his hands on. He has a second identity as a magician and his assistant is a tiny blue plunger."
  },
  {
    "objectID": "exploring/exploring_intro.html",
    "href": "exploring/exploring_intro.html",
    "title": "Exploring B/Log",
    "section": "",
    "text": "This is me (on the left), my sister (eating the bread) and my cousins at Monte Avaro. For most people this name means nothing, but this place is probably one of the most important for me. Still today, some of the most important and memories I have comes from there.\nMy grandparents used to bring us four up to this mountain every summer. However, this was not just a one day family hike, this was a month-long stay. A month-long stay in a van that my grandfather repourposed as a self-contained van, without running water, toilet or anything else. We slept all together, some years all inside the van, some other years we got a tent on top of it. We collected water from a spring a couple of hundred meters down the road, my grandmother used to cook us with a small gas stove or on a campfire, where some nights we prepared vin brulé (the Italian version of mulled wine).\nThis place was amazing. It was not popular, there was hardly anyone around, just us, an alpine cow farmer who became a family friend (you see, my grandparents used to bring our parent up here as well when they young) and a couple of people owning a hut up there. Electricity was not a thing, I remember drinking hot chocolate in the hut lighted up by oil lamps.\n\n\n\n\n\n\nSo how do we spend the majority of our time? Well, hiking for hours with my grandfather looking for mushrooms: boletus pinicola, boletus luridus, Lactarius deliciosus (but only if the sape was orange), mazze di tamburo (lit. drumsticks, or macrolepiota procera) amanita vaginata, russola virescens, russola cyanoxantha and a few more. These excursions were always fun for us and scary for our parents. The reason? My grandfather never used tracks, we always explored the woods, away from the areas where other people would go. So imagine, I was 6 and I was walking 8 hours next to cliffs or underneath steep woods. Surely a hard work, but fun and rewarding.\n\n\n\n\n\n\nboletus pinicola\n\n\n\n\n\n\n\nboletus luridus\n\n\n\n\n\n\n\n\n\namanita vaginata\n\n\n\n\n\n\n\nmacrolepiota procera\n\n\n\n\n\nThroughout the years I acquired some specific knowledge. First of all, how to navigate that terrain safely while keeping up with my grandfather. He’s a fast walker, something I acquired too. Then, how to recognise good mushrooms (above), from those that we should avoid (amanita phalloidis, amanita muscaria, russola emetica). How to deal without the comforts of our homes for a long time. But most importantly, the love for the mountain and nature.\nRecently, I went back home in Bergamo. It was a fantastic time and I managed to squeeze in a few days in the mountains. It was winter there, so found some snow (not as mush as I would have imagined, but still fun). My family and I went up to the rifugio Vodala equipped with crampons. After a great lunch, we decided to keep going up and my sister, my cousin (Luca, the one not looking at the camera in the first picture), my mum and I attempted to reach Cima Timogno. However, fog started to build up and we needed enough time to go down. So we stopped half way and turned back. I am not really experienced in winter alpine terrain, so that was a nice challenge. It made me want some training and experience in winter alpine excursions.\n\n\n\n\n\n\nI also brought my wife up to Monte Avaro twice. Once to explore around and once to try bob sleading. Not the one on the ice track, but the kid version on a plastic sled. We climbed up a steep section of Monte Avaro where there were no people around and we went down a few times. It was fun to try this again after many years.\n\n\n\n\n\n\nMy family going up to Rifugio Vodala\n\n\n\n\n\n\n\nTurn around point with my cousin\n\n\n\n\n\n\n\n\n\nMonte avaro winter 2022\n\n\n\n\n\n\n\nDescending from monte tetta - Avaro 2022\n\n\n\n\n\nNow I am back in New Zealand, and I have a strong wanting to explore and reconnect with this part of my life I have put a bit aside over the past few years since I moved here. For a draw of luck, a friend of mine and his partner are into hiking and camping too, so it would be nice to have people around here to explore with. As said at the very beginning, this section will be a collection of the different excursions, short or long, I’ll do. Each page will have a short description (definitely shorter than this), reporting the major elements of the hike.\n\nThis section is dedicated to my grandfather that, now 88 years old, keeps asking me when we will go to the mountain to look for mushrooms. He always jokes that he might not be able to hike much anymore, but every time we end up walking 6 hours no stop…and he leads the whole way"
  },
  {
    "objectID": "exploring/monte_avaro.html",
    "href": "exploring/monte_avaro.html",
    "title": "Monte Avaro collection",
    "section": "",
    "text": "My sister and I with Boletus Pinicola\n\n\n\n\n\nRiding a goat"
  },
  {
    "objectID": "exploring/monte_avaro.html#december-2022",
    "href": "exploring/monte_avaro.html#december-2022",
    "title": "Monte Avaro collection",
    "section": "December 2022",
    "text": "December 2022\n\n\n\nEating panettone in the snow\n\n\n\n\n\nDrinking from a natural spring\n\n\n\n\n\nWinter view\n\n\n\n\n\nGoing into the woods"
  },
  {
    "objectID": "exploring/monte_avaro.html#videos",
    "href": "exploring/monte_avaro.html#videos",
    "title": "Monte Avaro collection",
    "section": "Videos",
    "text": "Videos\n\n\nVideo\nLaura (wife) decided she had enough of me\n\n\n\n\nVideo\nI decided to slide too\n\n\n\n\nVideo\nBad attempt to a front flip\n\n\n\n\nVideo\nBad attempt to stay bare feet in the snow\n\n\n\n\nVideo\nWoods with snow\n\n\n\n\nVideo\nBob sleading first attempt\n\n\n\n\nVideo\nLaura’s first time on a bob\n\n\n\n\nVideo\nA steeper but better bob ride"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniele Scanzi",
    "section": "",
    "text": "I spend my day looking at squiggly lines produced by your brain. I would love to know how a bat sees the world. One day I ate five razorblades in front of 400 people."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html",
    "href": "posts/2022-10-03-k-means-clustering/index.html",
    "title": "K means clustering",
    "section": "",
    "text": "K-means is a class of unsupervised clustering algorithms that was developed within the field of signal processing. Given a set of data points \\(X={x_{1}, x_{2}, x_{3}, ..., x_{N}}\\), the aim is to find k clusters of points so that the Euclidean mean of the points within each cluster is minimised. Conversely, the distance between clusters is maximised.\nAlthough there are many different k-means algorithms, the general procedure is the following:\n\nDefine the number of clusters (k)\nInitialise the center point (centroid) for each cluster\nCompute the distance between each point and every centroid\nAssign points to the cluster whose centroid is minimally distant\nUpdate the centroid location\nRepeat assignment and updates until the difference between iterations in negligible\n\nFor this simulation we are going to use the data provided here\nThe dataframe looks like this.\n\n\n# A tibble: 6 × 2\n       x     y\n   <dbl> <dbl>\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "title": "K means clustering",
    "section": "Define the number of clusters",
    "text": "Define the number of clusters\nThe number of clusters depends on the specific application. For instance, in EEG microstate analysis one common practice is to define the use of 4 clusters, which are descriptively called A, B, C, and D. However, note that defining a number a priori is a drawback of this technique. Ideally we would like to find a value that allows explaining the greatest proportion of variability in the data (without assigning each data point to a different group). Consequently, forcing the use of 4 - or any other number - of clusters might be a suboptimal option. We are going to discuss this a bit more later on. For the moment let’s be sneaky and look at the data.\nOur data contains X and Y coordinates for 60 points which are so distributed:\n\n\n\n\n\nFrom the plot we can reasonably say that there are 3 clusters, so we are going to work with that.\n\n#Define number of clusters (k)\nk <- 3"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "title": "K means clustering",
    "section": "Initialise centroids",
    "text": "Initialise centroids\nThe first step of this algorithm is to select the k points that are stereotypes of the clusters. In other words, points that are representative of the groups we want to create. These points are called centroids or prototypes. Obviously, we do not know what groups we will end up with, so the simplest way to select the first centroids is to pick k points at random. Let’s define a function that does exactly this.\n\n# Define function that select k centroids from a dataset.By default the function will select 2 centroids if no k is provided\n\npick_centroids <- function(data, k, seed=1234){\n  # Randomly select k rows from the dataset provided\n  set.seed(seed)\n  centroids <- data[sample(nrow(data),k), ]\n  # Add a unique letter label\n  centroids <- cbind(centroids, 'label'=LETTERS[1:k])\n  return(centroids)\n}\n\nWe can now pick 3 random centroids and visualize them.\n\n# Select first centroids\ncentroids_1 <- pick_centroids(df, k=3, seed=19)\n\n# Visualise them\ndf %>% \n  ggplot(aes(x=x, y=y)) +\n  geom_point(size=2, alpha=0.5, colour='gray') +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=5, shape=15) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "href": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "title": "K means clustering",
    "section": "Compute the distance between each point and each cluster",
    "text": "Compute the distance between each point and each cluster\nOnce the first centroids have been selected, we can start to divide all the other points into the corresponding clusters. Each point will be assigned to the cluster represented by the centroid that is its closest geometrically. To do so, we need to compute the Euclidean distance between every point and every centroid. Then, we select the minimum distance and assign the point to that centroid’s group. The Euclidean formula is:\n\\[ \\bar{A,B} = \\sqrt{(x_{A} - x_{B})^{2} + (y_{A} - y_{B})^{2}} \\] The following function returns two pieces of information for each point. Firstly, the assigned group as defined by the minimum Euclidean distance from the corresponding centroid. Secondly, an “error’ value defined as the distance between the point and its closest centroid. We will use this error to set up a stopping rule for our k-means algorithm later on.\n\n# Define function to compute the Euclidean distance\neuclidean_distance <- function(data, centroid){\n  distance <- sapply(1:nrow(data), function(i){\n    sqrt(sum((data[i,] - centroid)^2))\n  })\n  return(distance)\n}\n\n\n# Define a function that applies the euclidean distance to each point and returns the minimum \n# Note that this function presupposes that the centroids have a x and y coordinates columns\nfind_min_distance <- function(data, centroids, c_coord){\n  # Firstly we compute the distance between each point and each centroid\n  distances <- sapply(1:nrow(centroids), function(i){\n    euclidean_distance(data, centroids[i, c_coord])\n  })\n  \n  # For each point let's find the centroid with the minimum distance\n  min_idx <- apply(distances, 1, which.min)\n  \n  # We also extract the minimum distance so we can return it\n  min_distance <- apply(distances, 1, FUN = min)\n  \n  # Extract the associated labels\n  min_labels <- sapply(1:length(min_idx), function(i){\n    centroids$label[min_idx[i]]\n  })\n  \n  return(list('error'=min_distance, 'labels'=min_labels))\n}\n\nNow we can apply this to every point in our dataset.\n\nfirst_iter <- find_min_distance(df, centroids_1, c('x', 'y'))\n\n# Let's plot this\ncbind(df, 'label' = first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour = label)) +\n  geom_point(size=2, alpha=0.7) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThat looks like a good starting point, although the B group dominates most of the data. To improve the categorisation, we can build from here by repeating this process over and over. Each time we will select new centroids and assign the points to the group represented by the closest centroid. We do this until there are no more significant changes in the groups."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "title": "K means clustering",
    "section": "Update centroids",
    "text": "Update centroids\nAfter one iteration, we need to update the centroids. A simple way to do this is by computing the mean coordinate values for each group. The new centroids will be defined by these mean coordinates.\n\nupdate_centroids <- function(df, labels){\n  new <- cbind(df, 'label' = labels) %>% \n    group_by(label) %>% \n    summarise(x = mean(x), \n              y = mean(y)) %>% \n    relocate(label)\n    \n  return(new)\n}\n\n# Compute new centroids\ncentroids_2 <- update_centroids(df, first_iter$labels)\n\n# Plot old and new centroids\ncbind(df, 'label'=first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour=label)) +\n  geom_point(size=2, alpha=0.5) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=3, shape=15) +\n  geom_point(data=centroids_2, aes(x=x, y=y, colour=label), size=5, shape=4) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe squares represent the original centroids, the Xs represent the new ones, and the points are still coloured according to their original categorisation. Notice how the blue centroid is now in the centre of its group."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "href": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "title": "K means clustering",
    "section": "Reiterate",
    "text": "Reiterate\nWe are ready to reiterate the update and assignation process N times. As said above, we will stop when there are no more significant differences between one categorisation and the next one. To quantify this, we will use the error value we introduce just before. For each point, this is represented by the distance of the point from its closest centroid. Thus, we can sum these error values and use this sum as the stopping rule. When the sum of errors is reduced below a predetermined threshold, then we can stop.\n\nmy_kmeans <- function(data, k=2, c_coord= c('x', 'y'), tolerance=1e-4, seed=1234){\n  # Firstly we find the first centroids\n  current_centroids <- pick_centroids(data, k=k, seed=seed)\n  \n  # Create datasets were to store results\n  labelling <- c()\n  centroids <- current_centroids\n  \n  # Reiterate labelling - assignment - update centroids\n  continue <- TRUE\n  iter <- 0\n  previous_error <- 0\n  while(continue){\n    \n    # Assign data to centroids\n    current_groups <- find_min_distance(data, current_centroids, c_coord)\n    \n    # Store assigned labels with column name as the iteration number\n    iter <- iter + 1\n    labelling <- cbind(labelling, current_groups$labels)\n    \n    # Update centroids\n    current_centroids <- update_centroids(data, current_groups$labels)\n    centroids <- rbind(centroids, current_centroids)\n    \n    # Check if we have minimizes the error below the threshold\n    current_error <- sum(current_groups$error)\n    current_err_diff <- abs(previous_error - current_error)\n    print(sprintf('Iteration %s -> Error: %s', iter, current_err_diff))\n    if(current_err_diff <= tolerance){\n      continue = FALSE\n    }\n    # If we did not reach the tolerance, update the current error\n    previous_error <- current_error\n    \n  }\n  colnames(labelling) <- 1:iter\n  # remove last centroid data as it has not been used and assign iter values\n  centroids <- centroids[1:(nrow(centroids)-k), ]\n  centroids <- cbind(centroids, 'iter'=rep(1:iter, each=k))\n  return(list('lables'=labelling, 'centroids'=centroids, 'error'=current_groups$error))\n}\n\nLet’s iterate on our data.\n\nresults1 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=4)\n\n[1] \"Iteration 1 -> Error: 458.171667198993\"\n[1] \"Iteration 2 -> Error: 72.4112246075529\"\n[1] \"Iteration 3 -> Error: 0.655622083297658\"\n[1] \"Iteration 4 -> Error: 0\"\n\n\nSweet, for this particular case the algorithm converged in 4 iterations. Let’s see the final result.\n\ncbind(df, 'group'=results1$lables[, ncol(results1$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThis looks all good! Yay!"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "title": "K means clustering",
    "section": "Initialization problems",
    "text": "Initialization problems\nThe previous result might make you think that this algorithm is amazing. It categorised our data in just four iterations and with a perfect division. However, things are always more complicated than they initially appear. Indeed, a drawback of the k-means approach is that the final result is highly dependent on the initial centroids. We can demonstrate this by starting the algorithm with different initial centroids. We will exploit the seed argument we provided to our functions.\n\nresults2 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=19)\n\n[1] \"Iteration 1 -> Error: 1278.04142581076\"\n[1] \"Iteration 2 -> Error: 549.175741899334\"\n[1] \"Iteration 3 -> Error: 120.573809438913\"\n[1] \"Iteration 4 -> Error: 7.0347771777997\"\n[1] \"Iteration 5 -> Error: 10.0949551678153\"\n[1] \"Iteration 6 -> Error: 5.53498901037312\"\n[1] \"Iteration 7 -> Error: 4.606053850374\"\n[1] \"Iteration 8 -> Error: 1.49594562166419\"\n[1] \"Iteration 9 -> Error: 2.36620189065968\"\n[1] \"Iteration 10 -> Error: 0\"\n\ncbind(df, 'group'=results2$lables[, ncol(results2$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2, alpha=.5) +\n  geom_point(data=results2$centroids %>% filter(iter==max(iter)), aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nHere the algorithm converged to a suboptimal solution. During the years different solutions have been created to address this problem, with the most popular and reliable being the kmeans++ algorithm created by David Arthur and Sergei Vassilvitskii. If you are interested, the procedure is presented here"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "title": "K means clustering",
    "section": "How many clusters?",
    "text": "How many clusters?\nAs stated in the introduction, one obvious limitation of this paradigm is that the number of clusters needs to be defined a priori. Thus, we need a system that would allow us to select the optimal number of clusters that reduces the classification error as much as possible without “overfitting”. One simple method to do so is to run the algorithm with different number of clusters and use the scree plot of the error as a guide. To explain this let’s change dataset and pick something with a greater number of observations and a less clear number of clusters. We will use the iris dataset provided in R. Our task is to cluster the flowers into species based on the sepal length and the petal width.\nIf we look at the raw data we can see two possible groups, but now the situation is more complex than before.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nTo establish the optimal amount of clusters, we are going to run our k-means algorithm 10 times adding one cluster at each iteration. Each time we will store the final error, so we can plot it later. Here’s the code:\n\nks <- 1:10\nerrors <- rep(0, length(ks))\niris_df <- iris[, c('Sepal.Length', 'Petal.Width')]\ncolnames(iris_df) <- c('x', 'y')\nfor(r in ks){\n  errors[r] <- sum(my_kmeans(iris_df, k=r)$error)\n}\n\nNow we can create the scree plot by visualising the final error for each iteration.\n\n# Make scree plot of the errors\ndata.frame('error'=errors, 'k'=1:length(errors)) %>% \n  ggplot(aes(x=k, y=error)) +\n  geom_point(size=2) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:length(errors)) +\n  theme_minimal()\n\n\n\n\nThe scree plot is a “descriptive tools” so it won’t tell specifically the correct number of clusters. The main idea here is to look at the elbow of the plot, that is the point at which the trend plateau. This flexion point indicates the value after which increasing the number of groups does not provide a significant decrease in the error. Looking at the plot we can say that the elbow is in between k=3 or k=4. As a starting point we can visualise both of them:\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3)\n\n[1] \"Iteration 1 -> Error: 84.5340503544893\"\n[1] \"Iteration 2 -> Error: 22.5320451284015\"\n[1] \"Iteration 3 -> Error: 0.12222997427822\"\n[1] \"Iteration 4 -> Error: 0.194408399479336\"\n[1] \"Iteration 5 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4)\n\n[1] \"Iteration 1 -> Error: 67.9944121319274\"\n[1] \"Iteration 2 -> Error: 9.13499108999842\"\n[1] \"Iteration 3 -> Error: 1.30457150911509\"\n[1] \"Iteration 4 -> Error: 0.932100341554694\"\n[1] \"Iteration 5 -> Error: 1.0714446191438\"\n[1] \"Iteration 6 -> Error: 0.716841774592375\"\n[1] \"Iteration 7 -> Error: 0.675081843250041\"\n[1] \"Iteration 8 -> Error: 0.318333061193755\"\n[1] \"Iteration 9 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nThe groups are very similar, or at least there are no weird differences. This is a good thing. To better investigate possible difference between the two clustering systems, we could re-run them with a different seed and see if they are consistent (although we have discussed the implications that the first initialization can have).\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 99.0311396766634\"\n[1] \"Iteration 2 -> Error: 24.4105692091305\"\n[1] \"Iteration 3 -> Error: 4.88652621704111\"\n[1] \"Iteration 4 -> Error: 3.47693331688762\"\n[1] \"Iteration 5 -> Error: 2.56320926311736\"\n[1] \"Iteration 6 -> Error: 1.62704674573919\"\n[1] \"Iteration 7 -> Error: 0.0724143145959744\"\n[1] \"Iteration 8 -> Error: 0.057240636048725\"\n[1] \"Iteration 9 -> Error: 0.0225024050884883\"\n[1] \"Iteration 10 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 97.5708407922748\"\n[1] \"Iteration 2 -> Error: 29.7874124595873\"\n[1] \"Iteration 3 -> Error: 7.83693472432838\"\n[1] \"Iteration 4 -> Error: 2.47590455673429\"\n[1] \"Iteration 5 -> Error: 0.153319685392383\"\n[1] \"Iteration 6 -> Error: 0.0496830708143321\"\n[1] \"Iteration 7 -> Error: 0.0799826359733871\"\n[1] \"Iteration 8 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nNote that the colours can change between iterations. What we are interested in are the patterns of groups. The results are interesting. k=3 produced the same results as before. Conversely, k=4 did something different; it divided the lower cluster of points into two groups. Although this is not a formal assessment, from here we might want to say that k=3 is more reliable and go with it. As a final test, let’s compare these results agains the real division by species.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour=Species)) +\n  geom_point(size=2) + \n  theme_minimal()\n\n\n\n\nLet’s hihglight the differences:\n\n# Let's assign a species label to to A, B, D clusters\nspecies <- unique(iris$Species)\n\n# Merge datasets and code whether there groups are the same\niris_binded <- cbind(iris, 'my_kmeans' = iris_k3$lables[, ncol(iris_k3$lables)]) %>% \n  mutate(\n    my_species = case_when(\n      my_kmeans == 'A' ~ species[3],\n      my_kmeans == 'B' ~ species[1], \n      TRUE ~ species[2]),\n    equal = case_when(my_species != Species ~ 0,\n                      TRUE ~ 1))\n\n# Plot highlighting difference\niris_binded %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour = factor(equal))) +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c('red', 'lightgray'), name = 'Difference', labels = c('different', 'equal')) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe red dots are the observations that our kmeans algorithm categorised differently compared to the original division into species. Overall, the result can be improved but it’s not too bad considering that the majority of points were correctly identified."
  },
  {
    "objectID": "posts/2023-02-13-coloured-gabors-psychopy/index.html",
    "href": "posts/2023-02-13-coloured-gabors-psychopy/index.html",
    "title": "Creating coloured Gabor stimuli with Psychopy",
    "section": "",
    "text": "Some of the work I am involved in requires the use of Gabor patches. These are formally defined as sinusoidal gratings convolved with a Gaussian kernel. The definition looks scarier than it is. Let’s look to a Gabor to understand why\nAs you can see, this stimulus is pretty simple, a bunch of black and white stripes that fade along their extremities. Lines and fading are the key points to understanding the definition. Indeed, we can create a parallel between lines and sinusoidal grating and between fading and Gaussian kernel.\nTo create a series of black-and-white lines, we can use a sinusoid. Think about this, a sinusoid is a function that fluctuates up and down. We can use it to describe continuous and repeating fluctuations between two states, for instance, between two opposite colours, like black and white. To do this, we set that the peak value of the sinusoid represents black, while the trough value black. Everything in between is a graded shade of grey.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nsamples  <- seq(1, 10, by = 0.0001)\nsinusoid <- sin(2*pi*1*samples)\n\nsimple_sine <- data.frame(x = samples, y = sinusoid) %>% \n    ggplot(aes(x=x, y=y)) +\n    geom_line(size = 1.5) +\n    theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ndata.frame(x = samples, y = sinusoid) %>% \n    ggplot(aes(x=x, y=y, colour = sinusoid)) +\n    geom_line(size = 1.5) +\n    scale_colour_gradient(low = \"black\", high = \"white\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Creating coloured Gabor stimuli with Psychopy\n\n\n\n\n\n\n\nPython\n\n\ncoding\n\n\nexperiments\n\n\n\n\nHow to use Psychopy to create coloured Gabor patches or grating stimuli\n\n\n\n\n\n\nFebruary 13, 2023\n\n\nDaniele Scanzi\n\n\n\n\n\n\n  \n\n\n\n\nK means clustering\n\n\n\n\n\n\n\nR\n\n\ncoding\n\n\nmicrostates\n\n\n\n\nK means algortihm walk through presented at the lab meeting to explain this technique.\n\n\n\n\n\n\nOctober 3, 2022\n\n\nDaniele Scanzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/bayesian_stat_material/bayes_intro.html",
    "href": "projects/bayesian_stat_material/bayes_intro.html",
    "title": "Overview",
    "section": "",
    "text": "A student-led group to learn Bayesian statistics - For students by students\n\n\nThis project was initially born when a friend of mine, Dylan Taylor (PhD student at the University of Auckland), and I wanted to meet over lunch to work through the book Statistical Rethinking. Talking with other PhD students, we soon realised that many researchers are interested in applying bayesian stats to their projects. However, most students didn’t have the opportunity to learn this and feel like learning it alone can be too difficult and time-consuming - a feeling we share too. So, we decided to expand the group to other interested students, and we started a series of weekly meetups. During these we go through the 2022 lecture series accompanying the book, we have discussions and work through exercises together. The group has expanded to around 15 people and has become a great opportunity to learn something useful together and create new connections between like-minded people.\nIn this section, you can find the slides and material we create to provide direction during the meetups.\n\nThis project has now been awarded a Creating Connections Grant which will help to provide refreshment during the meetups"
  },
  {
    "objectID": "projects/bayesian_stat_material/slides_list.html",
    "href": "projects/bayesian_stat_material/slides_list.html",
    "title": "Slides",
    "section": "",
    "text": "Here you can find the link to the slides openly available on RPubs.\n\nIntroduction to Bayesian Stat\nBayes Theorem\nBayesian Engines"
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "",
    "text": "The idea for this document comes from Makoto’s useful EEGLAB code. The intention behind this document is to have a place to store code that I produce while working on my projects and that I find useful. Besides being a somewhat unstructured place to save code that I could use in the future, this document might be helpful for researchers interested in using R to analyse EEG data."
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#from-eegutils-to-tidy-data",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#from-eegutils-to-tidy-data",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "From eegUtils to tidy data",
    "text": "From eegUtils to tidy data\neegUtils is an R package developed by Matt Craddock that allows you to analyse EEG data in R. Currently, I pre-process data in Matlab with EEGLAB. Thus, I use eegUtils to load pre-processed and epoched data in R to conduct statistical analysis. Although eegUils is a handy package, and I love that I can easily use R to do EEG data analysis and plot scalp topographies, some aspects of this package do not align with my preferences. Specifically:\n\nThe voltage data is stored in wide format, with one column for each electrode\nTime information is stored separately from the EEG signal information\n\nConsequently, I find it easier to extract the data I care about (voltage, electrode names and time values) and store those in a long-format tibble. As I do this often, I wrote a function for this.\nNote that some of the functions you find in the other paragraphs require the data extracted from eegUtils using this function. I highlighted when this is the case\n\ntidy_signal_from_eegUtils <- function(eegUtilsDataset) {\n  \n  tidy_data <- eegUtilsDataset %>% \n  # Add time column\n  dplyr::mutate(time = signal[[\"timings\"]][[\"time\"]]) %>% \n  # Reorganise the dataset \n  dplyr::pivot_longer(cols      = !time,\n               names_to  = \"electrode\",\n               values_to = \"volt\") \n  \n  return(tidy_data)\n  }"
  },
  {
    "objectID": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#computing-erp-amplitudes",
    "href": "projects/daniele_useful_code_r/daniele_useful_code_r_eeg.html#computing-erp-amplitudes",
    "title": "Daniele’s useful R code for EEG data analysis",
    "section": "Computing ERP amplitudes",
    "text": "Computing ERP amplitudes\nThe following code presupposes that the EEG data is stored in a dataframe or tibble in long format. The dataset must contain three columns:\n\ntime: time in ms for each sample\nelectrode: electrode name\nvolt: the voltage value for each sample\n\nTo create such a dataset from a file loaded through eegUtils see working with eegUtils.\nThere are two common ways to compute an ERP amplitude: average amplitude and peak amplitude. Average amplitude involves averaging the microvolts of a signal over a specified time range (where the ERP is expected). The peak amplitude consists in finding the maximum value of a peak or the minimum value of a trough. Peaks and troughs are the ERPs. Remember that peak amplitudes have drawbacks, and the average amplitude should be preferred.\nHere are some functions to compute both values. I also provide a wrapper function that extract both average and peak amplitudes.\n\nExtracting time bins\nFor flexibility, we need a function to select the signal within a specific time bin. We will use this function to allow the user to define the time range where to extract the average and peak values.\n\n# Function to extract the signal in a defined time bin. The time bin \n# can be specified as:\n#     - c(min, max): with the minimum and max time in ms\n#     - max: only the maximum value in ms\nextract_bin_signal <- function(eegDataset, bin = NULL) {\n    # Time bin must be provided\n    if (is.null(bin)) {\n        stop(\"Time bin is missing.\")\n    }\n    # If min and max provided\n    if (length(bin == 2)) {\n        binned <- eegDataset %>% \n          # Group by electrode, so to extract signal for each electrode independently\n          dplyr::group_by(electrode) %>% \n          # Select the requested time range\n          dplyr::filter(time >= bin[1] & time <= bin[2]) %>% \n          dplyr::ungroup()\n    # If only the max is provided\n    } else if (length(bin == 1)) {\n        binned <- eegDataset %>%\n          # Group by electrode, so to extract signal for each electrode independently\n          dplyr::group_by(electrode) %>% \n          # Select the requested time range - from first time point to requested max\n          dplyr::filter(time <= bin) %>% \n          dplyr::ungroup()\n    # If the time range is not a vector of one or two values throw an error\n    } else {\n        stop(\"Bin must be a vector of one or two values\")\n    }\n}\n\n\n\nCompute average amplitude\nTo extract the average amplitude, we can simply compute the mean of the voltage for each electrode.\n\n# Define a function to find the average amplitude\n# Note that this function requires that the dataset contains the electrode and \n# volt columns \nfind_average_amplitudes <- function(eegDataset) {\n    # compute average amplitude for each electrode\n    return(eegDataset %>%\n               group_by(electrode) %>% \n               summarise(average_amplitude = mean(volt)) %>% \n               ungroup()\n    )\n    \n}\n\n\n\nCompute the peak amplitude\nThis function is quite complex. The complexity is driven by the fact that an ERP can be any positive or negative deflection in an EEG signal. For a positive deflection, we want to find its maximum value. For a negative deflection, we want to find the minimum value. To do this, we use the findpeaks function of the pracma package. As this function finds only the local peaks of a signal, to find the local minima we will multiply the signal by -1 to invert peaks and troughs.\nNote that this function is quite flexible as the user is able to select whether to extract only peak, only troughs or both amplitudes. Importantly, as a signal might have only peaks and not troughs, or vice versa, if one or the other is not found, an NA is produced.\n\n#|eval: false\n\nfind_peak_amplitudes <- function(eegDataset, peaks = TRUE, valleys = TRUE) {\n    \n    # Split eeg dataset into list of dataframes, each one containing the signal\n    # for one channel\n    data_list <- eegDataset %>% \n        dplyr::group_by(electrode) %>% \n        dplyr::group_split()\n    \n    # Extract only the signal to pass to findpeaks\n    data_volt       <- lapply(data_list, function(x) x[[\"volt\"]])\n    # Extract the electrode information to use later\n    list_electrodes <- sapply(data_list, function(x) x[[\"electrode\"]][[1]])\n    # Extract time vectors \n    list_times      <- sapply(data_list, function(x) x[[\"time\"]]) \n    \n    \n    ## If both peaks and valleys are requested\n    if (peaks & valleys) {\n        \n        # Find peak amplitudes, peak_idx, [start:stop]\n        peaks_info   <- sapply(data_volt, pracma::findpeaks, npeaks = 1)\n        # Find valleys ampl, peak_idx, [start:stop] - Here we use the function\n        # findpeaks as above. Thus, we need to invert the signal so that valleys\n        # become peaks and will be detected. Because of this, later we will \n        # invert the sign of the signal to revert to the original value\n        valleys_info <- sapply(lapply(data_volt, function(x) x*-1), pracma::findpeaks, npeaks = 1)\n        \n        # NOTE: peaks_info and valleys_info work well when the signal has peaks\n        # and valleys. However, as soon as a signal does not have one of those,\n        # then `findpeaks` returns NULL and `sapply` fails to produce a matrix and \n        # defaults to a list. This, in turn, produce an error in extracting the \n        # peak times. As a NULL list as no dimension, when we call \n        # `[[peaks_info[[2, x]]` (see later), we get an error. To avoid this, we\n        # capture any NULL list, convert it into a matrix of [1 x 4] NA, and \n        # convert this into what sapply would produce (i.e. a matrix). The size\n        # of the matrix reflect the output of findpeaks when a peak is found.\n        \n        # If sapply produced a list - thus we have NULL nested lists\n        if (is.list(peaks_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            peaks_info_no_null <- lapply(peaks_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            peaks_info <- sapply(peaks_info_no_null, function(x) x)\n        }\n        \n        # Repeat the same with valleys_info\n        if (is.list(valleys_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            valleys_info_no_null <- lapply(valleys_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            valleys_info <- sapply(valleys_info_no_null, function(x) x)\n        }\n        \n        # Extract peak times\n        peaks_time   <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(peaks_info[[2, x]])) NA_real_ else list_times[[peaks_info[[2, x]], x]])\n        valleys_time <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(valleys_info[[2, x]])) NA_real_ else list_times[[valleys_info[[2, x]], x]])\n        \n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"peak\"        = peaks_info[1, ],\n            \"valley\"      = -valleys_info[1, ], # Revert the sign for the valleys\n            \"peak_time\"   = peaks_time,\n            \"valley_time\" = valleys_time\n            )\n        )\n    # If only peaks are requested\n    } else if (peaks & !valleys) {\n      # Find peaks ampl, peak_idx, [start:stop]\n        peaks_info   <- sapply(data_volt, pracma::findpeaks, npeaks = 1)\n        \n        # If sapply produced a list - thus we have NULL nested lists\n        if (is.list(peaks_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            peaks_info_no_null <- lapply(peaks_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            peaks_info <- sapply(peaks_info_no_null, function(x) x)\n        }\n        \n        # Extract times\n        peaks_time   <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(peaks_info[[2, x]])) NA_real_ else list_times[[peaks_info[[2, x]], x]])\n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"peak\"        = peaks_info[1, ],\n            \"peak_time\"   = peaks_time\n            )\n        )\n    # If only troughs are requested\n    } else if (!peaks & valleys) {\n        # Find valleys ampl, peak_idx, [start:stop] - Here we use the function\n        # findpeaks as above. Thus, we need to invert the signal so that valleys\n        # become peaks and will be detected. Because of this, later we will \n        # invert the sign of signal to revert to the original value\n        valleys_info <- sapply(lapply(data_volt, function(x) x*-1), pracma::findpeaks, npeaks = 1)\n        \n        # Repeat the same with valleys_info\n        if (is.list(valleys_info)) { \n            # Change all NULL to a [1x4] matrix - the dimension follows what would\n            # be the output of `findpeaks`\n            valleys_info_no_null <- lapply(valleys_info, \n                                         function(x) if(is.null(x)) matrix(rep(NA_real_, 4), nrow = 1) else x)\n            # Matrix the results through sapply\n            valleys_info <- sapply(valleys_info_no_null, function(x) x)\n        }\n        \n        # Extract times\n        valleys_time <- sapply(1:ncol(list_times), \n                               function(x) if(is.na(valleys_info[[2, x]])) NA_real_ else list_times[[valleys_info[[2, x]], x]])\n        # Store the results in a handy tibble\n        peaks_amplitudes <- as_tibble(cbind(\n            \"electrode\"  = list_electrodes,\n            \"valley\"      = -valleys_info[1, ], # Revert the sign for the valleys\n            \"valley_time\" = valleys_time\n            )\n        )\n    # if peaks and valleys argument are set both to FALSE\n    } else {\n        stop(\"At least one argument between 'peaks' and 'valleys' must be TRUE\")\n    }\n    \n    # Return the result tibble\n    return(peaks_amplitudes)\n    \n}\n\n\n\nWrapper function for ERP amplitudes\nThe above functions can be used separately. However, it could be handy to have a wrapper function that allows the user to compute both peak and average amplitudes simultaneously. Again, the following function is highly flexible, allowing the user to:\n\nDefine a time bin where to compute the amplitude measures. If no bin is provided, the whole signal is used\nSelecting whether to extract the only peak, only troughs, only averages amplitude or any combination of those\n\n\n# We then put all together in a single function\nfind_erp_amplitudes <- function(eegDataset, bin = NULL, peaks = TRUE, valleys = TRUE, avg = TRUE) {\n    \n    # If a time bin is provided, call extract_bin_signal (see above) to extract \n    # only the signal wirthin the requested time range\n    if (!is.null(bin)) {\n        # Extract the signal in the defined bin\n        binned_signal <- extract_bin_signal(eegDataset, bin = bin)\n    # If no time bin is provided, use the whole signal\n    } else {\n        binned_signal <- eegDataset\n    }\n    \n    # If peaks, troughs and average amplitude are requested\n    if ((peaks | valleys) & avg) {\n        # Extract peak and average amplitudes\n        peak_amps     <- find_peak_amplitudes(binned_signal, peaks = peaks, valleys = valleys)\n        avrg_amps     <- find_average_amplitudes(binned_signal)\n    \n        amplitudes    <- merge(peak_amps, avrg_amps)\n    # If only peak and troughs amplitudes are requested\n    } else if ((peaks | valleys) & !avg) {\n        amplitudes <- find_peak_amplitudes(binned_signal, peaks = peaks, valleys = valleys)\n    # If only troughs and average amplitude are requested\n    } else if (!(peaks | valleys) & avg) {\n        amplitudes <- find_average_amplitudes(binned_signal)\n    # If every amplitude argument is set to FALSE\n    } else {\n        stop(\"You need to provide at least one argument between peaks, valleys or avg\")\n    }\n\n    return(amplitudes)\n}"
  },
  {
    "objectID": "projects/projects_intro.html",
    "href": "projects/projects_intro.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a collection of material related to different projects I have been working on, either alone or in collaboration with other people."
  }
]