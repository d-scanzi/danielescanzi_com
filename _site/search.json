[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "~/About",
    "section": "",
    "text": "Daniele completed his Bachelor of Science in Psychology and Communication at the University of Milan - Bicocca in 2018. He joined the University of Auckland in 2021 as an Honours student. He is now starting a PhD working with Paul Corballis, director of the Cognitive Psychophysiology Lab and co-advised by Christopher Erb. He is interested in investigating the neural correlates of consciousness using EEG, eye-tracking and whatever machine he can get his hands on. He has a second identity as a magician and his assistant is a tiny blue plunger."
  },
  {
    "objectID": "educating/educating_intro.html",
    "href": "educating/educating_intro.html",
    "title": "Educating",
    "section": "",
    "text": "One of my favorite aspects of doing a PhD is teaching. I enjoy the educational aspect of academia, and I focus a lot on how to be a better educator. In this section, you’ll find a set of materials, projects, and data all related to my experience as an educator."
  },
  {
    "objectID": "educating/students_review.html",
    "href": "educating/students_review.html",
    "title": "Students Review",
    "section": "",
    "text": "When I started my PhD, I have also begun tutoring every semester. I enjoy teaching the tutorials, interacting with the students and sharing with them the knowledge I am passionate about. I often think about how to be a better educator, how to set up my tutorials and how to create a positive environment in my classes. I like to look at teaching through the lens of my past as a magician. Teaching is an act with a story to convey, attention to grab, and audience participation should be encouraged.\nMy idea of teaching may be far away from what the students want and need, I know. Unfortunately, the courses I teach do not ask students to provide feedback on their tutors, and I find this absurd. We are an integral part of their education experience, we should be able to know how we are doing and what we should work on. So, I have been actively collecting students’ evaluations for all the courses I teach. I do this at the end of each semester through an anonymous survey that students can voluntarily fill out.\nIn the name of transparency, I have decided to post all the anonymous feedback here. I will update this page each semester, and I will address the comments and reflect on what I need to work on.\nAs a side note, I try to encourage students to provide constructive feedback, and I try to stress that negative feedback is more than welcome. I really want to improve and work on how to be better, and negative feedback, if constructive, helps highlight things to change and work on. As such, I won’t hide any comments except if they are un-constructively negative or contain some sensitive information. I will add a note if this happens.\nOne of the survey’s question is to provide 3 words that describe me as a tutor. Above is a summary of the words provided so far"
  },
  {
    "objectID": "educating/students_review.html#students-marks",
    "href": "educating/students_review.html#students-marks",
    "title": "Students Review",
    "section": "Students’ marks",
    "text": "Students’ marks\nLet’s start with a simple and fun (hopefully) section where I swap the roles with the students and they are able to grade my work.\n\n\n\n\n\n\n\n\nHappy to see that there is nothing in Below Average and that the way I am currently running the tutorials is appreciated.\nIn terms of marks, the grades below are scaled by their relative proportion. The larger the letter, the higher the number of students that selected that grade. Again, I’m happy to see the A+ and A being large!"
  },
  {
    "objectID": "educating/students_review.html#what-students-appreciate",
    "href": "educating/students_review.html#what-students-appreciate",
    "title": "Students Review",
    "section": "What students appreciate",
    "text": "What students appreciate\nNow, we get to the core of the feedback. I will highlight at the top the comments that I think represent the major ideas. Nonetheless, at the end of the section, you can find an exhaustive table with all the feedback.\n\nYou were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping.\n\n\nYou did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it.\n\n\nTutorials are fun and friendly environments.\n\nThere are three main elements I think students appreciate:\n1- The content should be presented differently from the lecturers. This makes the tutorial feel less like a simple repetition and more like an opportunity to learn more and/or understand better.\n2- Give people time to ask questions. We all vary with how comfortable we are in asking questions in class. I encourage discussions, debates and questions since the very first tutorial to create an environment where everyone feels comfortable being an active part.\n3- Engage. We all agree that listening to one person talking for two hours is boring. So why do this? Showing a more fun and natural side helps create a connection, keep the attention high and make the tutorial time more enjoyable.\n\nI felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good.\n\nI agree on this point. One of the aspects I need to work more on is structure (more on this in the next session). I do not want to have everything predefined, as this is counterproductive, but I will try to define at the beginning of the tutorial the key areas I will discuss.\n\n\n\n\n \n  \n    Feedback \n  \n \n\n  \n    Interactiveness, enthusiasm and motivation \n  \n  \n    I really loved your passion and the time you took to check everybody understood. Also appreciated when you would draw diagrams to help us understand when we were all struggling to understand. \n  \n  \n    I enjoyed the activities we did!! It was fun especially with how the content was pretty full on but the fact we got to discuss often with our peers was a good aspect for me. I also liked how you went through the content first explaining everything we similarly learnt in lectures before we started because hearing it from the way you talked about it helped me understand lecture content a bit easier. And the white board examples and the pictures helped immensely!!!! It made things so much easier to grasp so please continue that because reading of PowerPoint slides for me personally doesn’t feel engaging \n  \n  \n    Engaged with smaller group discussions by walking around the room \nAnswered questions directly and was ready to do so in the middle of a presentation \n  \n  \n    Tutorials felt well prepared and always were well delivered, the content was explained clearly and sometimes having clarification of lecture material was super helpful. Your delivery was engaging and even content I'm not super into wasn't boring so thank you! You seem to be pretty into the topics that we covered which really helps. The feedback on the essay was pretty insightful too. \n  \n  \n    Trying to engage the class rather than lecturing, despite a lack of input form us 😆 \n  \n  \n    All content was explained clearly, and questions from the class were answered swiftly and very well. The way we walked through different programmes was extremely helpful for understanding the way they work and how data is arranged and understood. \n  \n  \n    You were fun and engaging, incredibly knowledgeable about the topics you were teaching, spent time with us one-on-one or in pairs when able and provided individual feedback + guidance \n  \n  \n    Constructive feedback allowing us to critically think \n  \n  \n    Always trying to make things fun and using interactive ways of learning, checking that everyone is understanding was really nice \n  \n  \n    I find it really helpful that you go in depth into the concepts as it helps me understand more. i also really appreciate in the tips you provide in our academic writing etc finding sources, i find them really helpful :) \n  \n  \n    I appreciated the quality of the content and how it was useful and complementary to the lectures \n  \n  \n    Everything is explained very well, you’re very patient with answering questions no matter how many are asked or if you get interrupted, you know the content well and relate it to us in a very simple and good way as well, you give us a very nice breakdown of how to write things and what to include in assignments (for the TBI essay) and in general you make it a very nice environment to be in and your chill/relaxed and funny vibe makes the tutorials far more engaging \n  \n  \n    Asked us questions that helped go to actively revise. \n  \n  \n    Support! You were very supportive and accurately explained what was required to do well in this class, and explained what we did not need to know as extensively - which was much appreciated as often times labs can be seen as just an overload of information. \n  \n  \n    i liked how you explained things in a different way to the lecturers which often made more sense to me. or having the content presented in a different way deepened my understanding of it. i liked how you took the time to explain something fully to me again if i still didn’t understand it \n  \n  \n    Giving supports to every student, really engaging with the class \n  \n  \n    tutorials are fun and friendly environments \n  \n  \n    Literally so fun for 9am on a Friday! You are just so engaging and knowledgeable and really get us thinking. Thanks for being such a legend! I really enjoy these labs \n  \n  \n    Always try to make tutorials fun, friendly demeanour in general, and genuinely trying to help us, thank you! \n  \n  \n    Always have interesting teaching way \n  \n  \n    Good at explaining thoroughly and gives a real opportunity for students to ask questions. Also answers questions as best as possible. Takes time to check in on each student/group individually and welcoming to ask questions to. Seems like you genuinely want to help your students understand everything clearly and do well. \n  \n  \n    You have always been very transparent and supportive of us. You helped us with everything you could and always gave us the right advice, pushing us to do so much more. The feedback on our assignments has been very thorough which also personally has helped me to incorporate into assignments from my other courses. \n  \n  \n    Support was great! feedback on the assignments was super helpful \n  \n  \n    Really encouraging and great with positive feedback. I really liked the feedback I received on my assignments throughout the course as it guided me as to how to improve in my work. \n  \n  \n    Explained alot of concepts and answers thoroughly and appreciated that he took the time to go through everything. Feedback felt subjective and catered to our learning \n  \n  \n    I felt like the tutorials were quite good, maybe trying to put more organisation with our meetings and not just us asking questions and more information being given would be good. \n  \n  \n    The feedback you gave on my blog post was super helpful! I didn't realise how much blog was lacking something big and your feedback helped me see it but you also provided specific tips to integrate so I didn't feel lost knowing I had alot of work to improve on :) You were also super on top of replying in teams and being there for support so that was something else that made you such an awesome tutor. \n  \n  \n    I appreciate how friendly and easy to approach you were. Unlike some other tutors, you really made it easy for us to ask questions - esp questions that we found hard to ask to other lecturers. Assignment feedbacks were extremely detailed and helpful and also very fair I personally think. I also like how chill you were - as long as we followed the rubric and did everything, you were very fair with your marking - literally made capstone life extremely enjoyable and rewarding to have a tutor who understood what you were doing! \n  \n  \n    Friendly manner, always willing to help \n  \n  \n    Feedback on assignments are good, you take the time to listen and help and make sure we understand \n  \n  \n    Very helpful, compassionate, supportive and understanding, explained really well \n  \n  \n    Explaining things throughly, questioning us and making us work things through \n  \n  \n    Good feedback & ways of trying to improve our learning. \n  \n  \n    You were very helpful and understanding of our places in knowledge. Your feedback was helpful in allowing for a better understanding. \n  \n  \n    The content covered was always relevant and helpful. Trying to engage the class \n  \n  \n    You were always super nice and friendly and you always made me feel super comfortable coming to class. I was also never nervous to ask you questions about assignments cause you were always very informative and good with helping. \n  \n  \n    Appreciated how patient you were, especially re-explaining something over and over again so our class understood. Your feedback and also going over and beyond. \n  \n  \n    i appreciated fast reply’s on email, and also giving feedback on assignments and work. friendly work environment \n  \n  \n    You did a great job in tutorials giving people opportunities to ask for help of they needed it and checking to make sure we were on the right track. Also you kept it engaging by relating the various topics and ideas to your own interest and the wider ideas around the topic, encouraging broader thinking about it."
  },
  {
    "objectID": "educating/students_review.html#what-students-think-i-should-do-better",
    "href": "educating/students_review.html#what-students-think-i-should-do-better",
    "title": "Students Review",
    "section": "What students think I should do better",
    "text": "What students think I should do better\n\nSometimes the discussion time in tutorials felt too long and often we’d finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway. Occasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes.\n\nI will try next year to have more class discussions and less small group work. A couple more people pointed this out. I believe that small groups would have encouraged people to talk more by reducing the pressure of having to speak out loud in front of everyone. However, doing this takes time that could be used differently.\n\nFound the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc.\n\nThis is a fair point. I almost always take the full 2 hours, most likely because I like to digress. As pointed out above, next year I will try to structure each tutorial a bit more. Specifically, I will highlight at the beginning the major point we will discuss, and I will add a recap at the end. However, I will keep some improvisation. I believe that having just a general structure of what to talk about allows for the exploration of topics, concepts and ideas that otherwise won’t be addressed. Tutorials, in my view, are not just a recap of the lectures but a moment where we can dive deeper into a topic or create connections across different areas. They are useful to broaden our understanding of the concepts discussed in the lectures and, by doing so, learn the material better.\n\nWould appreciate more concise marking, and felt marking scores were abit harsh sometimes compared to other students or the feedback received.\n\nGiven the multiple comments appreciating the lengthy feedback on the essays, I will not plan to change this. Assignments and exams are not just a test but an opportunity to improve and gain skills. By providing detailed feedback, students are able to improve their future work. I agree that I can be strict with my marking. This is partly caused by my study background and the way I have been assessed. Partly because I try to mark focusing hardly on the reasoning. That is, I don’t see just repeating the content from a book or lectures as being enough for an A+. The reasoning behind an answer is important. Nonetheless, in the class I teach, us tutors try to match our criteria, so if I am being too harsh, other tutors and professors will call me out on this before the marks are finalized.\n\n\n\n\n \n  \n    answers \n  \n \n\n  \n    Nothing really \n  \n  \n    Only suggestion I have is to speak slower. Your excitement and English are amazing, but when you speak really fast with excitement, it made it difficult to understand and keep up sometimes. \n  \n  \n    This isn't really a you thing but it was better communication with the lecturers (not really your fault) as it's kind of disheartening when tutors don't know whata's going on with what lecturers are doing for example when lab quizzes results are released or even whats going on with exams. I also wish we got to discuss with more of our peers around the room, not just the people we sat with but I guess that’s kind of hard with the layout of the room \n  \n  \n    Sometimes skipped over points too quick. \n  \n  \n    Sometimes the discussion time in tutorials felt too long and often we'd finish the task and end up going off task. Maybe less small group discussion and then a longer period for class discussion would have been more to my personal preference, but its not a major issue cause there was normally plenty of time anyway. \n  \n  \n    Occasionally you start to rush when you get excited, which is fun to see but it can lead to us missing certain points sometimes \n  \n  \n    Honestly, thought you were amazing. Maybe you could just slow down sometimes with your explanations because it can be a bit hard to keep up with \n  \n  \n    Genuinely cannot think of anything sorry \n  \n  \n    I am happy with the way the tutorials are going :) \n  \n  \n    I don't believe there is anything you could do better :) keep up the great work! \n  \n  \n    Honestly nothing, the way you do things is amazing \n  \n  \n    Summarise the lab at the end \n  \n  \n    I think maybe the first few labs had a LOT of information to be taught and not so much time for questions to be asked. This isn’t so much a tutor specific issue, rather it is a course coordination issue of how much is taught in those labs \n  \n  \n    occasionally (not often) i couldn’t understand what you were saying when you spoke fast \n  \n  \n    some tutorials cover ground that has already been covered in previous years, such as essay writing basics, though this is likely more of an issue with the general course coordination \n  \n  \n    More magic please! \n  \n  \n    Keep doing what you're doing :) \n  \n  \n    Maybe speaking a little bit slower is better \n  \n  \n    More of feedback for tutorials in general rather than the tutor but Maybe, especially in the first lesson, have more focus on having students introduce themselves in the class and have opportunities to work together and discuss with each other so that students can build relationships to each other and to tutor. \n  \n  \n    I honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done. \n  \n  \n    I honestly cannot think of anything because I believe you have been amazing and very supportive with the work that I have done. \n  \n  \n    communication could've been better, at times was difficult to get our points across as our research differed greatly from yours, which is completely okay but having a tutor who knows the topic and understands the specific research would have made it a wee bit easier for you and us. \n  \n  \n    Would appreciate more concise marking, and felt markinf scores were abit harsh sometimes compared to other students or the feedback received \n  \n  \n    Maybe provide more tips on what we should be careful of when doing each assignment - like providing common places where student often lose marks or mess up \n  \n  \n    Nothing much - everything was all good! Don't really have much feedback \n  \n  \n    deeper explanation of that weeks lecture content \n  \n  \n    I think you are good \n  \n  \n    More magic and pizza \n  \n  \n    Nothing I love this tutor \n  \n  \n    Don’t overestimate students understanding of topics \n  \n  \n    I think maybe just more in depth feedback and being super critical, even though people might not like it it’s actually very helpful. \n  \n  \n    Found the we had to rush some tutorials to fit within the 2hr time. Could try changing what info to include / exclude to fit within the 2hrs and no information is rushed. Could spend a little time going over lectures etc."
  },
  {
    "objectID": "educating/students_review.html#extra-comments",
    "href": "educating/students_review.html#extra-comments",
    "title": "Students Review",
    "section": "Extra comments",
    "text": "Extra comments\nAs my survey is meant to be quick, I leave the opportunity to express other ideas freely.\n\n\n\n\n \n  \n    answers \n  \n \n\n  \n    Great job! Thanks for being a great tutor! \n  \n  \n    Thanks so much for your help and enthusiasm! I found Psych 305 to be a really difficult class, but having you as a tutor helped me so much with the work content because your excitement made me get more excited about it, which helped me engage and learn better! I really loved having you teach us! All the best. :) \n  \n  \n    Thanks for a great semester!! \n  \n  \n    Thank you for teaching us over the past semester, and good luck with your PhD! \n  \n  \n    Great job. I personally hate tutorials, and you made them fun and engaging and made me look forward to coming :) \n  \n  \n    Thank you for a lovely lab! \n  \n  \n    I think you are a great tutor (: \n  \n  \n    thank you for all your hard work :) \n  \n  \n    Thank you! \n  \n  \n    I'm not actually in your normal stream but joined it once when I couldn’t attend my normal one and I enjoyed your tutorial so much that I made it my regular stream and have been coming along ever since \n  \n  \n    LOVE YOU DANIELE!!! You are an awesome tutor \n  \n  \n    you never gave us the pizza making class :( \n  \n  \n    Please keep tutoring! You're literally my fav! \n  \n  \n    Thank you for being such a lovely tutor this semester, I wish you luck on all your future eneavours. \n  \n  \n    No \n  \n  \n    Great mustache! And you did a great job giving students opportunity to ask questions individually (rather than to whole class) so felt more comfortable and open. Thanks! \n  \n  \n    Daniele is an amazing tutor and an amazing person. With great professionalism that can be seen on our assignments feedback and advice, he is super friendly and ensures that you are not being negatively criticised. He is highly supportive and always pushes you to perform to the best of your ability. \n  \n  \n    Daniele is an amazing tutor, who is thoughtful and empathetic. He is a great help to those who are stuck and need a direction. \n  \n  \n    Daniele was my tutor for my year 3 capstone course of my degree. He was an amazing communicator who was ways able to convey feedback in an depth manner - because of this I was motivated improve my work rather than seeing it as a daunting task. He also always came to meetings with a positive attitude and willingness to help! \n  \n  \n    Daniele is an extremely friendly tutor. Very thoughtful and always eager to help his students. His tutorial sessions were always helpful and his assignment feedbacks were very detailed and accurate. Would love to have him as my tutor for other classes too. \n  \n  \n    You are a great tutor. Continue to do the awesome work of teaching your students. \n  \n  \n    Daniele is very calm in nature and provides a positive environment that allows for learning and is very entertaining when adding his magic into his tutorials. \n  \n  \n    Daniele was a great tutor to have as he was always so nice to everyone and helpful when questions were asked. I learnt a lot in his class and it made me enjoy going to my tutorial because he was a good teacher."
  },
  {
    "objectID": "exploring/exploring_intro.html",
    "href": "exploring/exploring_intro.html",
    "title": "Exploring B/Log",
    "section": "",
    "text": "This is me (on the left), my sister (eating the bread) and my cousins at Monte Avaro. For most people this name means nothing, but this place is probably one of the most important for me. Still today, some of the most important and memories I have comes from there.\nMy grandparents used to bring us four up to this mountain every summer. However, this was not just a one day family hike, this was a month-long stay. A month-long stay in a van that my grandfather repourposed as a self-contained van, without running water, toilet or anything else. We slept all together, some years all inside the van, some other years we got a tent on top of it. We collected water from a spring a couple of hundred meters down the road, my grandmother used to cook us with a small gas stove or on a campfire, where some nights we prepared vin brulé (the Italian version of mulled wine).\nThis place was amazing. It was not popular, there was hardly anyone around, just us, an alpine cow farmer who became a family friend (you see, my grandparents used to bring our parent up here as well when they young) and a couple of people owning a hut up there. Electricity was not a thing, I remember drinking hot chocolate in the hut lighted up by oil lamps.\n\n\n\n\n\n\nSo how do we spend the majority of our time? Well, hiking for hours with my grandfather looking for mushrooms: boletus pinicola, boletus luridus, Lactarius deliciosus (but only if the sape was orange), mazze di tamburo (lit. drumsticks, or macrolepiota procera) amanita vaginata, russola virescens, russola cyanoxantha and a few more. These excursions were always fun for us and scary for our parents. The reason? My grandfather never used tracks, we always explored the woods, away from the areas where other people would go. So imagine, I was 6 and I was walking 8 hours next to cliffs or underneath steep woods. Surely a hard work, but fun and rewarding.\n\n\n\n\n\n\nboletus pinicola\n\n\n\n\n\n\n\nboletus luridus\n\n\n\n\n\n\n\n\n\namanita vaginata\n\n\n\n\n\n\n\nmacrolepiota procera\n\n\n\n\n\nThroughout the years I acquired some specific knowledge. First of all, how to navigate that terrain safely while keeping up with my grandfather. He’s a fast walker, something I acquired too. Then, how to recognise good mushrooms (above), from those that we should avoid (amanita phalloidis, amanita muscaria, russola emetica). How to deal without the comforts of our homes for a long time. But most importantly, the love for the mountain and nature.\nRecently, I went back home in Bergamo. It was a fantastic time and I managed to squeeze in a few days in the mountains. It was winter there, so found some snow (not as mush as I would have imagined, but still fun). My family and I went up to the rifugio Vodala equipped with crampons. After a great lunch, we decided to keep going up and my sister, my cousin (Luca, the one not looking at the camera in the first picture), my mum and I attempted to reach Cima Timogno. However, fog started to build up and we needed enough time to go down. So we stopped half way and turned back. I am not really experienced in winter alpine terrain, so that was a nice challenge. It made me want some training and experience in winter alpine excursions.\n\n\n\n\n\n\nI also brought my wife up to Monte Avaro twice. Once to explore around and once to try bob sleading. Not the one on the ice track, but the kid version on a plastic sled. We climbed up a steep section of Monte Avaro where there were no people around and we went down a few times. It was fun to try this again after many years.\n\n\n\n\n\n\nMy family going up to Rifugio Vodala\n\n\n\n\n\n\n\nTurn around point with my cousin\n\n\n\n\n\n\n\n\n\nMonte avaro winter 2022\n\n\n\n\n\n\n\nDescending from monte tetta - Avaro 2022\n\n\n\n\n\nNow I am back in New Zealand, and I have a strong wanting to explore and reconnect with this part of my life I have put a bit aside over the past few years since I moved here. For a draw of luck, a friend of mine and his partner are into hiking and camping too, so it would be nice to have people around here to explore with. As said at the very beginning, this section will be a collection of the different excursions, short or long, I’ll do. Each page will have a short description (definitely shorter than this), reporting the major elements of the hike.\n\nThis section is dedicated to my grandfather that, now 88 years old, keeps asking me when we will go to the mountain to look for mushrooms. He always jokes that he might not be able to hike much anymore, but every time we end up walking 6 hours no stop…and he leads the whole way"
  },
  {
    "objectID": "exploring/monte_avaro.html",
    "href": "exploring/monte_avaro.html",
    "title": "Monte Avaro collection",
    "section": "",
    "text": "My sister and I with Boletus Pinicola\n\n\n\n\n\nRiding a goat"
  },
  {
    "objectID": "exploring/monte_avaro.html#december-2022",
    "href": "exploring/monte_avaro.html#december-2022",
    "title": "Monte Avaro collection",
    "section": "December 2022",
    "text": "December 2022\n\n\n\nEating panettone in the snow\n\n\n\n\n\nDrinking from a natural spring\n\n\n\n\n\nWinter view\n\n\n\n\n\nGoing into the woods"
  },
  {
    "objectID": "exploring/monte_avaro.html#videos",
    "href": "exploring/monte_avaro.html#videos",
    "title": "Monte Avaro collection",
    "section": "Videos",
    "text": "Videos\n\n\nVideo\nLaura (wife) decided she had enough of me\n\n\n\n\nVideo\nI decided to slide too\n\n\n\n\nVideo\nBad attempt to a front flip\n\n\n\n\nVideo\nBad attempt to stay bare feet in the snow\n\n\n\n\nVideo\nWoods with snow\n\n\n\n\nVideo\nBob sleading first attempt\n\n\n\n\nVideo\nLaura’s first time on a bob\n\n\n\n\nVideo\nA steeper but better bob ride"
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html",
    "href": "exploring/waihaha_2023_09_04.html",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "",
    "text": "Location: Pureora forest\nDate: from 2023/09/02 to 2023/09/04\nThree of us finally managed to squeeze two nights out for an end-of-winter camping trip. We headed to the Pureora Forest Park to walk the Waihāhā hut Track. The track follows the Waihāhā River and it offers a nice and easy walk across a variety of vegetation.\nWe spent the night at the Waihāhā hut, and the next day, we made our way back to the car. We decided to take the day slow, visiting Kinloch and then relaxing at the Wairakei Terraces.\nGiven that the weather was nice and would have remained clear until the next afternoon, we decided to camp out for another night. We headed to the Kakaho Campsite. The site was desert, we were the only one there. The night was clear, and we got treated to an amazing starry sky while trying to keep warm next to a campfire.\nAfter a quite cold night, we packed up and completed the short Rimu Track Loop within the Pureora Forest."
  },
  {
    "objectID": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "href": "exploring/waihaha_2023_09_04.html#notes-for-the-future",
    "title": "Waihāhā hut and Kakaho Campsite",
    "section": "Notes for the future",
    "text": "Notes for the future\n\nFor winterish camping, source an Italian Army Blanket (or similar) or bring a hot water bottle. The night was fine, but it would have been nicer to have an extra source of warmth.\nInstant polenta is a great camping food. It packs compact and it’s filling. It might be better than pasta.\nNeed to learn some handy knots"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniele Scanzi",
    "section": "",
    "text": "I spend my day looking at squiggly lines produced by your brain. I would love to know how a bat sees the world. One day I ate five razorblades in front of 400 people."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html",
    "href": "posts/2022-10-03-k-means-clustering/index.html",
    "title": "K means clustering",
    "section": "",
    "text": "K-means is a class of unsupervised clustering algorithms that was developed within the field of signal processing. Given a set of data points \\(X={x_{1}, x_{2}, x_{3}, ..., x_{N}}\\), the aim is to find k clusters of points so that the Euclidean mean of the points within each cluster is minimised. Conversely, the distance between clusters is maximised.\nAlthough there are many different k-means algorithms, the general procedure is the following:\n\nDefine the number of clusters (k)\nInitialise the center point (centroid) for each cluster\nCompute the distance between each point and every centroid\nAssign points to the cluster whose centroid is minimally distant\nUpdate the centroid location\nRepeat assignment and updates until the difference between iterations in negligible\n\nFor this simulation we are going to use the data provided here\nThe dataframe looks like this.\n\n\n# A tibble: 6 × 2\n       x     y\n   <dbl> <dbl>\n1 24.4   32.9 \n2 35.2   12.2 \n3 26.3   41.7 \n4  0.376 15.5 \n5 26.1    3.96\n6 25.9   31.5"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#define-the-number-of-clusters",
    "title": "K means clustering",
    "section": "Define the number of clusters",
    "text": "Define the number of clusters\nThe number of clusters depends on the specific application. For instance, in EEG microstate analysis one common practice is to define the use of 4 clusters, which are descriptively called A, B, C, and D. However, note that defining a number a priori is a drawback of this technique. Ideally we would like to find a value that allows explaining the greatest proportion of variability in the data (without assigning each data point to a different group). Consequently, forcing the use of 4 - or any other number - of clusters might be a suboptimal option. We are going to discuss this a bit more later on. For the moment let’s be sneaky and look at the data.\nOur data contains X and Y coordinates for 60 points which are so distributed:\n\n\n\n\n\nFrom the plot we can reasonably say that there are 3 clusters, so we are going to work with that.\n\n#Define number of clusters (k)\nk <- 3"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialise-centroids",
    "title": "K means clustering",
    "section": "Initialise centroids",
    "text": "Initialise centroids\nThe first step of this algorithm is to select the k points that are stereotypes of the clusters. In other words, points that are representative of the groups we want to create. These points are called centroids or prototypes. Obviously, we do not know what groups we will end up with, so the simplest way to select the first centroids is to pick k points at random. Let’s define a function that does exactly this.\n\n# Define function that select k centroids from a dataset.By default the function will select 2 centroids if no k is provided\n\npick_centroids <- function(data, k, seed=1234){\n  # Randomly select k rows from the dataset provided\n  set.seed(seed)\n  centroids <- data[sample(nrow(data),k), ]\n  # Add a unique letter label\n  centroids <- cbind(centroids, 'label'=LETTERS[1:k])\n  return(centroids)\n}\n\nWe can now pick 3 random centroids and visualize them.\n\n# Select first centroids\ncentroids_1 <- pick_centroids(df, k=3, seed=19)\n\n# Visualise them\ndf %>% \n  ggplot(aes(x=x, y=y)) +\n  geom_point(size=2, alpha=0.5, colour='gray') +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=5, shape=15) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "href": "posts/2022-10-03-k-means-clustering/index.html#compute-the-distance-between-each-point-and-each-cluster",
    "title": "K means clustering",
    "section": "Compute the distance between each point and each cluster",
    "text": "Compute the distance between each point and each cluster\nOnce the first centroids have been selected, we can start to divide all the other points into the corresponding clusters. Each point will be assigned to the cluster represented by the centroid that is its closest geometrically. To do so, we need to compute the Euclidean distance between every point and every centroid. Then, we select the minimum distance and assign the point to that centroid’s group. The Euclidean formula is:\n\\[ \\bar{A,B} = \\sqrt{(x_{A} - x_{B})^{2} + (y_{A} - y_{B})^{2}} \\] The following function returns two pieces of information for each point. Firstly, the assigned group as defined by the minimum Euclidean distance from the corresponding centroid. Secondly, an “error’ value defined as the distance between the point and its closest centroid. We will use this error to set up a stopping rule for our k-means algorithm later on.\n\n# Define function to compute the Euclidean distance\neuclidean_distance <- function(data, centroid){\n  distance <- sapply(1:nrow(data), function(i){\n    sqrt(sum((data[i,] - centroid)^2))\n  })\n  return(distance)\n}\n\n\n# Define a function that applies the euclidean distance to each point and returns the minimum \n# Note that this function presupposes that the centroids have a x and y coordinates columns\nfind_min_distance <- function(data, centroids, c_coord){\n  # Firstly we compute the distance between each point and each centroid\n  distances <- sapply(1:nrow(centroids), function(i){\n    euclidean_distance(data, centroids[i, c_coord])\n  })\n  \n  # For each point let's find the centroid with the minimum distance\n  min_idx <- apply(distances, 1, which.min)\n  \n  # We also extract the minimum distance so we can return it\n  min_distance <- apply(distances, 1, FUN = min)\n  \n  # Extract the associated labels\n  min_labels <- sapply(1:length(min_idx), function(i){\n    centroids$label[min_idx[i]]\n  })\n  \n  return(list('error'=min_distance, 'labels'=min_labels))\n}\n\nNow we can apply this to every point in our dataset.\n\nfirst_iter <- find_min_distance(df, centroids_1, c('x', 'y'))\n\n# Let's plot this\ncbind(df, 'label' = first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour = label)) +\n  geom_point(size=2, alpha=0.7) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThat looks like a good starting point, although the B group dominates most of the data. To improve the categorisation, we can build from here by repeating this process over and over. Each time we will select new centroids and assign the points to the group represented by the closest centroid. We do this until there are no more significant changes in the groups."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "href": "posts/2022-10-03-k-means-clustering/index.html#update-centroids",
    "title": "K means clustering",
    "section": "Update centroids",
    "text": "Update centroids\nAfter one iteration, we need to update the centroids. A simple way to do this is by computing the mean coordinate values for each group. The new centroids will be defined by these mean coordinates.\n\nupdate_centroids <- function(df, labels){\n  new <- cbind(df, 'label' = labels) %>% \n    group_by(label) %>% \n    summarise(x = mean(x), \n              y = mean(y)) %>% \n    relocate(label)\n    \n  return(new)\n}\n\n# Compute new centroids\ncentroids_2 <- update_centroids(df, first_iter$labels)\n\n# Plot old and new centroids\ncbind(df, 'label'=first_iter$labels) %>% \n  ggplot(aes(x=x, y=y, colour=label)) +\n  geom_point(size=2, alpha=0.5) +\n  geom_point(data=centroids_1, aes(x=x, y=y, colour=label), size=3, shape=15) +\n  geom_point(data=centroids_2, aes(x=x, y=y, colour=label), size=5, shape=4) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe squares represent the original centroids, the Xs represent the new ones, and the points are still coloured according to their original categorisation. Notice how the blue centroid is now in the centre of its group."
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "href": "posts/2022-10-03-k-means-clustering/index.html#reiterate",
    "title": "K means clustering",
    "section": "Reiterate",
    "text": "Reiterate\nWe are ready to reiterate the update and assignation process N times. As said above, we will stop when there are no more significant differences between one categorisation and the next one. To quantify this, we will use the error value we introduce just before. For each point, this is represented by the distance of the point from its closest centroid. Thus, we can sum these error values and use this sum as the stopping rule. When the sum of errors is reduced below a predetermined threshold, then we can stop.\n\nmy_kmeans <- function(data, k=2, c_coord= c('x', 'y'), tolerance=1e-4, seed=1234){\n  # Firstly we find the first centroids\n  current_centroids <- pick_centroids(data, k=k, seed=seed)\n  \n  # Create datasets were to store results\n  labelling <- c()\n  centroids <- current_centroids\n  \n  # Reiterate labelling - assignment - update centroids\n  continue <- TRUE\n  iter <- 0\n  previous_error <- 0\n  while(continue){\n    \n    # Assign data to centroids\n    current_groups <- find_min_distance(data, current_centroids, c_coord)\n    \n    # Store assigned labels with column name as the iteration number\n    iter <- iter + 1\n    labelling <- cbind(labelling, current_groups$labels)\n    \n    # Update centroids\n    current_centroids <- update_centroids(data, current_groups$labels)\n    centroids <- rbind(centroids, current_centroids)\n    \n    # Check if we have minimizes the error below the threshold\n    current_error <- sum(current_groups$error)\n    current_err_diff <- abs(previous_error - current_error)\n    print(sprintf('Iteration %s -> Error: %s', iter, current_err_diff))\n    if(current_err_diff <= tolerance){\n      continue = FALSE\n    }\n    # If we did not reach the tolerance, update the current error\n    previous_error <- current_error\n    \n  }\n  colnames(labelling) <- 1:iter\n  # remove last centroid data as it has not been used and assign iter values\n  centroids <- centroids[1:(nrow(centroids)-k), ]\n  centroids <- cbind(centroids, 'iter'=rep(1:iter, each=k))\n  return(list('lables'=labelling, 'centroids'=centroids, 'error'=current_groups$error))\n}\n\nLet’s iterate on our data.\n\nresults1 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=4)\n\n[1] \"Iteration 1 -> Error: 458.171667198993\"\n[1] \"Iteration 2 -> Error: 72.4112246075529\"\n[1] \"Iteration 3 -> Error: 0.655622083297658\"\n[1] \"Iteration 4 -> Error: 0\"\n\n\nSweet, for this particular case the algorithm converged in 4 iterations. Let’s see the final result.\n\ncbind(df, 'group'=results1$lables[, ncol(results1$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThis looks all good! Yay!"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "href": "posts/2022-10-03-k-means-clustering/index.html#initialization-problems",
    "title": "K means clustering",
    "section": "Initialization problems",
    "text": "Initialization problems\nThe previous result might make you think that this algorithm is amazing. It categorised our data in just four iterations and with a perfect division. However, things are always more complicated than they initially appear. Indeed, a drawback of the k-means approach is that the final result is highly dependent on the initial centroids. We can demonstrate this by starting the algorithm with different initial centroids. We will exploit the seed argument we provided to our functions.\n\nresults2 <- my_kmeans(df, k=3, c_coord=c('x', 'y'), tolerance=1e-4, seed=19)\n\n[1] \"Iteration 1 -> Error: 1278.04142581076\"\n[1] \"Iteration 2 -> Error: 549.175741899334\"\n[1] \"Iteration 3 -> Error: 120.573809438913\"\n[1] \"Iteration 4 -> Error: 7.0347771777997\"\n[1] \"Iteration 5 -> Error: 10.0949551678153\"\n[1] \"Iteration 6 -> Error: 5.53498901037312\"\n[1] \"Iteration 7 -> Error: 4.606053850374\"\n[1] \"Iteration 8 -> Error: 1.49594562166419\"\n[1] \"Iteration 9 -> Error: 2.36620189065968\"\n[1] \"Iteration 10 -> Error: 0\"\n\ncbind(df, 'group'=results2$lables[, ncol(results2$lables)]) %>%\n  ggplot(aes(x=x, y=y, colour = group)) +\n  geom_point(size=2, alpha=.5) +\n  geom_point(data=results2$centroids %>% filter(iter==max(iter)), aes(x=x, y=y, colour=label), shape=15, size=5) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nHere the algorithm converged to a suboptimal solution. During the years different solutions have been created to address this problem, with the most popular and reliable being the kmeans++ algorithm created by David Arthur and Sergei Vassilvitskii. If you are interested, the procedure is presented here"
  },
  {
    "objectID": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "href": "posts/2022-10-03-k-means-clustering/index.html#how-many-clusters",
    "title": "K means clustering",
    "section": "How many clusters?",
    "text": "How many clusters?\nAs stated in the introduction, one obvious limitation of this paradigm is that the number of clusters needs to be defined a priori. Thus, we need a system that would allow us to select the optimal number of clusters that reduces the classification error as much as possible without “overfitting”. One simple method to do so is to run the algorithm with different number of clusters and use the scree plot of the error as a guide. To explain this let’s change dataset and pick something with a greater number of observations and a less clear number of clusters. We will use the iris dataset provided in R. Our task is to cluster the flowers into species based on the sepal length and the petal width.\nIf we look at the raw data we can see two possible groups, but now the situation is more complex than before.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nTo establish the optimal amount of clusters, we are going to run our k-means algorithm 10 times adding one cluster at each iteration. Each time we will store the final error, so we can plot it later. Here’s the code:\n\nks <- 1:10\nerrors <- rep(0, length(ks))\niris_df <- iris[, c('Sepal.Length', 'Petal.Width')]\ncolnames(iris_df) <- c('x', 'y')\nfor(r in ks){\n  errors[r] <- sum(my_kmeans(iris_df, k=r)$error)\n}\n\nNow we can create the scree plot by visualising the final error for each iteration.\n\n# Make scree plot of the errors\ndata.frame('error'=errors, 'k'=1:length(errors)) %>% \n  ggplot(aes(x=k, y=error)) +\n  geom_point(size=2) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:length(errors)) +\n  theme_minimal()\n\n\n\n\nThe scree plot is a “descriptive tools” so it won’t tell specifically the correct number of clusters. The main idea here is to look at the elbow of the plot, that is the point at which the trend plateau. This flexion point indicates the value after which increasing the number of groups does not provide a significant decrease in the error. Looking at the plot we can say that the elbow is in between k=3 or k=4. As a starting point we can visualise both of them:\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3)\n\n[1] \"Iteration 1 -> Error: 84.5340503544893\"\n[1] \"Iteration 2 -> Error: 22.5320451284015\"\n[1] \"Iteration 3 -> Error: 0.12222997427822\"\n[1] \"Iteration 4 -> Error: 0.194408399479336\"\n[1] \"Iteration 5 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4)\n\n[1] \"Iteration 1 -> Error: 67.9944121319274\"\n[1] \"Iteration 2 -> Error: 9.13499108999842\"\n[1] \"Iteration 3 -> Error: 1.30457150911509\"\n[1] \"Iteration 4 -> Error: 0.932100341554694\"\n[1] \"Iteration 5 -> Error: 1.0714446191438\"\n[1] \"Iteration 6 -> Error: 0.716841774592375\"\n[1] \"Iteration 7 -> Error: 0.675081843250041\"\n[1] \"Iteration 8 -> Error: 0.318333061193755\"\n[1] \"Iteration 9 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nThe groups are very similar, or at least there are no weird differences. This is a good thing. To better investigate possible difference between the two clustering systems, we could re-run them with a different seed and see if they are consistent (although we have discussed the implications that the first initialization can have).\n\n# Compute kmeans with 3 nd 4 clusters clusters\niris_k3 <- my_kmeans(iris_df, k = 3, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 99.0311396766634\"\n[1] \"Iteration 2 -> Error: 24.4105692091305\"\n[1] \"Iteration 3 -> Error: 4.88652621704111\"\n[1] \"Iteration 4 -> Error: 3.47693331688762\"\n[1] \"Iteration 5 -> Error: 2.56320926311736\"\n[1] \"Iteration 6 -> Error: 1.62704674573919\"\n[1] \"Iteration 7 -> Error: 0.0724143145959744\"\n[1] \"Iteration 8 -> Error: 0.057240636048725\"\n[1] \"Iteration 9 -> Error: 0.0225024050884883\"\n[1] \"Iteration 10 -> Error: 0\"\n\niris_k4 <- my_kmeans(iris_df, k = 4, seed = 0905)\n\n[1] \"Iteration 1 -> Error: 97.5708407922748\"\n[1] \"Iteration 2 -> Error: 29.7874124595873\"\n[1] \"Iteration 3 -> Error: 7.83693472432838\"\n[1] \"Iteration 4 -> Error: 2.47590455673429\"\n[1] \"Iteration 5 -> Error: 0.153319685392383\"\n[1] \"Iteration 6 -> Error: 0.0496830708143321\"\n[1] \"Iteration 7 -> Error: 0.0799826359733871\"\n[1] \"Iteration 8 -> Error: 0\"\n\n# Add clustering information resulting from k=3 and k=4 and plot\niris_mykmean <- cbind(iris_df, \n      'k3' = iris_k3$lables[, ncol(iris_k3$lables)],\n      'k4' = iris_k4$lables[, ncol(iris_k4$lables)]) %>% \n  pivot_longer(cols = starts_with('k'),\n               values_to = 'labels',\n               names_to = 'k')\n  \niris_mykmean %>% \n  ggplot(aes(x=x, y=y, colour=labels)) +\n  geom_point(size=2) +\n  labs(x = 'Length',\n       y = 'Width') +\n  facet_grid(.~k) +\n  theme_minimal()\n\n\n\n\nNote that the colours can change between iterations. What we are interested in are the patterns of groups. The results are interesting. k=3 produced the same results as before. Conversely, k=4 did something different; it divided the lower cluster of points into two groups. Although this is not a formal assessment, from here we might want to say that k=3 is more reliable and go with it. As a final test, let’s compare these results agains the real division by species.\n\niris %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour=Species)) +\n  geom_point(size=2) + \n  theme_minimal()\n\n\n\n\nLet’s hihglight the differences:\n\n# Let's assign a species label to to A, B, D clusters\nspecies <- unique(iris$Species)\n\n# Merge datasets and code whether there groups are the same\niris_binded <- cbind(iris, 'my_kmeans' = iris_k3$lables[, ncol(iris_k3$lables)]) %>% \n  mutate(\n    my_species = case_when(\n      my_kmeans == 'A' ~ species[3],\n      my_kmeans == 'B' ~ species[1], \n      TRUE ~ species[2]),\n    equal = case_when(my_species != Species ~ 0,\n                      TRUE ~ 1))\n\n# Plot highlighting difference\niris_binded %>% \n  ggplot(aes(x=Sepal.Length, y=Petal.Width, colour = factor(equal))) +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c('red', 'lightgray'), name = 'Difference', labels = c('different', 'equal')) +\n  labs(x = 'Length',\n       y = 'Width') +\n  theme_minimal()\n\n\n\n\nThe red dots are the observations that our kmeans algorithm categorised differently compared to the original division into species. Overall, the result can be improved but it’s not too bad considering that the majority of points were correctly identified."
  },
  {
    "objectID": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "href": "posts/2023-06-08-competence-vs-competitiveness/index.html",
    "title": "You don’t need to be competitive if you know you’re competent",
    "section": "",
    "text": "In the past two months, I’ve been obsessed with the new album of Pinguini Tattici Nucleari. Every song deals with a different theme in a powerful way. I cried multiple times to Ricordi and its earthly depiction of dealing with a parent suffering from Alzheimer’s. I reflect on the concept of faith with Fede and the Italian socio-political situation with Coca Zero. However, Zen is the single I go back to over and over again.\nThe reason for this is simple. It describes something I often feel, being often worried about the future and looking for a moment of Zen. But one sentence got stuck in my mind, and it has nothing to do with my psychological state and everything with academia.\n\n Essere competitivo non serve se sai di еssere competеnte\n\n\nWhich translates to:\n\nYou don’t need to be competitive if you know you’re competent\n\n\nIt’s just one sentence, a simple one. Still, it captures so well the academic environment, where people feel the need to show their knowledge, achievement and prizes just for the sake of establishing their position. Something exacerbated by the Universities themselves, putting people against each to obtain one position (often underpaid), a breadcrumb of notoriety in the field. So much so that we are still here having well-established academics that have to act as the “bad cop” in recruitment panels for the sake of it. Or you need to fight for years for an opportunity to become a clinical psychologist because, you know, we need psychologists, but only if they are elitists.\nBut do I need to take part in this? Do I need to play the game?\nNo, I don’t. I know my worth, I know my strengths and weaknesses. I don’t want to be competitive. It’s useless. It’s worthless. It’s damaging to others.\nA simple song, a verse sentence. Yet so much to think about."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "You don’t need to be competitive if you know you’re competent\n\n\n\n\n\n\n\nopinions\n\n\nacademia\n\n\n\n\nMusical reflections on academic competitiveness\n\n\n\n\n\n\nJune 8, 2023\n\n\nDaniele Scanzi\n\n\n\n\n\n\n  \n\n\n\n\nK means clustering\n\n\n\n\n\n\n\nR\n\n\ncoding\n\n\nmicrostates\n\n\n\n\nK means algortihm walk through presented at the lab meeting to explain this technique.\n\n\n\n\n\n\nOctober 3, 2022\n\n\nDaniele Scanzi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/bayesian_stat_material/bayes_intro.html",
    "href": "projects/bayesian_stat_material/bayes_intro.html",
    "title": "Overview",
    "section": "",
    "text": "A student-led group to learn Bayesian statistics - For students by students\n\n\nThis project was initially born when a friend of mine, Dylan Taylor (PhD student at the University of Auckland), and I wanted to meet over lunch to work through the book Statistical Rethinking. Talking with other PhD students, we soon realised that many researchers are interested in applying bayesian stats. However, most students didn’t have the opportunity to learn this and feel like learning it alone can be too difficult and time-consuming - a feeling we share too. So, we expanded the group to other interested people, and we began a series of weekly meetups. During these, we go through the 2022 lecture series accompanying the book, we have discussions and work through the exercises together. The group has expanded to around 15 people and has become a great opportunity to learn something useful together and create new connections between like-minded people.\nIn this section, you can find the slides and material we create to provide direction during the meetups.\n\nThis project has now been awarded a Creating Connections Grant which will help to provide refreshment during the meetups"
  },
  {
    "objectID": "projects/bayesian_stat_material/slides_list.html",
    "href": "projects/bayesian_stat_material/slides_list.html",
    "title": "Slides",
    "section": "",
    "text": "Here you can find the link to the slides openly available on RPubs.\n\nIntroduction to Bayesian Stat\nBayes Theorem\nBayesian Engines"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html",
    "title": "EEG preprocessing functions",
    "section": "",
    "text": "A collection of functions and code-snippets useful for EEG preprocessing with EEGLAB (Matlab). Note, I distribute these functions under the GNU General Public License. I removed the statement to slim the code and the page. However, if you use or modify any of these functions in your projects, please add the GNU licence statement at the end of the function help. You can find the statement at the end of this page."
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#counting-triggers",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#counting-triggers",
    "title": "EEG preprocessing functions",
    "section": "Counting triggers",
    "text": "Counting triggers\nThis function takes an EEGLAB EEG structure and returns a cell array containing the number of triggers (aka events) divided for each trigger type. I use it as a sanity check to ensure that my data does not contain missing triggers and that the participant completed the correct number of trials/blocks.\n% nTrig = count_triggers(EEG)\n%\n%    Count the number of unique triggers contained in the EEG.event \n%    structure.\n%\n% In:\n%    EEG   - An EEGLAB data structure\n%\n% Out:\n%    nTrig - Cell array containing the unique trigger names and how many\n%            times they occur in the recording.\n\n% Author: Daniele Scanzi\n\nfunction nTrig = count_triggers(EEG)\n\n    % Check that event structure is in EEG structure\n    if ~isfield(EEG, \"event\")\n        error(\"Cannot find event structure. Is EEG an EEGLAB structure?\")\n    end\n    \n    % Find unique events\n    nTrig = unique({EEG.event.type})';\n    if isempty(nTrig)\n        error(\"Cannot find any events in the event structure. Do you need to load them?\")\n    end\n\n    % Create cell array storing name of event and their number\n    for iEvent = 1:size(nTrig,1)\n        nTrig{iEvent, 2} = sum(strcmp({EEG.event.type}, nTrig{iEvent, 1}));\n    end\n\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#add-extra-trigger-information",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#add-extra-trigger-information",
    "title": "EEG preprocessing functions",
    "section": "Add extra trigger information",
    "text": "Add extra trigger information\nThis function is useful if you have triggers represented as codes (eg. T100) and you would like to add their meaning in the EEG.event structure. However, you can add any other information.\n% EEG = add_trigger_info(EEG, trigInfo, varargin)\n%\n%    Add extra information to the EEG.event structure. It is useful to add\n%    trigger information for each trigger. For instance, you can add the\n%    meaning of each trigger if trigger codes have been used.\n%\n% In:\n%    EEG      - An EEGLAB data structure\n%    trigInfo - Cell array of size NxM. N should reflect the number of\n%               triggers for which extra information should be added. M\n%               should reflect the nuber of extra information to add. M new\n%               columns will be added to the EEG.event structure. M must be\n%               at least 2, where the first column should contain the\n%               names of the triggers (one for each row) and the second\n%               column contains the information to add. For instance, if\n%               you have three triggers and you want to add their meaning,\n%               you could pass a cell array such as:\n%               {'T100' 'start_recording'; \n%                'T150' 'pause_recording';\n%                'T200' 'stop_recording'}\n%\n% Optional:\n%     fieldName - Cell array containing the names of the new columns to add\n%                 to the EEG.event structure. The cell array should have\n%                 size of 1xM, where M is the number of the extra\n%                 information columns to add. M must match the number of\n%                 columns of trigInfo. If no names are provided, the new\n%                 columns will be named 'X1', 'X2',..., 'XM'\n%\n% Out:\n%    EEG        - Modified copy of the EEG structure\n\n% Author: Daniele Scanzi\n\nfunction EEG = add_trigger_info(EEG, trigInfo, varargin)\n\n    p = inputParser;\n    \n    % Mandatory\n    addRequired(p, 'EEG', @isstruct);     % EEG structure\n    addRequired(p, 'trigInfo', @iscell);  % Cell array containing tirgger info\n    \n    % Optional\n    addParameter(p, 'fieldName', [], @iscell);         % Name/s to provide to the fields to add\n    \n    % Parse user input\n    parse(p, EEG, trigInfo, varargin{:});\n\n    EEG = p.Results.EEG;\n    triggInfo   = p.Results.trigInfo;\n    fieldName   = p.Results.fieldName;\n\n    %% Checks\n   \n   if size(trigInfo, 2) <= 1\n       error('triggerInfo should be a cell array with at least two columns: Trigger names - New field1 \\n');\n   end\n\n   % Create names for field if not provided, check otherwise\n   if isempty(fieldName)\n       fieldName = cell(size(trigInfo, 2) - 1, 1);\n       for iField = 1:( size(trigInfo, 2) - 1 )\n           fieldName{iField, 1} = strcat('X', num2str(iField));\n       end\n   else\n       if length(fieldName) ~= ( size(trigInfo, 2) - 1 )\n           error('fieldNames should match the number of new fields contained in trigInfo \\n');\n       end\n   end\n   \n   for iEvent = 1:length(EEG.event)\n       currentEvent       = EEG.event(iEvent).type;\n       currentTrigInfoIdx = find(strcmp(trigInfo(:, 1), currentEvent));\n\n       % Add info\n       for iInfo = 1:length(fieldName)\n           if isempty(currentTrigInfoIdx)\n               EEG.event(iEvent).(fieldName{iInfo}) = 'n/a';\n           else\n               EEG.event(iEvent).(fieldName{iInfo}) = trigInfo{currentTrigInfoIdx, iInfo+1};\n           end\n       end\n   end\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#compute-latencies-with-photosensor",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#compute-latencies-with-photosensor",
    "title": "EEG preprocessing functions",
    "section": "Compute latencies with photosensor",
    "text": "Compute latencies with photosensor\nIn our current EEG setup, we have a Brainproducts system where triggers are sent through a triggerbox. We also have a photosensor, which allows us to detect with precision when a stimulus is presented on the screen. Unfortunately, we do not have the StimTrack used to convert the photosensor data into triggers. So, we need to do this ourselves.\nThe photosensor data is stored as an extra channel in the EEG data. If the photosensor is set up to detect a switch from a dark to a light stimulus, then it creates a trace with a spike (NOTE: not a square wave) when the switch happens. The spike begins at the onset of the light stimulus and peaks just after that.\nI did not test this function with other systems, but I would assume it would work (grating that the photosensor records luminance and not just the change in luminance).\nBriefly, this function does the following: 1. Detect the photosensor spikes onsets 2. Add triggers at the detected onset latencies 3. For each trigger, check whether there is an associated photosensor trigger, accounting for the fact that the photosensor could appear before or after the associated trigger.\nThe order of photosensor-trigger seems to depend on how the experiment is coded and reflects whether the trigger lags behind the stimulus or not. I’m still discussing this point with friends, but we don’t have a conclusive explanation for this. Any insight on this is welcomed.\n% [eventLatenciesInfo, EEG] = trigger_photo_latency(EEG, eventNames, varargin)\n%\n%         Align the recorded triggers to the onset times recorded by\n%         Brainproducts' photosensor. Conversion follows the procedure:\n%         1. Extract data from photosensor\n%         2. Find onset of spikes - spikes reflect changes in contrast\n%         3. Add found onsets to EEG.event structure in chronological order\n%         4. For eah trigger, find closest spike (within defined tolerance)\n%         5. Convert the original onset time of each trigger to its\n%            assciated spike onset time\n%\n% In:\n%    EEG        - An EEGLAB data structure\n%    eventNames - Cell array of strings represenitng the names of the\n%                 triggers to align\n%\n% Optional:\n%    photoTrigger       - Name to provide to the triggers representing the\n%                         photosensor event onsets. Default: 'photo'\n%    photoChannel       - Channel number (int) or channel name (string) of \n%                         the channel containing the photosensor data\n%    normalisePhotoData - Logical (true|false). Whether to normalise the \n%                         photosensor data prior to find the peaks. If true, \n%                         data is normalised in range [0,1]. Default: true\n%\n%    peakHeightThresh   - Minimum peak height for a photosensor spike to be\n%                         considered an event. This value is empirical and\n%                         it depends on multiple factors (eg. colour of the\n%                         photosensor stimulus). Can be useful to evoid \n%                         considering events changes in diplay contrast not\n%                         associated with the experiment (eg. experiment \n%                         window opening). Deafult: 0\n%    missedTrigTresh    - Maximum acceptable lag (in ms) between recorded \n%                         trigger and photosensor spike. If the lag is\n%                         higher than thism a warning is produced. This\n%                         often occurs if you try to align a trigger that\n%                         does not have an associated photosensor.\n%    modifyOriginal     - Logical (true|false). Whether to return a modified\n%                         copy of the EEG structure containin the\n%                         re-aligned events. Default: true\n%\n% Out:\n%    eventLatenciesInfo - Cell array containing delays information for each\n%                         trigger included in eventNames. Each row\n%                         represents a different trigger. Columns\n%                         represent: \n%                         1. Trigger name\n%                         2. Trigger idx in modified EEG.event structure\n%                         3. Array of delays (ms) for each trigger\n%                         4. Average delay (ms) for each trigger\n%    EEG                - Modified copy of the EEG structure where the \n%                         EEG.event structure now contains:\n%                         1. Photosensor events as triggers\n%                         2. Onset of triggers alligned with the\n%                            photosensor events\n\n% Author: Daniele Scanzi\n\nfunction [eventLatenciesInfo, EEG] = trigger_photo_latency(EEG, eventNames, varargin)\n    \n    % INPUTS\n    p = inputParser;\n    \n    % Mandatory\n    addRequired(p, 'EEG', @isstruct);      % EEG structure with EEG data\n    addRequired(p, 'eventNames', @iscellstr); % Signal matrix\n    \n    % Optional\n    addParameter(p, 'photoTrigger', 'photo', @ischar);       % Name of the trigger to add\n    addParameter(p, 'photoChannel', 64);                     % Channel containing the photosensor data\n    addParameter(p, 'normalisePhotoData', true, @islogical); % Whether to normalise the data or not\n    addParameter(p, 'peakHeightThresh', 0, @isnumeric);      % Minimum height to consider something a peak\n    addParameter(p, 'missedTrigTresh', 16, @isnumeric);      % Max delay (in ms) acceptable\n    addParameter(p, 'modifyOriginal', true, @islogical);     % Whether to modify the original EEG structure or not\n    \n    parse(p, EEG, eventNames, varargin{:});\n    \n    EEG                = p.Results.EEG;\n    eventNames         = p.Results.eventNames;\n    photoTrigger       = p.Results.photoTrigger;\n    photoChannel       = p.Results.photoChannel;\n    normalisePhotoData = p.Results.normalisePhotoData;\n    peakHeightThresh   = p.Results.peakHeightThresh;\n    missedTrigTresh    = p.Results.missedTrigTresh;\n    modifyOriginal     = p.Results.modifyOriginal;\n\n    %% MAIN FUNCTION\n\n    % Check that event structure is in EEG structure\n    if ~isfield(EEG, \"event\")\n        error(\"Cannot find event structure. Is EEG an EEGLAB structure?\")\n    end\n\n    % Check that provided events exist\n    for iEvent = 1:length(eventNames)\n        if ~any(strcmp({EEG.event.type}, eventNames{iEvent}))\n            error(\"Cannot find %s in EEG.event structure\", eventNames{iEvent})\n        end\n    end\n\n    % Check that data contains the channel requested\n    if isinteger(photoChannel)\n        if ~size(EEG.data, 1) < photoChannel\n            error(\"Channel %i out of bound of data size %i\", photoChannel, size(EEG.data, 1))\n        end\n    elseif ischar(photoChannel) || isstring(photoChannel)\n        if ~any(strcmp({EEG.chanlocs.labels}, photoChannel))\n            error(\"Channel %s not found\", photoChannel)\n        else\n            % find channel number and overwrite string\n            photoChannel = find(strcmp({EEG.chanlocs.labels}, photoChannel));\n        end\n    end\n\n    % The function should run even with epoched data, but this has not been\n    % tested yet. There are no many reasons for using this function with\n    % epoched data anyway\n    if ndims(EEG.data) > 2\n        warning(\"Function not tested with epoched data (or data with more \" + ...\n            \"than two dimensions in general\" )\n    end\n\n    % Extract photsensor data (account for possibility of data being\n    % epoched). Correct peaks so to find photosensor onset\n    photoData = EEG.data(photoChannel, :, :);\n    photoData = diff(photoData);\n    \n    if normalisePhotoData\n        photoData = normalize(photoData, 'range');\n    end\n\n    % Find peaKs\n    [~, peaksLocs] = findpeaks(photoData, 'MinPeakHeight', peakHeightThresh);\n\n    % Add peaks to the event structure data\n    fprintf(\"Adding photosensor events to EEG.event structure\")\n    % Add peaks to chanloc values\n    for iPeak = 1:length(peaksLocs)\n        EEG.event(end+1).latency = peaksLocs(iPeak);\n        EEG.event(end).type = photoTrigger;\n    end\n\n    % Reorder events by latency\n    EEG = eeg_checkset(EEG,'eventconsistency');\n\n    % First find triggers indices\n    eventLatenciesInfo = {length(eventNames), 4};\n    for iEvent = 1:length(eventNames)\n        eventLatenciesInfo{iEvent, 1} = eventNames{iEvent};\n        eventLatenciesInfo{iEvent, 2} = find(strcmp({EEG.event.type}, eventNames{iEvent}));\n    end\n    \n    % Compute event latencies finding the nearest photosensor event\n    for iEvent = 1:size(eventLatenciesInfo, 1)\n        % create arry to store latencies\n        currentLatencies = nan(length(eventLatenciesInfo{iEvent, 2}), 1);\n        for iTrig = 1:length(eventLatenciesInfo{iEvent, 2})\n\n            % Check previous event\n            previousTrigger = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)-1);\n            nextTrigger     = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)+1);\n            if strcmp({previousTrigger.type}, photoTrigger)\n                previousLatencyDiff = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency - EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)-1).latency;\n            else\n                % if there is no photosensor trigger before, set this diff\n                % tp -Inf\n                previousLatencyDiff = nan;\n            end\n\n            % Check next event\n            if strcmp({nextTrigger.type}, photoTrigger)\n                nextLatencyDiff = EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)+1).latency - EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency;\n            else\n                % if there is no photosensor trigger before, set this diff\n                % tp -Inf\n                nextLatencyDiff = nan;\n            end\n\n            % Find minimum between the two\n            [currentMin, minIdx] = min([previousLatencyDiff nextLatencyDiff]);\n            if  currentMin > missedTrigTresh\n                warning(\"Possible missing triggers around trigger number %i. Event not included in latency calculation \\n\", eventLatenciesInfo{iEvent, 2}(iTrig));\n                currentLatencies(iTrig) = nan;\n            else\n                currentLatencies(iTrig) = currentMin;\n            end\n            \n            % Modify the original dataset event structure if requested\n            if modifyOriginal\n                if minIdx == 1\n                    EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency = previousTrigger.latency;\n                elseif minIdx == 2\n                    EEG.event(eventLatenciesInfo{iEvent, 2}(iTrig)).latency = nextTrigger.latency;\n                end\n            end\n\n        end\n\n        % Compute average latency excluding missing triggers\n        currentLatencies = currentLatencies(~isnan(currentLatencies));\n        eventLatenciesInfo{iEvent, 3} = currentLatencies;\n        eventLatenciesInfo{iEvent, 4} = mean(currentLatencies);\n    end\n    EEG = eeg_checkset(EEG);\nend\n\n\n\n% Helper\nfunction isText(myVar)\n    isstring(myVar) || ischar(myVar);\nend"
  },
  {
    "objectID": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#gnu-statement",
    "href": "projects/eeg_matlab_code/eeg_preprocessing_functions.html#gnu-statement",
    "title": "EEG preprocessing functions",
    "section": "GNU statement",
    "text": "GNU statement\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details (https://www.gnu.org/licenses)."
  },
  {
    "objectID": "projects/projects_intro.html",
    "href": "projects/projects_intro.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a collection of material related to different projects I have been working on, either alone or in collaboration with other people."
  },
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html",
    "href": "projects/psychopy_code/coloured_gabor_patches.html",
    "title": "Psychopy useful functions",
    "section": "",
    "text": "The following document contains a collection of functions created to run experiments in Psychopy."
  },
  {
    "objectID": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "href": "projects/psychopy_code/coloured_gabor_patches.html#creating-coloured-gratings-pseudo-gabor.",
    "title": "Psychopy useful functions",
    "section": "Creating coloured gratings (pseudo-Gabor).",
    "text": "Creating coloured gratings (pseudo-Gabor).\nfrom psychopy import visual, event, monitors, tools\nfrom psychopy.visual import filters\nfrom psychopy.tools import monitorunittools as mut\nimport numpy as np\nimport math\nimport os\n\n###############################################################################\n#                                VARIABLES\n###############################################################################\n\nSAVE_IMAGE = True             #Set to true if you want to output an image\nDIFFERENT_BRIGHTNESS = True   #Should the stimuli have different luminance?\nSWITCH_SIDE = True            #Invert the left and right grating\nWHAT_TO_DRAW = \"right\"        #Either \"both\", \"left\" or \"rigth\"\n# Parameters for the gratings\nparam_stim = {\n    \"resolution\": 5,           #Size of the stimulus grating in deg (this will be coverted in pix later)\n    \"mask_resolution\": 2**11,  #Resolution of the mask used to render the gratings as circles (must be a power of 2)\n    \"ori_left\": 45,            #Orientation of the first grating\n    \"ori_right\": -45,          #Orientation of the second grating \n    \"pos_left\": (0, 0),        #Position of the first grating (0,0 is the center of the screen)\n    \"pos_right\": (0, 0),       #Position of the second grating (0,0 is the center of the screen)\n    \"cycles\": 4*5,             #Spatial frequency of the gratings. This should be resolution X cycles per deg\n    \"vergence_cycles\": 5,      #Spatial frequency of the gratings used to create the vergence patterns\n    \"vergence_sf\": 0.03,       #This value controls the number of gratings used in the vergence patterns (use values < 0.5)\n    \"alpha_left\": 1,\n    \"max_value_first\": -0.3 #Red: Psychopy [-0.0030, -1,-1], RGB [89,0,0], HSV[0,100,35]\n}\n\n# Screen and window parameters - for Psychopy\nparam_pc = {\n    \"resolution\": (1920, 1080),\n    \"width\": 34.2}\n\n# Directories and file names for the image output\nthis_dir = os.path.dirname(os.path.abspath(__file__))\nimage_name = \"red_single_03-1-1.png\"  #Name of the file to output at the end\n\n###############################################################################\n#                                   WINDOW\n###############################################################################\n\n# Create monitor and windows\nmon = monitors.Monitor(\n    name=\"desk_monitor\",\n    width=param_pc[\"width\"],\n    distance=57\n)\nmon.setSizePix = param_pc[\"resolution\"]\n\nwin = visual.Window(\n    size=param_pc[\"resolution\"],\n    monitor=mon,\n    units=\"pix\",\n    allowGUI=False,\n    screen=1,\n    fullscr=False,\n    color=(-1, -1, -1),\n    colorSpace='rgb',\n    blendMode='avg',\n    winType='pyglet',\n    useFBO=True)\n\n\n###############################################################################\n#                                  FROM DEG TO PIX\n###############################################################################\n\n# Convert to pix\nparam_stim[\"resolution\"] = int(mut.deg2pix(param_stim[\"resolution\"], mon))\n# Round pix to the closest power of 2. NOTE this works for \"low\" values but \n# cannot be generalized to high values (eg. 100000). However, here we work with \n# values in the 100 range (eg. 256 pix).\nparam_stim[\"resolution\"] = 2**round(math.log2(param_stim[\"resolution\"]))\n\n###############################################################################\n#                                    STIMULI\n# To create the gratings we start by creating a black texture defined as a \n# matrix of dimension [dim1, dim2, 3], where the three layers represent the RGB\n# colours. Then, we will replace the layer of the colour we are interested in \n# with a grating, which is a [dim1, dim2] array, conatining values from -1 to 1 \n# representing the intensity of the colour. Doing this will create a grating \n# stimulus of the desired colour.\n# For the red stimulusg, we are interested in manipulating its brightness. To do \n# so, we define a colour in HSV space and convert it into RGB (use tool online)\n# Then we modify the grating range, so that it goes from -1 (black) to N, where\n# N is the values obtained online\n###############################################################################\n\n# Switch side if requested\nif SWITCH_SIDE:\n    pos_left  = param_stim[\"pos_left\"]\n    pos_right = param_stim[\"pos_right\"]\n    param_stim[\"pos_left\"] = pos_right\n    param_stim[\"pos_right\"] = pos_left\n    \n# ---Coloured Gabors---#\n\n# Create a black texture for both stimuli...\ngrating_left = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\ngrating_right = np.ones((param_stim[\"resolution\"], param_stim[\"resolution\"], 3)) * -1\n\n# GREEN --> For the green stimulus we simply overaly the grating to the G channel\ngrating_right[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                              ori=param_stim[\"ori_right\"],\n                                              cycles=param_stim[\"cycles\"],\n                                              gratType=\"sin\")\n\n\n\n# RED --> For the red stimulus we need to do some more work...\n\n# Create a grating\nsin_mask = filters.makeGrating(res=param_stim[\"resolution\"],\n                                            ori=param_stim[\"ori_left\"],\n                                            cycles=param_stim[\"cycles\"],\n                                            gratType=\"sin\")\n\n# If different luminance is requested\nif DIFFERENT_BRIGHTNESS:\n    # Scale only positive values to change the colour (RED) but not the black through the Rohan's transform\n    # NOTE: it's not a real transform...it was a tip from a friend\n    scale_factor = 0.5*(param_stim[\"max_value_first\"]+1)\n    sin_mask_scaled = scale_factor * (sin_mask + 1) - 1\n   \n    grating_left[:,:,0] = sin_mask_scaled\n# If no difference in brightness is required, apply the grating as above\nelse:\n    grating_left[:, :, 0] = sin_mask\n\n\n#---Vergence Gratings---#\n\n# Create a gray texture (Psychopy [0,0,0] is gray)...\ngrating_vergence = np.zeros((param_stim[\"resolution\"], param_stim[\"resolution\"], 3))\n\n#...then overimpose a grid on all the three RGB channels\ngrating_vergence[:, :, 0] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 1] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\ngrating_vergence[:, :, 2] = filters.makeGrating(res=param_stim[\"resolution\"],\n                                                cycles=param_stim[\"vergence_cycles\"],\n                                                ori=45,\n                                                gratType='sin')\n\n#---Circle Mask---#\n\n# Generate a nice smooth (at least almost) circle mask\nmask = filters.makeMask(matrixSize=param_stim[\"mask_resolution\"],\n                        shape=\"circle\")\n\n#---Generate Stimuli with Psychopy---#\n\n# left grating stimulus\nstim_left = visual.GratingStim(\n    name=\"stimL\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_left,\n    mask=mask,\n    units=\"pix\")\n# Right grating stimulus\nstim_right = visual.GratingStim(\n    name=\"stimR\",\n    win=win,\n    size=(param_stim[\"resolution\"], param_stim[\"resolution\"]),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_right,\n    mask=mask,\n    units=\"pix\")\n# Left vergence pattern\nvergence_left = visual.GratingStim(\n    name=\"vergL\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_left\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n# Right vergence pattern\nvergence_right = visual.GratingStim(\n    name=\"vergR\",\n    win=win,\n    size=(param_stim[\"resolution\"]+50, param_stim[\"resolution\"]+50),\n    pos=param_stim[\"pos_right\"],\n    tex=grating_vergence,\n    mask=mask,\n    units=\"pix\",\n    sf=param_stim[\"vergence_sf\"])\n\n# Left fixation dot\nfixation_left = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_left\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\nfixation_right = visual.ShapeStim(\n    win=win,\n    name='polygon',\n    size=(param_stim[\"resolution\"]/50, param_stim[\"resolution\"]/50),\n    vertices='circle',\n    ori=0.0,\n    pos=param_stim[\"pos_right\"],\n    anchor='center',\n    lineWidth=1.0,\n    colorSpace='rgb',\n    lineColor='white',\n    fillColor='white',\n    opacity=None,\n    depth=0.0,\n    interpolate=True)\n\n\n###############################################################################\n#                                    DRAW\n###############################################################################\n\nif WHAT_TO_DRAW == \"both\":\n# Draw stimuli on buffer\n    vergence_left.draw()\n    vergence_right.draw()\n    stim_left.draw()\n    stim_right.draw()\n    fixation_left.draw()\n    fixation_right.draw()\nelif WHAT_TO_DRAW == \"left\":\n    vergence_left.draw()\n    stim_left.draw()\n    fixation_left.draw()\nelse:\n    vergence_right.draw()\n    stim_right.draw()\n    fixation_right.draw()\n\n# Present stimuli on the window\nwin.flip()\n# Save stimuli if requested\nif SAVE_IMAGE:\n    frame = win.getMovieFrame()\n    frame.save(os.path.join(this_dir, image_name))\n# Terminate when a key is pressed\nevent.waitKeys()\n# Close the Psychopy window\nwin.close()"
  }
]